<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>Reference documentation</title><link rel="stylesheet" type="text/css" href="style.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="book" title="Reference documentation"><div class="titlepage"><div><div><h1 class="title"><a name="d5e1"></a>Reference documentation</h1></div><div><p class="releaseinfo">5.2
		</p></div><div><p class="copyright">Copyright &copy; 2008-2017 </p></div><div><div class="legalnotice" title="Legal Notice"><a name="d5e8"></a>
			<p>Copies of this document may be made for your own use and for
				distribution to others, provided that you do not charge any fee for
				such copies and further provided that each copy contains this
				Copyright Notice, whether distributed in print or electronically.
			</p>
		</div></div></div><hr></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="part"><a href="#d5e10">I. Introduction to DataCleaner</a></span></dt><dd><dl><dt><span class="chapter"><a href="#d5e13">1. Background and concepts</a></span></dt><dd><dl><dt><span class="section"><a href="#what_is_dq">What is data quality (DQ)?</a></span></dt><dt><span class="section"><a href="#what_is_data_profiling">What is data profiling?</a></span></dt><dt><span class="section"><a href="#what_is_data_wrangling">What is data wrangling?</a></span></dt><dt><span class="section"><a href="#what_is_datastore">What is a datastore?</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e55">Composite datastore</a></span></dt></dl></dd><dt><span class="section"><a href="#what_is_data_monitoring">What is data monitoring?</a></span></dt><dt><span class="section"><a href="#what_is_mdm">What is master data management (MDM)?</a></span></dt></dl></dd><dt><span class="chapter"><a href="#d5e69">2. Getting started with DataCleaner desktop</a></span></dt><dd><dl><dt><span class="section"><a href="#installing_datacleaner">Installing the desktop application</a></span></dt><dt><span class="section"><a href="#connecting_to_your_datastore">Connecting to your datastore</a></span></dt><dt><span class="section"><a href="#adding_components">Adding components to the job</a></span></dt><dt><span class="section"><a href="#wiring_components">Wiring components together</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e153">Transformer output</a></span></dt><dt><span class="section"><a href="#d5e159">Filter requirement</a></span></dt><dt><span class="section"><a href="#wiring_components_output_data_streams">Output data streams</a></span></dt></dl></dd><dt><span class="section"><a href="#executing">Executing jobs</a></span></dt><dt><span class="section"><a href="#saving_and_opening_jobs">Saving and opening jobs</a></span></dt><dt><span class="section"><a href="#template_jobs">Template jobs</a></span></dt><dt><span class="section"><a href="#writing-cleansed-data">Writing cleansed data to files</a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#d5e232">II. Analysis component reference</a></span></dt><dd><dl><dt><span class="chapter"><a href="#transformations">3. Transform</a></span></dt><dd><dl><dt><span class="section"><a href="#javascript_transformer">JavaScript transformer</a></span></dt><dt><span class="section"><a href="#invoke_child_analysis_job">Invoke child Analysis job</a></span></dt><dt><span class="section"><a href="#equals_filter">Equals</a></span></dt><dt><span class="section"><a href="#max_rows_filter">Max rows</a></span></dt><dt><span class="section"><a href="#not_null_filter">Not null</a></span></dt><dt><span class="section"><a href="#union"> Union </a></span></dt></dl></dd><dt><span class="chapter"><a href="#improve">4. Improve</a></span></dt><dd><dl><dt><span class="section"><a href="#synonym_lookup_transformer">Synonym lookup</a></span></dt><dt><span class="section"><a href="#table_lookup_transformer">Table lookup</a></span></dt></dl></dd><dt><span class="chapter"><a href="#analyzers">5. Analyze</a></span></dt><dd><dl><dt><span class="section"><a href="#boolean_analyzer">Boolean analyzer</a></span></dt><dt><span class="section"><a href="#completeness_analyzer">Completeness analyzer</a></span></dt><dt><span class="section"><a href="#character_set_distribution">Character set distribution</a></span></dt><dt><span class="section"><a href="#date_gap_analyzer">Date gap analyzer</a></span></dt><dt><span class="section"><a href="#date_time_analyzer">Date/time analyzer</a></span></dt><dt><span class="section"><a href="#number_analyzer">Number analyzer</a></span></dt><dt><span class="section"><a href="#pattern_finder">Pattern finder</a></span></dt><dt><span class="section"><a href="#reference_data_matcher_analyzer">Reference data matcher</a></span></dt><dt><span class="section"><a href="#referential_integrity_analyzer">Referential integrity</a></span></dt><dt><span class="section"><a href="#string_analyzer">String analyzer</a></span></dt><dt><span class="section"><a href="#unique_key_check">Unique key check</a></span></dt><dt><span class="section"><a href="#value_distribution">Value distribution</a></span></dt><dt><span class="section"><a href="#value_matcher">Value matcher</a></span></dt><dt><span class="section"><a href="#weekday_distribution">Weekday distribution</a></span></dt></dl></dd><dt><span class="chapter"><a href="#writers">6. Write</a></span></dt><dd><dl><dt><span class="section"><a href="#writers_create_csv">Create CSV file</a></span></dt><dt><span class="section"><a href="#writers_create_excel">Create Excel spreadsheet</a></span></dt><dt><span class="section"><a href="#writers_create_staging">Create staging table</a></span></dt><dt><span class="section"><a href="#writers_insert_into_table">Insert into table</a></span></dt><dt><span class="section"><a href="#writers_update_table">Update table</a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#d5e633">III. Reference data</a></span></dt><dd><dl><dt><span class="chapter"><a href="#reference_data_dictionaries">7. Dictionaries</a></span></dt><dt><span class="chapter"><a href="#synonym_catalogs">8. Synonyms (aka. Synonym catalogs)</a></span></dt><dd><dl><dt><span class="section"><a href="#reference_data_synonym_catalog_text">Text file synonym catalog</a></span></dt><dt><span class="section"><a href="#reference_data_synonym_catalog_datastore">Datastore synonym catalog</a></span></dt></dl></dd><dt><span class="chapter"><a href="#reference_data_string_patterns">9. String patterns</a></span></dt></dl></dd><dt><span class="part"><a href="#d5e683">IV. Configuration reference</a></span></dt><dd><dl><dt><span class="chapter"><a href="#chapter_configuration_file">10. Configuration file</a></span></dt><dd><dl><dt><span class="section"><a href="#configuration_file_xml_schema">XML schema</a></span></dt><dt><span class="section"><a href="#configuration_file_datastores">Datastores</a></span></dt><dd><dl><dt><span class="section"><a href="#configuration_file_datastore_jdbc">Database (JDBC) connections</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_csv">Comma-Separated Values (CSV) files</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_fixed_width">Fixed width value files</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_excel">Excel spreadsheets</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_xml">XML file datastores</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_elasticsearch">ElasticSearch index</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_mongodb">MongoDB databases</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_couchdb">CouchDB databases</a></span></dt><dt><span class="section"><a href="#d5e767">Composite datastore</a></span></dt></dl></dd><dt><span class="section"><a href="#configuration_file_reference_data">Reference data</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e774">Dictionaries</a></span></dt><dt><span class="section"><a href="#d5e786">Synonym catalogs</a></span></dt><dt><span class="section"><a href="#d5e795">String patterns</a></span></dt></dl></dd><dt><span class="section"><a href="#configuration_file_task_runner">Task runner</a></span></dt><dt><span class="section"><a href="#configuration_file_storage_provider">Storage provider</a></span></dt></dl></dd><dt><span class="chapter"><a href="#d5e825">11. Analysis job files</a></span></dt><dd><dl><dt><span class="section"><a href="#job_files_xml_schema">XML schema</a></span></dt><dt><span class="section"><a href="#job_files_source_section">Source section</a></span></dt></dl></dd><dt><span class="chapter"><a href="#d5e853">12. Logging</a></span></dt><dd><dl><dt><span class="section"><a href="#logging_configuration_file">Logging configuration file</a></span></dt><dt><span class="section"><a href="#logging_default_configuration">Default logging configuration</a></span></dt><dt><span class="section"><a href="#logging_levels">Modifying logging levels</a></span></dt><dt><span class="section"><a href="#logging_alternative_outputs">Alternative logging outputs</a></span></dt></dl></dd><dt><span class="chapter"><a href="#d5e903">13. Database drivers</a></span></dt><dd><dl><dt><span class="section"><a href="#database_drivers_desktop">Installing Database drivers in DataCleaner desktop</a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#d5e926">V. Invoking DataCleaner jobs</a></span></dt><dd><dl><dt><span class="chapter"><a href="#command_line_interface">14. Command-line interface</a></span></dt><dd><dl><dt><span class="section"><a href="#cli_executables">Executables</a></span></dt><dt><span class="section"><a href="#cli_usage_scenarios">Usage scenarios</a></span></dt><dt><span class="section"><a href="#cli_executing_a_job">Executing an analysis job</a></span></dt><dt><span class="section"><a href="#cli_listing">Listing datastore contents and available components</a></span></dt><dt><span class="section"><a href="#section_parameterizable_jobs">Parameterizable jobs</a></span></dt><dt><span class="section"><a href="#cli_overriding_configuration_elements">Dynamically overriding configuration elements</a></span></dt></dl></dd><dt><span class="chapter"><a href="#hadoop_interface">15. Apache Hadoop and Spark interface</a></span></dt><dd><dl><dt><span class="section"><a href="#hadoop_deployment">Hadoop deployment overview</a></span></dt><dt><span class="section"><a href="#hadoop_spark_setup">Setting up Spark and DataCleaner environment</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e1052">Upload configuration file to HDFS</a></span></dt><dt><span class="section"><a href="#d5e1065">Upload job file to HDFS</a></span></dt><dt><span class="section"><a href="#d5e1079">Upload executables to HDFS</a></span></dt></dl></dd><dt><span class="section"><a href="#hadoop_launch">Launching DataCleaner jobs using Spark</a></span></dt><dt><span class="section"><a href="#hadoop_desktop">Using Hadoop in DataCleaner desktop</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e1123">Configure Hadoop clusters</a></span></dt><dt><span class="section"><a href="#d5e1150">CSV datastores on HDFS</a></span></dt></dl></dd><dt><span class="section"><a href="#hadoop_limitations">Limitations of the Hadoop interface</a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#d5e1175">VI. Third party integrations</a></span></dt><dd><dl><dt><span class="chapter"><a href="#d5e1178">16. Pentaho integration</a></span></dt><dd><dl><dt><span class="section"><a href="#pentaho_configure_dc_installation">Configure DataCleaner in Pentaho Data Integration </a></span></dt><dt><span class="section"><a href="#pentaho_launch_dc_from_pdi">Launch DataCleaner to profile Pentaho Data Integration steps
		</a></span></dt><dt><span class="section"><a href="#pentaho_run_dc_jobs_from_pdi">Run DataCleaner jobs in Pentaho Data Integration</a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#d5e1210">VII. Developer's guide</a></span></dt><dd><dl><dt><span class="chapter"><a href="#d5e1213">17. Architecture</a></span></dt><dd><dl><dt><span class="section"><a href="#architecture_data_access">Data access</a></span></dt><dt><span class="section"><a href="#architecture_processing_framework">Processing framework</a></span></dt></dl></dd><dt><span class="chapter"><a href="#chapter_executing_jobs_through_code">18. Executing jobs through code</a></span></dt><dd><dl><dt><span class="section"><a href="#section_development_executing_job_steps">Overview of steps and options</a></span></dt><dt><span class="section"><a href="#section_development_configuration_configuration">Step 1: Configuration</a></span></dt><dt><span class="section"><a href="#section_development_configuration_job">Step 2: Job</a></span></dt><dt><span class="section"><a href="#section_development_configuration_execution">Step 3: Execution</a></span></dt><dt><span class="section"><a href="#section_development_configuration_result">Step 4: Result</a></span></dt></dl></dd><dt><span class="chapter"><a href="#chapter_developer_resources">19. Developer resources</a></span></dt><dd><dl><dt><span class="section"><a href="#section_development_tutorials">Extension development tutorials</a></span></dt><dt><span class="section"><a href="#developer_building_datacleaner">Building DataCleaner</a></span></dt></dl></dd><dt><span class="chapter"><a href="#d5e1407">20. Extension packaging</a></span></dt><dd><dl><dt><span class="section"><a href="#extensions_annotated_components">Annotated components</a></span></dt><dt><span class="section"><a href="#extensions_single_jar_file">Single JAR file</a></span></dt><dt><span class="section"><a href="#extensions_metadata_xml">Extension metadata XML</a></span></dt><dt><span class="section"><a href="#extensions_component_icons">Component icons</a></span></dt></dl></dd><dt><span class="chapter"><a href="#embedding_datacleaner">21. Embedding DataCleaner</a></span></dt></dl></dd></dl></div><div class="list-of-tables"><p><b>List of Tables</b></p><dl><dt>3.1. <a href="#d5e249">JavaScript variables</a></dt><dt>3.2. <a href="#d5e284">JavaScript data types</a></dt><dt>5.1. <a href="#d5e408">Completeness analyzer properties</a></dt><dt>5.2. <a href="#d5e445">Pattern finder properties</a></dt><dt>5.3. <a href="#d5e507">Referential integrity properties</a></dt><dt>5.4. <a href="#d5e528">Unique key check properties</a></dt><dt>5.5. <a href="#d5e546">Value distribution properties</a></dt></dl></div>

	

	<div class="part" title="Part&nbsp;I.&nbsp;Introduction to DataCleaner"><div class="titlepage"><div><div><h1 class="title"><a name="d5e10"></a>Part&nbsp;I.&nbsp;Introduction to DataCleaner</h1></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="chapter"><a href="#d5e13">1. Background and concepts</a></span></dt><dd><dl><dt><span class="section"><a href="#what_is_dq">What is data quality (DQ)?</a></span></dt><dt><span class="section"><a href="#what_is_data_profiling">What is data profiling?</a></span></dt><dt><span class="section"><a href="#what_is_data_wrangling">What is data wrangling?</a></span></dt><dt><span class="section"><a href="#what_is_datastore">What is a datastore?</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e55">Composite datastore</a></span></dt></dl></dd><dt><span class="section"><a href="#what_is_data_monitoring">What is data monitoring?</a></span></dt><dt><span class="section"><a href="#what_is_mdm">What is master data management (MDM)?</a></span></dt></dl></dd><dt><span class="chapter"><a href="#d5e69">2. Getting started with DataCleaner desktop</a></span></dt><dd><dl><dt><span class="section"><a href="#installing_datacleaner">Installing the desktop application</a></span></dt><dt><span class="section"><a href="#connecting_to_your_datastore">Connecting to your datastore</a></span></dt><dt><span class="section"><a href="#adding_components">Adding components to the job</a></span></dt><dt><span class="section"><a href="#wiring_components">Wiring components together</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e153">Transformer output</a></span></dt><dt><span class="section"><a href="#d5e159">Filter requirement</a></span></dt><dt><span class="section"><a href="#wiring_components_output_data_streams">Output data streams</a></span></dt></dl></dd><dt><span class="section"><a href="#executing">Executing jobs</a></span></dt><dt><span class="section"><a href="#saving_and_opening_jobs">Saving and opening jobs</a></span></dt><dt><span class="section"><a href="#template_jobs">Template jobs</a></span></dt><dt><span class="section"><a href="#writing-cleansed-data">Writing cleansed data to files</a></span></dt></dl></dd></dl></div>
		

		<div class="chapter" title="Chapter&nbsp;1.&nbsp;Background and concepts"><div class="titlepage"><div><div><h2 class="title"><a name="d5e13"></a>Chapter&nbsp;1.&nbsp;Background and concepts</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>In this chapter we will try to define how we see the concepts
				and terms surrounding the environment(s) around DataCleaner.
			</p>
			<p>Although these terms have no strict definitions, you can use
				this chapter as a guide, at least for the scope of how to use and
				what to expect from DataCleaner in relation to the described topics.
			</p>
			<p>As a lot of the statements in this chapter are in deed
				subjective or based upon personal experience, we encourage everyone
				to provide their feedback and to contribute corrections/improvements
				to it.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#what_is_dq">What is data quality (DQ)?</a></span></dt><dt><span class="section"><a href="#what_is_data_profiling">What is data profiling?</a></span></dt><dt><span class="section"><a href="#what_is_data_wrangling">What is data wrangling?</a></span></dt><dt><span class="section"><a href="#what_is_datastore">What is a datastore?</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e55">Composite datastore</a></span></dt></dl></dd><dt><span class="section"><a href="#what_is_data_monitoring">What is data monitoring?</a></span></dt><dt><span class="section"><a href="#what_is_mdm">What is master data management (MDM)?</a></span></dt></dl></div>

	

	

	<div class="section" title="What is data quality (DQ)?"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="what_is_dq"></a>What is data quality (DQ)?</h2></div></div></div>
		
		<p>Data Quality (DQ) is a concept and a business term covering the
			quality of the data used for a particular purpose. Often times the DQ
			term is applied to the quality of data used in business decisions but
			it may also refer to the quality of data used in research, campaigns,
			processes and more.
		</p>
		<p>Working with Data Quality typically varies a lot from project to
			project, just as the issues in the quality of data vary a lot.
			Examples of data quality issues include:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				Completeness of data
			</li><li class="listitem">
				Correctness of data
			</li><li class="listitem">
				Duplication of data
			</li><li class="listitem">
				Uniformedness/standardization of data
			</li></ol></div>
		<p>A less technical definition of high-quality data is, that data
			are of high quality "if they are fit for their intended uses in
			operations, decision making and planning" (J. M. Juran).
		</p>

		<p>Data quality analysis (DQA) is the (human) process of
			examining
			the quality of data for a particular process or
			organization. The DQA
			includes both technical and non-technical
			elements. For example, to
			do a good DQA you will probably need to talk
			to users, business
			people, partner organizations and maybe customers.
			This is needed to
			asses what the goal of the DQA should be.
		</p>
		<p>From a technical viewpoint the main task in a DQA is the data
			profiling activity, which will help you discover and measure the
			current state of affairs in the data.
		</p>
	</div>

	<div class="section" title="What is data profiling?"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="what_is_data_profiling"></a>What is data profiling?</h2></div></div></div>
		
		<p>Data profiling is the activity of investigating a datastore to
			create a 'profile' of it. With a profile of your datastore you will
			be a lot better equipped to actually use and improve it.
		</p>
		<p>
			The way you do profiling often depends on whether you already
			have
			some ideas about the quality of the data or if you're not
			experienced
			with the datastore at hand. Either way we recommend an
			<span class="emphasis"><em>explorative</em></span>
			approach, because even though you think there are only a certain
			amount of issues you need to look for, it is our experience (and
			reasoning behind a lot of the features of DataCleaner) that it is
			just as important to check those items in the data that you think are
			correct! Typically it's cheap to include a bit more data into your
			analysis and the results just might surprise you and save you time!
		</p>
		<p>DataCleaner comprises (amongst other aspects) a desktop
			application for doing data profiling on just about any kind of
			datastore.
		</p>
	</div>
	
	<div class="section" title="What is data wrangling?"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="what_is_data_wrangling"></a>What is data wrangling?</h2></div></div></div>
		
		<p>
			From <a class="link" href="#">Wikipedia</a>, we get a
			good introductory explanation of the term 'Data wrangling':
		</p>
		<div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Tip</h3>
			<p>
				Data munging or data wrangling is loosely the process of manually converting or mapping data
				from one "raw" form into another format that allows for more convenient consumption of the data
				with the help of semi-automated tools. This may include further munging, data visualization,
				data aggregation, training a statistical model, as well as many other potential uses.
			</p>
		</div>
		<p>
			As can be seen, it's often important to do changes to data in order to analyze it. That's why DataCleaner
			bundles a lot of transformational and improvement features which allow the user to not only profile data,
			but also to reshape it into a form that is fitting.
		</p>
	</div>

	<div class="section" title="What is a datastore?"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="what_is_datastore"></a>What is a datastore?</h2></div></div></div>
		
		<p>
			A datastore is the place where data is stored. Usually
			enterprise data
			lives in relational databases, but there are numerous
			exceptions to
			that rule.
		</p>
		<p>
			To comprehend different sources of data, such as
			databases,
			spreadsheets, XML files and even standard business
			applications, we
			employ the umbrella term
			<span class="emphasis"><em>datastore</em></span>
			. DataCleaner is capable of retrieving data from a very wide range of
			datastores. And furthermore, DataCleaner can update the data of most
			of these datastores as well.
		</p>
		<p>
			A datastore can be created in the UI or via
			<a class="link" href="#chapter_configuration_file" title="Chapter&nbsp;10.&nbsp;Configuration file"> the configuration file</a>
			.
			You can create a datastore from any type of source such
			as:
			CSV,
			Excel, Oracle Database, MySQL, etc.
		</p>
		<div class="mediaobject"><img src="register_datastore.png"></div>
		<div class="section" title="Composite datastore"><div class="titlepage"><div><div><h3 class="title"><a name="d5e55"></a>Composite datastore</h3></div></div></div>
			
			<p>
				A
				<span class="emphasis"><em>composite</em></span>
				datastore contains
				<span class="emphasis"><em>multiple datastores</em></span>
				. The main advantage of a composite datastore is that it allows you
				to analyze and process data from multiple
				sources in the same job.
			</p>
		</div>
	</div>

	<div class="section" title="What is data monitoring?"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="what_is_data_monitoring"></a>What is data monitoring?</h2></div></div></div>
		
		<p>We've argued that data profiling is ideally an explorative
			activity. Data monitoring typically isn't! The measurements that you
			do when profiling often times needs to be continuously checked so
			that your improvements are enforced through time. This is what data
			monitoring is typically about.
		</p>
		<p>Data monitoring solutions come in different shapes and sizes.
			You can set
			up your own bulk of scheduled jobs that run every night.
			You can
			build alerts around it that send you emails if a particular
			measure
			goes beyond its allowed thresholds, or in some cases you can
			attempt
			ruling out the issue entirely by applying First-Time-Right
			(FTR)
			principles that validate data at entry-time. eg. at data
			registration
			forms and more.
		</p>

	</div>

	<div class="section" title="What is master data management (MDM)?"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="what_is_mdm"></a>What is master data management (MDM)?</h2></div></div></div>
		
		<p>Master data management (MDM) is a very broad term and is seen
			materialized in a variety of ways. For the scope of this document it
			serves more as a context of data quality than an activity that we
			actually target with DataCleaner per-se.
		</p>
		<p>The overall goals of MDM is to manage the important data of an
			organization. By "master data" we refer to "a single version of the
			truth", ie. not the data of a particular system, but for example all
			the customer data or product data of a company. Usually this data is
			dispersed over
			multiple datastores, so an important part of MDM
			is the
			process of unifying the data into a single model.
		</p>
		<p>Obviously another of the very important issues to handle in MDM
			is
			the quality of data. If you simply gather eg. "all customer data"
			from all systems in an organization, you will most likely see a lot
			of data quality issues. There will be a lot of duplicate entries,
			there will be variances in the way that customer data is filled,
			there will be different identifiers and even different levels of
			granularity for defining "what is a customer?". In the context of
			MDM, DataCleaner can serve as the engine to cleanse, transform and
			unify data from multiple datastores into the single view of the
			master data.
		</p>
	</div>

</div>
		<div class="chapter" title="Chapter&nbsp;2.&nbsp;Getting started with DataCleaner desktop"><div class="titlepage"><div><div><h2 class="title"><a name="d5e69"></a>Chapter&nbsp;2.&nbsp;Getting started with DataCleaner desktop</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#installing_datacleaner">Installing the desktop application</a></span></dt><dt><span class="section"><a href="#connecting_to_your_datastore">Connecting to your datastore</a></span></dt><dt><span class="section"><a href="#adding_components">Adding components to the job</a></span></dt><dt><span class="section"><a href="#wiring_components">Wiring components together</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e153">Transformer output</a></span></dt><dt><span class="section"><a href="#d5e159">Filter requirement</a></span></dt><dt><span class="section"><a href="#wiring_components_output_data_streams">Output data streams</a></span></dt></dl></dd><dt><span class="section"><a href="#executing">Executing jobs</a></span></dt><dt><span class="section"><a href="#saving_and_opening_jobs">Saving and opening jobs</a></span></dt><dt><span class="section"><a href="#template_jobs">Template jobs</a></span></dt><dt><span class="section"><a href="#writing-cleansed-data">Writing cleansed data to files</a></span></dt></dl></div>

	

	<div class="section" title="Installing the desktop application"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="installing_datacleaner"></a>Installing the desktop application</h2></div></div></div>
		

		<p>These are the system requirements of DataCleaner:</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				A computer (with a graphical display, except if run in command-line
				mode).
			</li><li class="listitem">
				A
				<a class="link" href="http://www.java.com" target="_top">Java Runtime Environment</a>
				(JRE), version 7 or higher.
			</li><li class="listitem">
				A DataCleaner software license file for professional editions. If you've requested a free
				trial or purchased DataCleaner online, this file will have been sent
				to your email address.
			</li></ol></div>

		<p>
			Start the installation procedure using the installer program.
			The
			installer program is an executable JAR file, which is executable
			on
			most systems if you simply double-click it.
		</p>

		<div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Tip</h3>
			<p>If the installer does not launch when you double-click it,
				open
				a command prompt and enter:
			</p>
			<div class="blockquote"><blockquote class="blockquote">
				java -jar DataCleaner-[edition]-[version]-install.jar
			</blockquote></div>
		</div>

		<p>Troubleshooting</p>
		<p>Usually the installation procedure is trivial and
			self-explanatory. But in case something is not working as expected,
			please check the following points:
		</p>

		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>
					On Windows systems, if you do not have
					<span class="emphasis"><em>Administrative privileges</em></span>
					on the machine, we encourage you to install DataCleaner in your
					user's directory instead of in 'Program Files'.
				</p>
			</li><li class="listitem">
				<p>
					On some Windows systems you can get a warning '
					<span class="emphasis"><em>There is no script engine for file extension '.js'
					</em></span>
					'. This happens when .js files (JavaScript) files are associated
					with an editor instead of Windows' built-in scripting engine. To
					resolve this issue, please refer to these help links:
				</p>
				<div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
						<p>
							<a class="link" href="http://answers.microsoft.com/en-us/windows/forum/windows_7-system/fix-there-is-no-script-engine-for-file-extension/95d58867-3f31-45e3-aeaf-32ecea4d33c1" target="_top">answers.microsoft.com</a>
							, which address the issue and recommends...
						</p>
					</li><li class="listitem">
						<p>
							<a class="link" href="http://www.winhelponline.com/articles/230/1/Error-There-is-no-script-engine-for-file-extension-when-running-js-files.html" target="_top">winhelponline.com</a>
							, which has a fix for the issue
						</p>
					</li></ol></div>
			</li><li class="listitem">
				<p>If you have issues with locating or selecting the software
					license file, you can skip the step in the installer by copying the
					license file manually to this folder: '~/.datacleaner' (where ~ is
					your User's home folder). Note that on Windows machines it is
					prohibited by Windows explorer to create directories starting with
					dot (.), but it can be done using the command prompt:
				</p>
				<div class="blockquote"><blockquote class="blockquote">
					mkdir .datacleaner
				</blockquote></div>
			</li><li class="listitem">
				<p>
					If you have problems with running out of heap memory, you can adjust
					the amount of memory allocated in the file 'DataCleaner.l4j', alternatively
					the file 'DataCleaner-console.l4j' if you are running from console.
				</p>

				<p>
					In order to increase Java heap memory you can use the following
					parameters:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="code">-Xms</code> - set the minimum heap size
						</li><li class="listitem">
							<code class="code">-Xmx</code> - set the maximum heap size
						</li></ul></div><p>
				</p>
				<p>
					Eg. <code class="code">-Xmx4g</code> to set the maximum size of the heap to 4 GB.
				</p>
			</li></ol></div>

	</div>

	<div class="section" title="Connecting to your datastore"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="connecting_to_your_datastore"></a>Connecting to your datastore</h2></div></div></div>
		
		<p>Below is a screenshot of the initial screen that
			will be
			presented when launching DataCleaner (desktop community edition).
			A
			new datastore can be added in the "New job from scratch" or in
			"Manage datastores" screens available by clicking the buttons in the
			bottom of the screen.
		</p>
		<div class="mediaobject"><img src="welcome_screen.jpg"></div>
		<p>
			File datastores can be added using a drop zone (or browse
			button) located in the top of the screen. Below, there are buttons
			that enable adding databases or cloud services.
		</p>
		<div class="mediaobject"><img src="new_job_from_scratch_screen.jpg"></div>
		<p>
			If the file is added
			using the drop zone, its format will be
			inferred. If you need more control
			over how the file is going to be
			interpreted, use the alternative way
			to add a new datastore - "Manage
			datastores" button in the welcome screen.
		</p>
		<div class="mediaobject"><img src="datastore_management_screen.jpg"></div>
		<p>
			The "Datastore management" screen - except from viewing and
			editing existing datastores - has an option to add a new one based on
			its type. Choose an icon in the bottom of the screen that suits your
			datastore type.
		</p>

		<p>Once you've registered ('created') your own datastore, you can
			select it from the list and (in "New job from scratch" screen) or
			select it from the list and click "Build job" (in "Datastore
			Management" screen) to start working with
			it!
		</p>

		<div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Tip</h3>
			<p>
				You can also configure your datastore by means of the
				configuration
				file (conf.xml), which has both some pros and some cons.
				For more
				information, read the
				<a class="link" href="#chapter_configuration_file" title="Chapter&nbsp;10.&nbsp;Configuration file">configuration file chapter</a>
				.
			</p>
		</div>
	</div>

	<div class="section" title="Adding components to the job"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="adding_components"></a>Adding components to the job</h2></div></div></div>
		
		<p>There are a few different kinds of components that you can add
			to your
			job:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>
					<span class="emphasis"><em>Analyzers</em></span>
					, which are the most important components. Actually, without at
					least one analyzer the job will not run (if you execute the job
					without adding one, DataCleaner will suggest adding a basic one
					that will save the output to a file). An analyzer is a component
					that inspects the data that it receives and generates a result or a
					report. The majority of the data profiling cruft is created as
					analyzers.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>Transformers</em></span>
					are components used to modify the data before analyzing it.
					Sometimes it's neccessary to extract parts of a value or combine
					two
					values to correctly get an idea about a particular measure. In
					other scenarios, transformers can be used to perform reference data
					lookups or other similar tasks and place the results of an
					operation into the stream of data in the job.
				</p>
				<p>
					The result of a transformer is a set of output columns. These
					columns work exactly like regular columns in your job, except that
					they have a preceding step in the flow before they become
					materialized.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>Filters</em></span>
					are components that split the flow of processing in a job. A filter
					will have a number of possible outcomes and depending on the
					outcome of a filter, a particular row might be processed by
					different sub-flows. Filters are often used simply to disregard
					certain rows from the analysis, eg. null values or values outside
					the range of interest.
				</p>
			</li></ol></div>
		<p>Each of these
			components will be presented as a node in the job
			graph. Double-clicking a component (graph node) will bring its
			configuration dialog.
		</p>
		<p>
			Transformers and filters are added to your job using the "Transform"
			and "Improve"
			menus. The menus are available in component library on the left
			or by right-clicking on an empty space in the canvas.
			Please refer to
			the reference chapter
			<a class="link" href="#transformations" title="Chapter&nbsp;3.&nbsp;Transform">Transformations</a>
			for more
			information on specific transformers and filters.
		</p>
		<p>
			Analyzers are added to your job using the "Analyze" menu (in most
			cases), but also "Write" menu for analyzers that save output to a
			datastore. Please
			refer to the reference chapter
			<a class="link" href="#analyzers" title="Chapter&nbsp;5.&nbsp;Analyze">Analyzers</a>
			for more
			information on specific analyzers.
		</p>
	</div>

	<div class="section" title="Wiring components together"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="wiring_components"></a>Wiring components together</h2></div></div></div>
		
		<p>Simply adding a transformer or filter actually doesn't change
			your job as such! This is because these components only have an
			impact if you wire them together.
		</p>
		<div class="section" title="Transformer output"><div class="titlepage"><div><div><h3 class="title"><a name="d5e153"></a>Transformer output</h3></div></div></div>
			
			<p>To wire a transformer you simply need to draw an arrow between
				the components in the graph. You can start drawing it by
				right-clicking the first of the components and choosing "Link to..."
				from the context menu. An alternative way to enter the drawing mode
				is to select the component and connect the components with Shift
				button pressed.
			</p>
		</div>
		<div class="mediaobject"><img src="connecting_components.jpg"></div>
		<div class="section" title="Filter requirement"><div class="titlepage"><div><div><h3 class="title"><a name="d5e159"></a>Filter requirement</h3></div></div></div>
			
			<p>To wire a filter you need to set up a dependency on either of
				it's outcomes. All components have a button for selecting filter
				outcomes in the top-right corners of their configuration dialogs.
				Click this button to select a filter outcome to depend on.
			</p>

			<p>If you have multiple filters you can chain these simply by having dependent
				outcomes of the individual filters. This will require <span class="emphasis"><em>all</em></span> filter requirements in
				the chain to be met, for a record to be passed to the component (AND logic).
			</p>

			<div class="mediaobject"><img src="filter_chain.png"><div class="caption">
					<p>Chained filters</p>
				</div></div>

			<p>
				Using "Link to...", it is also possible to wire several filters to a component
				in a kind of diamond shape. In that case, if <span class="emphasis"><em>any</em></span> of the the filter requirements
				are met, the record will be passed to the component (OR logic).
			</p>

			<div class="mediaobject"><img src="filter_diamond.png"><div class="caption">
					<p>"Diamond" filters</p>
				</div></div>
		</div>
		<div class="section" title="Output data streams"><div class="titlepage"><div><div><h3 class="title"><a name="wiring_components_output_data_streams"></a>Output data streams</h3></div></div></div>
			
			<p>
				The "Link to..." option wires components together in the "main
				flow". However, some components are able to produce additional
				output
				data streams. For example, the main feature of a Completeness
				Analyzer is to
				produce a summary of records completeness in the job
				result window.
				Additionally, it produces two output data streams -
				"Complete
				records" and "Incomplete records".
				Output data streams behave similarly to a source table, although such
				a table is created dynamically by a component.
				This enables further processing of such output.
			</p>
			<p>Components producing output data streams have additional "Link
				to..." position in the right-click menu to wire the output with
				subsequent components.
			</p>
			<div class="mediaobject"><img src="output_data_streams_link_to.png"></div>
			<p>Instead of wiring components with "Link to..." menu option,
				double-clicking a component brings up a configuration dialog that
				can be used to choose its input columns. In the top-right corner of
				the dialog, the scope of the component can be chosen. Switching
				between scopes gives us the possibility to choose input columns from
				the "main flow" (default scope) or from output data streams.
			</p>
			<div class="mediaobject"><img src="output_data_streams_change_scope.png"></div>
			<p>An example job using output data streams:
			</p>
			<div class="mediaobject"><img src="wiring_components_output_data_streams_job.png"></div>
		</div>
		<div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Tip</h3>
			<p>
                 The canvas displays messages (in the bottom of the screen) which
				contain instructions with the next steps that need to be performed
				in other to build a valid job.
			</p>
		</div>

	</div>

	<div class="section" title="Executing jobs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="executing"></a>Executing jobs</h2></div></div></div>
		
		<p>When a job has been built you can execute it. To check whether
			your job is correctly configured and ready to execute, check the
			status bar in the bottom of the job building window.
		</p>
		<p>To execute the job, simply click the "Execute" button in
			the
			top-right corner of the window. This will bring up the result
			window,
			which contains:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>
					The
					<span class="emphasis"><em>Progress information</em></span>
					tab which contains useful information and progress indications
					while the job is being executed.
				</p>
			</li><li class="listitem">
				<p>Additional tabs for each component type that produces a
					result/report. For instance 'Value distribution' if such a
					component was added to the job.
				</p>
			</li></ol></div>
		<p>Here's an example of an analysis result window:</p>
		<div class="mediaobject"><img src="analysis_job_05_result.jpg"></div>
	</div>

	<div class="section" title="Saving and opening jobs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="saving_and_opening_jobs"></a>Saving and opening jobs</h2></div></div></div>
		
		<p>You can save your jobs in order to reuse them at a later time.
			Saving a job is simple: just click the "Save" button
			in the top
			panel of the window.
		</p>
		<p>Analysis jobs are saved in files with the ".analysis.xml"
			extension. These files are XML files that are readable and editable
			using any XML editor.
		</p>
		<p>Opening jobs can be done using the "Open" menu item. Opening a
			job will restore a job building window
			from where you can edit and run
			the job.
		</p>
	</div>

	<div class="section" title="Template jobs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="template_jobs"></a>Template jobs</h2></div></div></div>
		
		<p>DataCleaner contains a feature where you can reuse a job for
			multiple datastores or just multiple columns in the same datastore.
			We call this feature 'template jobs'.
		</p>
		<p>When opening a job you are presented with a file chooser. When
			you select a job file a panel will appear, containing some
			information about the job as well as available actions:
		</p>
		<div class="mediaobject"><img src="template_job_01_filechooser.jpg"></div>
		<p>If you click the 'Open as template' button you will be presented
			with a dialog where you can map the job's original columns to a new
			set of columns:
		</p>
		<div class="mediaobject"><img src="template_job_02_mapping.jpg"></div>
		<p>First you need to specify the datastore to use. On the left side
			you see the name of the original datastore, but the job is not
			restricted to use only this datastore. Select a datastore from the
			list and the fields below for the columns will become active.
		</p>
		<p>Then you need to map individual columns. If you have two
			datastore that have the same column names, you can click the "Map
			automatically" button and they will be automatically assigned.
			Otherwise you need to map the columns from the new datastore's
			available columns.
		</p>
		<p>Finally your job may contain 'Job-level variables'. These are
			configurable properties of the job that you might also want to fill
			out.
		</p>
		<p>Once these 2-3 steps have been completed, click the "Open job"
			button, and DataCleaner will be ready for executing the job on a new
			set of columns!
		</p>
	</div>

	<div class="section" title="Writing cleansed data to files"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="writing-cleansed-data"></a>Writing cleansed data to files</h2></div></div></div>
		
		<p>Although the primary focus of DataCleaner is analysis, often
			during such analysis you will find yourself actually improving data
			by means of applying transformers and filters on it. When this is the
			case, obviously you will want to export the improved/cleansed data so
			you can utilize it in other situations than the analysis.
		</p>
		<p>
			Please refer to the reference chapter
			<a class="link" href="#writers" title="Chapter&nbsp;6.&nbsp;Write">Writers</a>
			for more information on writing cleansed data.
		</p>
	</div>

</div>
	</div>

	<div class="part" title="Part&nbsp;II.&nbsp;Analysis component reference"><div class="titlepage"><div><div><h1 class="title"><a name="d5e232"></a>Part&nbsp;II.&nbsp;Analysis component reference</h1></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="chapter"><a href="#transformations">3. Transform</a></span></dt><dd><dl><dt><span class="section"><a href="#javascript_transformer">JavaScript transformer</a></span></dt><dt><span class="section"><a href="#invoke_child_analysis_job">Invoke child Analysis job</a></span></dt><dt><span class="section"><a href="#equals_filter">Equals</a></span></dt><dt><span class="section"><a href="#max_rows_filter">Max rows</a></span></dt><dt><span class="section"><a href="#not_null_filter">Not null</a></span></dt><dt><span class="section"><a href="#union"> Union </a></span></dt></dl></dd><dt><span class="chapter"><a href="#improve">4. Improve</a></span></dt><dd><dl><dt><span class="section"><a href="#synonym_lookup_transformer">Synonym lookup</a></span></dt><dt><span class="section"><a href="#table_lookup_transformer">Table lookup</a></span></dt></dl></dd><dt><span class="chapter"><a href="#analyzers">5. Analyze</a></span></dt><dd><dl><dt><span class="section"><a href="#boolean_analyzer">Boolean analyzer</a></span></dt><dt><span class="section"><a href="#completeness_analyzer">Completeness analyzer</a></span></dt><dt><span class="section"><a href="#character_set_distribution">Character set distribution</a></span></dt><dt><span class="section"><a href="#date_gap_analyzer">Date gap analyzer</a></span></dt><dt><span class="section"><a href="#date_time_analyzer">Date/time analyzer</a></span></dt><dt><span class="section"><a href="#number_analyzer">Number analyzer</a></span></dt><dt><span class="section"><a href="#pattern_finder">Pattern finder</a></span></dt><dt><span class="section"><a href="#reference_data_matcher_analyzer">Reference data matcher</a></span></dt><dt><span class="section"><a href="#referential_integrity_analyzer">Referential integrity</a></span></dt><dt><span class="section"><a href="#string_analyzer">String analyzer</a></span></dt><dt><span class="section"><a href="#unique_key_check">Unique key check</a></span></dt><dt><span class="section"><a href="#value_distribution">Value distribution</a></span></dt><dt><span class="section"><a href="#value_matcher">Value matcher</a></span></dt><dt><span class="section"><a href="#weekday_distribution">Weekday distribution</a></span></dt></dl></dd><dt><span class="chapter"><a href="#writers">6. Write</a></span></dt><dd><dl><dt><span class="section"><a href="#writers_create_csv">Create CSV file</a></span></dt><dt><span class="section"><a href="#writers_create_excel">Create Excel spreadsheet</a></span></dt><dt><span class="section"><a href="#writers_create_staging">Create staging table</a></span></dt><dt><span class="section"><a href="#writers_insert_into_table">Insert into table</a></span></dt><dt><span class="section"><a href="#writers_update_table">Update table</a></span></dt></dl></dd></dl></div>
		
		<div class="chapter" title="Chapter&nbsp;3.&nbsp;Transform"><div class="titlepage"><div><div><h2 class="title"><a name="transformations"></a>Chapter&nbsp;3.&nbsp;Transform</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>With transformations you can pre- and postprocess your data as
				part
				of your DQ project.
			</p>
			<p>Technically speaking there are two kinds of transformations:
				Transformers and Filters. Transformers are used to extract, generate
				or refine data (new columns and sometimes also new rows), whereas
				filters are used to limit or split the dataset into separate
				processing streams.
			</p>
			<p>There's quite a lot of transformations available in
				DataCleaner,
				more than will be feasible to describe all in detail.
				This chapter
				provides a documentation for some of the essential ones.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#javascript_transformer">JavaScript transformer</a></span></dt><dt><span class="section"><a href="#invoke_child_analysis_job">Invoke child Analysis job</a></span></dt><dt><span class="section"><a href="#equals_filter">Equals</a></span></dt><dt><span class="section"><a href="#max_rows_filter">Max rows</a></span></dt><dt><span class="section"><a href="#not_null_filter">Not null</a></span></dt><dt><span class="section"><a href="#union"> Union </a></span></dt></dl></div>

	

	

	<div class="section" title="JavaScript transformer"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="javascript_transformer"></a>JavaScript transformer</h2></div></div></div>
		
		<p>The JavaScript transformer allows the user to define his/her own
			script which can
			perform rather intricate things like conditioning,
			looping. It can also be
			used as a
			way to express small business
			rules.
		</p>
		<p>For this documentation, a complete reference of JavaScript is
			out of scope. But we will show a few examples and more importantly
			talk about the available variables and their types.
		</p>
		<p>The JavaScript transformer returns a single string. The entered
			script should provide this string as the last line of the script.
			This is why the template script is as follows (so you can just
			implement the eval() function):
		</p>
		<pre class="programlisting">
			function&nbsp;eval() {
			&nbsp;&nbsp;&nbsp;return&nbsp;\"hello&nbsp;\"&nbsp;+&nbsp;values[0];
			}
			eval();
		</pre>

		<p>Variables:</p>
		<div class="table"><a name="d5e249"></a><p class="title"><b>Table&nbsp;3.1.&nbsp;JavaScript variables</b></p><div class="table-contents">
			
			<table summary="JavaScript variables" border="1"><colgroup><col><col></colgroup><thead><tr><th>Variable</th><th>Description</th></tr></thead><tbody><tr><td>values</td><td>
							<p>An array of all values in the row (as mapped by the
								"Columns" property).
							</p>
							<p>Using "values" you can reference eg. the first and third
								values like this:
							</p>
							<pre class="programlisting">
								var first = values[0];
								var third = values[2];
							</pre>
							<p>Note that JavaScript arrays are 0-based.</p>
							<p>Instead of indexes you can also reference by column name,
								like this:
							</p>
							<pre class="programlisting">
								var idValue = values["id"];
							</pre>

						</td></tr><tr><td>
							<span class="emphasis"><em>column_name</em></span>
							*
						</td><td>
							<p>
								Any column name that is also a valid JavaScript and
								<span class="emphasis"><em>not</em></span>
								a reserved variable name will also be added directly to the
								scope of the script as a variable. For example, if you have two
								columns, FIRST_NAME and LAST_NAME, you can concatenate them
								easily, like this:
							</p>
							<pre class="programlisting">var fullname = FIRST_NAME + " " + LAST_NAME;
							</pre>
						</td></tr><tr><td>out</td><td>
							<p>A reference to the system console's "out" stream. If
								running DataCleaner with the console visible, you can print
								messages to the console, like this:
							</p>
							<pre class="programlisting">out.println("Value: " + values[0]);
							</pre>
						</td></tr><tr><td>log</td><td>
							<p>A reference to the logging subsystem. Logging can be
								configured and log messages are stored in files, which makes it
								a bit more flexible than simply using "out". Here's how you
								write a few log messages with varying severities:
							</p>
							<pre class="programlisting">
								log.debug("This&nbsp;is&nbsp;a&nbsp;DEBUG&nbsp;message,&nbsp;it&nbsp;will&nbsp;probably&nbsp;be&nbsp;disregarded");
								log.info("This&nbsp;is&nbsp;a&nbsp;INFO&nbsp;message,&nbsp;it&nbsp;will&nbsp;probably&nbsp;be&nbsp;written&nbsp;to&nbsp;the&nbsp;logs");
								log.warn("This&nbsp;is&nbsp;a&nbsp;WARN&nbsp;message,&nbsp;it&nbsp;will&nbsp;most&nbsp;likely&nbsp;be&nbsp;written&nbsp;to&nbsp;the&nbsp;logs");
								log.error("This&nbsp;is&nbsp;a&nbsp;ERROR&nbsp;message,&nbsp;it&nbsp;will&nbsp;almost&nbsp;certainly&nbsp;be&nbsp;written&nbsp;to&nbsp;the&nbsp;logs");
							</pre>
						</td></tr></tbody></table>
		</div></div><br class="table-break">

		<p>Data types:</p>
		<div class="table"><a name="d5e284"></a><p class="title"><b>Table&nbsp;3.2.&nbsp;JavaScript data types</b></p><div class="table-contents">
			
			<table summary="JavaScript data types" border="1"><colgroup><col><col></colgroup><thead><tr><th>Data type</th><th>Description</th></tr></thead><tbody><tr><td>STRING</td><td>
							<p>String values are represented as JavaScript strings, which
								means that they have (among others) methods like:
							</p>
							<pre class="programlisting">
								var str = values[0];

								//&nbsp;get&nbsp;the&nbsp;length&nbsp;of&nbsp;a&nbsp;string
								var&nbsp;len&nbsp;=&nbsp;str.length();

								//&nbsp;uppercase&nbsp;variant&nbsp;of&nbsp;a&nbsp;string
								var&nbsp;up&nbsp;=&nbsp;str.toUpperCase();

								//&nbsp;lowercase&nbsp;variant&nbsp;of&nbsp;a&nbsp;string
								var&nbsp;lw&nbsp;=&nbsp;str.toLowerCase();
							</pre>
							<p>
								For more information, we recommend
								<a class="link" href="http://www.w3schools.com/jsref/jsref_obj_string.asp" target="_top">W3 schools JavaScript string reference</a>
								.
							</p>
						</td></tr><tr><td>NUMBER</td><td>
							<p>Numbers are treated as regular JavaScript numbers, which
								means that they have (among others) methods and operators like:
							</p>
							<pre class="programlisting">
								var&nbsp;num&nbsp;=&nbsp;values[0];

								//&nbsp;format&nbsp;with&nbsp;2&nbsp;decimals
								var&nbsp;formattedNumber&nbsp;=&nbsp;num.toFixed(2);

								//&nbsp;add,&nbsp;subtract,&nbsp;multiply&nbsp;or&nbsp;divide
								var&nbsp;m&nbsp;=&nbsp;(4&nbsp;+&nbsp;num&nbsp;*&nbsp;2&nbsp;-&nbsp;1)&nbsp;/&nbsp;2;
							</pre>
							<p>
								For more information, we recommend
								<a class="link" href="http://www.w3schools.com/jsref/jsref_obj_number.asp" target="_top">W3 schools JavaScript number reference</a>
								and also check out the
								<a class="link" href="http://www.w3schools.com/jsref/jsref_obj_math.asp" target="_top">Math function</a>
								reference.
							</p>
						</td></tr><tr><td>DATE</td><td>
							<p>Date values are treated as Java dates, which is a bit
								unusual, but leaves you with almost an identical interface as a
								regular JavaScript date. Here's a summary of typical methods:
							</p>
							<pre class="programlisting">
								var&nbsp;d&nbsp;=&nbsp;values[0];

								var&nbsp;year&nbsp;=&nbsp;d.getYear();
								var&nbsp;month&nbsp;=&nbsp;d.getMonth();
								var&nbsp;date&nbsp;=&nbsp;d.getDate();
								var&nbsp;hour&nbsp;=&nbsp;d.getHour();
								var&nbsp;minutes&nbsp;=&nbsp;d.getMinutes();
								var&nbsp;seconds&nbsp;=&nbsp;d.getSeconds();

								//&nbsp;milliseconds&nbsp;since&nbsp;1970-01-01
								var&nbsp;timestamp&nbsp;=&nbsp;d.getTime();
							</pre>
							<p>
								For a full reference, please look at the Java
								<a class="link" href="http://download.oracle.com/javase/6/docs/api/java/util/Date.html" target="_top">Date class reference</a>
								.
							</p>
						</td></tr><tr><td>BOOLEAN</td><td>Boolean (true/false) values are simply booleans, no sugar
							coating added :)
						</td></tr></tbody></table>
		</div></div><br class="table-break">
	</div>

	<div class="section" title="Invoke child Analysis job"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="invoke_child_analysis_job"></a>Invoke child Analysis job</h2></div></div></div>
		
		<p>With this transformation it is possible to compose a job that
			embeds functionality from another saved job.
		</p>
		<p>The job that is invoked will be stripped from it's analysis
			section, which means that only transformations remain. That way you
			can split up complex transformation flows into smaller pieces - each
			represented as a job file.
		</p>
		<p>
			To configure the transformation, select the columns from your
			current job's source which needs to be piped into the child job. The
			amount of columns selected needs to be the same as the amount of
			columns defined in the child job. This will be checked automatically,
			making it impossible to run the transformation otherwise.
		</p>
	</div>

	<div class="section" title="Equals"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="equals_filter"></a>Equals</h2></div></div></div>
		
		<p>
			The
			<span class="emphasis"><em>Equals</em></span>
			filter provides a way to make a simple filtering condition based on a
			white list / valid list of values. Simply enter a list of values that
			you accept for a given column, and then you can map your flow to the
			VALID outcome of the filter.
		</p>
		<p>Here's an example of an Equals filter configuration where valid
			Gender values are being checked.
		</p>
		<div class="mediaobject"><img src="filter_reference_equals.jpg"></div>
		<p>Use the plus/minus buttons to grow or shrink the list of values
			you want to accept.
		</p>
		<p>If placed as the first component in a flow, the Equals filter is
			optimizable in a way where it will modify your originating query.
			This means that it is also an appropriate filter to use if you just
			want to sample the data used in your job.
		</p>
	</div>

	<div class="section" title="Max rows"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="max_rows_filter"></a>Max rows</h2></div></div></div>
		
		<p>
			The
			<span class="emphasis"><em>Max rows</em></span>
			filter is used to limit the amount of records that
			are passed further
			on in the job's flow.
		</p>
		<p>If placed as the first component in a flow, the Max rows filter
			is
			optimizable in a way where it will modify your originating query.
			This means that it is also an appropriate filter to use if you just
			want to sample the data used in your job.
		</p>
	</div>

	<div class="section" title="Not null"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="not_null_filter"></a>Not null</h2></div></div></div>
		
		<p>
			The
			<span class="emphasis"><em>Not null</em></span>
			filter is a simple filter that can be used to exclude null values
			from your flow. Additionally you can select whether or not you want
			to accept empty strings ("") or not.
		</p>
		<p>If placed as the first component in a flow, the Not null filter
			is
			optimizable in a way where it will modify your originating query.
			This means that it is also an appropriate filter to use if you just
			want to sample the data used in your job.
		</p>
	</div>
	<div class="section" title="Union"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="union"></a> Union </h2></div></div></div>
		
		<p>
			The
			<span class="emphasis"><em> Union</em></span>
			transformer allows you to combine multiple streams into one.
			Providing what is equivalent to a union of tables.
			Use it to fuse data
			streams coming from different source
			tables. You can
			define new fields
			whose values represent whatever is
			available from
			one of the input
			streams.
		</p>

		<p>
			Below is an example of a job with the
			<span class="emphasis"><em>Union</em></span>
			transformation and a
			<a class="link" href="#what_is_datastore" title="What is a datastore?">composite datastore</a>
			. The composite datastore contains several data sources,
			each with customer information. The Union transformer in this
			example aims to combine tables of these data sources so that a
			single stream of data can be consumed with records from all the sources.
		</p>
		<div class="mediaobject"><img src="union_example_job.png"></div>
		<p>
			The configuration of the Union transformation is done by lining up
			column from the sources that should be combined. In the example shown
			'CUSTOMERNUMBER' and 'id' are combined into one new field. Similarly
			'CUSTOMERNAME' and 'family_name' is combined and so on.
		</p>
	</div>

</div>
		<div class="chapter" title="Chapter&nbsp;4.&nbsp;Improve"><div class="titlepage"><div><div><h2 class="title"><a name="improve"></a>Chapter&nbsp;4.&nbsp;Improve</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>The functions in the 'Improve' tree are all what we would
				describe as first class 'Data Quality functions'. They not only
				analyze a problem but also usually present a solution to it.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#synonym_lookup_transformer">Synonym lookup</a></span></dt><dt><span class="section"><a href="#table_lookup_transformer">Table lookup</a></span></dt></dl></div>

	

	

	<div class="section" title="Synonym lookup"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="synonym_lookup_transformer"></a>Synonym lookup</h2></div></div></div>
		
		<p>
			The Synonym lookup transformation is a critical part of DataCleaner's
			ability to standardize and cleanse data. Using this component you can
			look up values in a
			<a class="link" href="#synonym_catalogs" title="Chapter&nbsp;8.&nbsp;Synonyms (aka. Synonym catalogs)">synonym catalog</a>
			and have it replaced with its master term, if it is found to be a
			synonym.
		</p>
		<p>Below is a screenshot of the synonym lookup's configuration
			panel:
		</p>
		<div class="mediaobject"><img src="transformer_reference_synonym_lookup.png"></div>
		<p>
			The configuration of the Synonym lookup is simple:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>Select the column to apply the lookup function to.</p>
			</li><li class="listitem">
				<p>Use the 'Retain original value' option to determine if
					unmatched values (non-synonyms) should be retained or if a null
					value should be returned if there is no match.
				</p>
			</li><li class="listitem">
				<p>Select the synonym catalog to use for lookups.</p>
			</li></ol></div>

		<p>
			If your synonym catalog contains all the allowed values for a
			particula column, it can be a good idea to uncheck the 'Retain
			original value' checkbox and then do a simple null-check on the
			resulting output column. If null values are found, it's because there
			are values in the column that the synonym catalog is not able to
			standardize.
		</p>
	</div>
	
	<div class="section" title="Table lookup"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="table_lookup_transformer"></a>Table lookup</h2></div></div></div>
		
		<p>The table lookup transformer allows you to look up values in a
			different table. Any amount of columns can be used for mapping
			(lookup conditioning) and for outputting (the outcome of the lookup).
		</p>
		<p>The configuration screen for the table lookup transformer looks
			like this:
		</p>
		<div class="mediaobject"><img src="transformer_reference_table_lookup.png"></div>
		<p>To make the mapping you need to select the target datastore,
			schema and table names. Once selected you will be able to select
			which columns to use for condition setting when looking up values.
		</p>
		<p>The semantics of the Table lookup are close to the semantics
			of a
			LEFT JOIN. If no lookup value is found, nulls will be returned.
			However, if multiple records are found to match the conditions, only
			the first will be returned.
		</p>
		<p>Note that the Table lookup will use a cache for looking up
			values, to avoid querying the target table for every incoming value.
		</p>
	</div>

</div>
		<div class="chapter" title="Chapter&nbsp;5.&nbsp;Analyze"><div class="titlepage"><div><div><h2 class="title"><a name="analyzers"></a>Chapter&nbsp;5.&nbsp;Analyze</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>This chapter deals with one of the most important concepts in
				DataCleaner: Analysis of data quality.
			</p>
			<p>An analyzer is a component that consumes a (set of) column(s)
				and generates an
				analysis result based on the values in the consumed
				columns.
			</p>
			<p>Here is an example of a configuration panel pertaining to an
				analyzer:
			</p>
			<div class="mediaobject"><img src="analysis_job_04_analyzer.jpg"></div>
			<p>In the panel there will always be one or more selections of
				columns. The configuration panel may also contain additional
				properties for configuration.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#boolean_analyzer">Boolean analyzer</a></span></dt><dt><span class="section"><a href="#completeness_analyzer">Completeness analyzer</a></span></dt><dt><span class="section"><a href="#character_set_distribution">Character set distribution</a></span></dt><dt><span class="section"><a href="#date_gap_analyzer">Date gap analyzer</a></span></dt><dt><span class="section"><a href="#date_time_analyzer">Date/time analyzer</a></span></dt><dt><span class="section"><a href="#number_analyzer">Number analyzer</a></span></dt><dt><span class="section"><a href="#pattern_finder">Pattern finder</a></span></dt><dt><span class="section"><a href="#reference_data_matcher_analyzer">Reference data matcher</a></span></dt><dt><span class="section"><a href="#referential_integrity_analyzer">Referential integrity</a></span></dt><dt><span class="section"><a href="#string_analyzer">String analyzer</a></span></dt><dt><span class="section"><a href="#unique_key_check">Unique key check</a></span></dt><dt><span class="section"><a href="#value_distribution">Value distribution</a></span></dt><dt><span class="section"><a href="#value_matcher">Value matcher</a></span></dt><dt><span class="section"><a href="#weekday_distribution">Weekday distribution</a></span></dt></dl></div>

	

	

	<div class="section" title="Boolean analyzer"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="boolean_analyzer"></a>Boolean analyzer</h2></div></div></div>
		
		<p>Boolean analyzer is an analyzer targeted at boolean values. For
			a single boolean column it is quite simple: It will show the
			distribution of true/false (and optionally null) values in a column.
			For several columns it will also show the value combinations and the
			frequencies of the combinations. The combination matrix makes the
			Boolean analyzer a handy analyzer for use with combinations of
			matching transformers and other transformers that yield boolean
			values.
		</p>
		<p>Boolean analyzer has no configuration parameters, except for the
			input columns.
		</p>
	</div>

	<div class="section" title="Completeness analyzer"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="completeness_analyzer"></a>Completeness analyzer</h2></div></div></div>
		
		<p>The completeness analyzer provides a really simple way to check
			that all required fields in your records have been filled. Think of
			it like a big "not null" check across multiple fields. In combination
			with the monitoring application, this analyzer makes it easy to track
			which records needs additional information.
		</p>
		<p>Here is a screenshot of the configuration panel of the
			Completeness analyzer:
		</p>
		<div class="mediaobject"><img src="analyzer_reference_completeness_analyzer.jpg"></div>
		<p>The configuration properties of the Completeness analyzer are:
		</p>
		<div class="table"><a name="d5e408"></a><p class="title"><b>Table&nbsp;5.1.&nbsp;Completeness analyzer properties</b></p><div class="table-contents">
			
			<table summary="Completeness analyzer properties" border="1"><colgroup><col><col></colgroup><thead><tr><th>Property</th><th>Description</th></tr></thead><tbody><tr><td>Values</td><td>Select the columns you want to evaluate with your
							completeness analyzer. For each selected column you get to choose
							whether the analyzer should simply do a null-check, or if it
							should also check for blank values.
						</td></tr><tr><td>Evaluation mode</td><td>
							This determines the mode that the completeness check runs in.
							Here you can configure whether the analyzer should consider
							records as "incomplete" if
							<span class="emphasis"><em>any</em></span>
							of the selected values are null/blank, or if
							<span class="emphasis"><em>all</em></span>
							the values need to be null/blank before the record is counted as
							incomplete.
						</td></tr></tbody></table>
		</div></div><br class="table-break">
	</div>

	<div class="section" title="Character set distribution"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="character_set_distribution"></a>Character set distribution</h2></div></div></div>
		
		<p>The Character set distribution analyzer inspects and maps text
			characters according to character set affinity, such as Latin,
			Hebrew, Cyrillic, Chinese and more.
		</p>
		<p>Such analysis is convenient for getting insight into the
			international aspects of your data. Are you able to read and
			understand all your data? Will it work in your non-internationalized
			systems?
		</p>
	</div>

	<div class="section" title="Date gap analyzer"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="date_gap_analyzer"></a>Date gap analyzer</h2></div></div></div>
		
		<p>The Date gap analyzer is used to identify gaps in recorded time
			series. This analyzer is useful for example if you have employee time
			registration systems which record FROM and TO dates. It will allow
			you to identify if there are unexpected gaps in the data.
		</p>
	</div>

	<div class="section" title="Date/time analyzer"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="date_time_analyzer"></a>Date/time analyzer</h2></div></div></div>
		
		<p>The Date/time analyzer provides general purpose profiling
			metrics for temporal column types such as DATE, TIME and TIMESTAMP
			columns.
		</p>
	</div>

	<div class="section" title="Number analyzer"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="number_analyzer"></a>Number analyzer</h2></div></div></div>
		
		<p>The number analyzer provides general purpose profiling
			metrics
			for numerical column types.
		</p>
	</div>

	<div class="section" title="Pattern finder"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="pattern_finder"></a>Pattern finder</h2></div></div></div>
		
		<p>The pattern finder is one of the more advanced, but also very
			popular analyzers of DataCleaner.
		</p>
		<p>Here is a screenshot of the configuration panel of the Pattern
			finder:
		</p>
		<div class="mediaobject"><img src="analyzer_reference_pattern_finder.jpg"></div>
		<p>From the screenshot we can see that the Pattern finder has these
			configuration properties:
		</p>
		<div class="table"><a name="d5e445"></a><p class="title"><b>Table&nbsp;5.2.&nbsp;Pattern finder properties</b></p><div class="table-contents">
			
			<table summary="Pattern finder properties" border="1"><colgroup><col><col></colgroup><thead><tr><th>Property</th><th>Description</th></tr></thead><tbody><tr><td>Group column</td><td>Allows you to define a pattern group column. With a pattern
							group column you can separate the identified patterns into
							separate buckets/groups. Imagine for example that you want to
							check if the phone numbers of your customers are consistent. If
							you have an international customer based, you should then group
							by a country column to make sure that phone patterns identified
							are not matched with phone patterns from different countries.
						</td></tr><tr><td>Discriminate text case</td><td>Defines whether or not to discriminate (ie. consider as
							different pattern parts) based on text case. If true
							"DataCleaner" and "datacleaner" will be considered instances of
							different patterns, if false they will be matched within same
							pattern.
						</td></tr><tr><td>Discriminate negative numbers</td><td>When parsing numbers, this property defines if negative
							numbers should be discriminated from positive numbers.
						</td></tr><tr><td>Discriminate decimals</td><td>When parsing numbers, this property defines if decimal
							numbers should be discriminated from integers.
						</td></tr><tr><td>Enable mixed tokens</td><td>
							<p>Defines whether or not to categorize tokens that contain
								both letters and digits as "mixed", or alternatively as two
								separate tokens. Mixed tokens are represented using questionmark
								('?')
								symbols.
							</p>
							<p>This is one of the more important configuration properties.
								For example if mixed tokens are enabled (default), all these
								values will
								be matched against the same pattern: foo123, 123foo,
								foobar123, foo123bar. If mixed tokens are NOT enabled only
								foo123 and foobar123 will be matched (because 123foo and
								foo123bar represent different combinations of letter and digit
								tokens).
							</p>
						</td></tr><tr><td>Ignore repeated spaces</td><td>Defines whether or not to discriminate based on amount of
							whitespaces.
						</td></tr><tr><td>Upper case patterns expand in size</td><td>Defines whether or not upper case tokens automatically
							"expand" in size. Expandability refers to whether or not the
							found patterns will include matches if a candidate has the same
							type of token, but with a different size. The default
							configuration for upper case characters is false (ie. ABC is
							not
							matched with ABCD).
						</td></tr><tr><td>Lower case patterns expand in size</td><td>
							<p>Defines whether or not lower case tokens automatically
								"expand" in size. As with upper case expandability, this
								property
								refers to whether or not the found patterns will include
								matches
								if a candidate has the same type of token, but with a
								different
								size. The default configuration for lower case
								characters is
								true
								(ie. 'abc' is not matched with 'abc').
							</p>
							<p>The defaults in the two "expandability" configuration
								properties mean that eg. name pattern recognition is meaningful:
								'James' and 'John' both pertain to the same pattern ('Aaaaa'),
								while 'McDonald' pertain to a different pattern ('AaAaaaaa').
							</p>
						</td></tr><tr><td>Predefined token name</td><td>Predefined tokens make it possible to define a token to
							look for and classify using either just a fixed list of values or
							regular expressions. Typically this is used if the values contain
							some additional parts which you want to manually define a
							matching category for. The 'Predefined token name' property
							defines the name of such a category.
						</td></tr><tr><td>Predefined token regexes</td><td>Defines a number of string values and/or regular
							expressions which are used to match values against the
							(pre)defined token category.
						</td></tr><tr><td>Decimal separator</td><td>The decimal separator character, used when parsing numbers
						</td></tr><tr><td>Thousand separator</td><td>The thousand separator character, used when parsing numbers
						</td></tr><tr><td>Minus sign</td><td>The minus sign character, used when parsing numbers</td></tr></tbody></table>
		</div></div><br class="table-break">
	</div>

	<div class="section" title="Reference data matcher"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="reference_data_matcher_analyzer"></a>Reference data matcher</h2></div></div></div>
		
		<p>The 'Reference data matcher' analyzer provides an easy means to
			match several
			columns against several dictionaries and/or several
			string patterns.
			The result is a matrix of match information for all
			columns and all
			matched resources.
		</p>
	</div>

	<div class="section" title="Referential integrity"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="referential_integrity_analyzer"></a>Referential integrity</h2></div></div></div>
		
		<p>With the 'Referential integrity' analyzer you can check that key
			relationships between records are intact. The analyzer will work with
			relationships within a single table, between tables and even between
			tables of different datastores.
		</p>
		<p>Here is a screenshot of the configuration panel of the
			Referential integrity analyzer:
		</p>
		<div class="mediaobject"><img src="analyzer_reference_referential_integrity.jpg"></div>
		<p>
			Apply the analyzer on the table with the foreign key in the
			relationship, and configure it to do a check on the table that holds
			all the valid keys.
		</p>
		<div class="table"><a name="d5e507"></a><p class="title"><b>Table&nbsp;5.3.&nbsp;Referential integrity properties</b></p><div class="table-contents">
			
			<table summary="Referential integrity properties" border="1"><colgroup><col><col></colgroup><thead><tr><th>Property</th><th>Description</th></tr></thead><tbody><tr><td>Cache lookups</td><td>Whether or not the analyzer should speed up referential
							integrity checking by caching previous lookup results. Whether or
							not this will gain performance ultimately depends on the amount
							of repetition in the keys to be checked. If all foreign key
							values are more or less unique, it should definitely be turned
							off. But if there is a fair amount of duplication in the foreign
							keys (e.g. orderlines referring to the same products or
							customers), then it makes the lookups faster.
						</td></tr><tr><td>Ignore null values</td><td>Defines whether or not "null" values should be ignored or
							if they should be considered as an integrity issue. When ignored,
							all records with null foreign key values will simply be discarded
							by the analyzer.
						</td></tr></tbody></table>
		</div></div><br class="table-break">
	</div>

	<div class="section" title="String analyzer"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="string_analyzer"></a>String analyzer</h2></div></div></div>
		
		<p>The string analyzer provides general purpose profiling
			metrics
			for string column types. Of special concern to the string analyzer is
			the amount of words, characters, special signs, diacritics and other
			metrics that are vital to understanding what kind of string values
			occur in the data.
		</p>
	</div>

	<div class="section" title="Unique key check"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="unique_key_check"></a>Unique key check</h2></div></div></div>
		
		<p>The 'Unique key check' analyzer provides an easy way to verify
			that keys/IDs are unique - as it is usually expected.
		</p>
		<p>The properties of 'Unique key check' are:</p>
		<div class="table"><a name="d5e528"></a><p class="title"><b>Table&nbsp;5.4.&nbsp;Unique key check properties</b></p><div class="table-contents">
			
			<table summary="Unique key check properties" border="1"><colgroup><col><col></colgroup><thead><tr><th>Property</th><th>Description</th></tr></thead><tbody><tr><td>Column</td><td>Pick the column that this analyzer should perform the
							uniqueness check on.
						</td></tr><tr><td>Buffer size</td><td>The buffer represents the internal resource for sorting and
							comparison of keys. Having a large buffer makes the analyzer run
							faster and take up fewer resources on disk, but at the expense of
							using memory. If your job is not already memory intensive, we
							recommend increasing the buffer size up to 1M.
						</td></tr></tbody></table>
		</div></div><br class="table-break">
	</div>

	<div class="section" title="Value distribution"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="value_distribution"></a>Value distribution</h2></div></div></div>
		
		<p>The value distribution (often also referred to as 'Frequency
			analysis') allows you to identify all the values of a particular
			column. Furthermore you can investigate which rows pertain to
			specific values.
		</p>
		<p>Here are the configuration properties for the value distribution
			analyzer:
		</p>
		<div class="table"><a name="d5e546"></a><p class="title"><b>Table&nbsp;5.5.&nbsp;Value distribution properties</b></p><div class="table-contents">
			
			<table summary="Value distribution properties" border="1"><colgroup><col><col></colgroup><thead><tr><th>Property</th><th>Description</th></tr></thead><tbody><tr><td>Group column</td><td>Allows you to define a column for grouping the result. With
							a
							group column you can separate the identified value distributions
							into
							separate buckets/groups. Imagine for example that you want to
							check if the postal codes and city names correspond or if you
							just want to segment your value distribution on eg. country or
							gender or ...
						</td></tr><tr><td>Record unique values</td><td>By default all unique values will be included in the result
							of the value distribution. This can potentially cause memory
							issues if your analyzed columns contains a LOT of unique values
							(eg. if it's a unique key). If the actual unique values are not
							of interest, then uncheck this checkbox to only count (but not
							save for inspection) the unique values.
						</td></tr><tr><td>Top n most frequent vales</td><td>An optional number used if the analysis should only display
							eg. the "top 5 most frequent values". The result of the analysis
							will only contain top/bottom n most frequent values, if this
							property is supplied.
						</td></tr><tr><td>Bottom n most frequent values</td><td>An optional number used if the analysis should only display
							eg. the "bottom 5 most frequent values". The result of the
							analysis
							will only contain top/bottom n most frequent values, if
							this
							property is supplied.
						</td></tr></tbody></table>
		</div></div><br class="table-break">
	</div>

	<div class="section" title="Value matcher"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="value_matcher"></a>Value matcher</h2></div></div></div>
		
		<p>
			The value matcher works very similar to the
			<a class="link" href="#value_distribution" title="Value distribution">Value distribution</a>
			, except for the fact that it takes a list of expected values and
			everything else is put into a group of 'unexpected values'. This
			division of values means a couple of things:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>You get a built-in validation mechanism. You expect maybe only
					'M' and 'F' values for your 'gender' column, and everything else is
					in a sense invalid, since it is unexpected.
				</p>
			</li><li class="listitem">
				<p>The division makes it easier to monitor specific values in the
					data quality monitoring web application.
				</p>
			</li><li class="listitem">
				<p>This analyzer scales much better for large datasets, since the
					groupings are deterministic and thus can be prepared for in the
					batch run.
				</p>
			</li></ol></div>
	</div>

	<div class="section" title="Weekday distribution"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="weekday_distribution"></a>Weekday distribution</h2></div></div></div>
		
		<p>The weekday distribution provides a frequency analysis for date
			columns, where you can easily identify which weekdays a date field
			represents.
		</p>
	</div>

</div>
		<div class="chapter" title="Chapter&nbsp;6.&nbsp;Write"><div class="titlepage"><div><div><h2 class="title"><a name="writers"></a>Chapter&nbsp;6.&nbsp;Write</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>Although the primary focus of DataCleaner is analysis, often
				during such analysis you will find yourself actually improving data.
				When this is the
				case, obviously you will want to export the
				improved/cleansed data so
				you can utilize it in other situations than
				the analysis. In this chapter we will look at the various writing
				options that DataCleaner provide.
			</p>
			<div class="mediaobject"><img src="write_cleansed_data_options.jpg"></div>
			<p>In the following sections each output format option will be
				described:
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#writers_create_csv">Create CSV file</a></span></dt><dt><span class="section"><a href="#writers_create_excel">Create Excel spreadsheet</a></span></dt><dt><span class="section"><a href="#writers_create_staging">Create staging table</a></span></dt><dt><span class="section"><a href="#writers_insert_into_table">Insert into table</a></span></dt><dt><span class="section"><a href="#writers_update_table">Update table</a></span></dt></dl></div>

	
	

	<div class="section" title="Create CSV file"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="writers_create_csv"></a>Create CSV file</h2></div></div></div>
		
		<p>Writes a data set to an Comma Separated Values file.
			CSV files
			are
			a popular choise for interoperability with other
			systems and
			loading
			of data into databases.
		</p>
	</div>
	<div class="section" title="Create Excel spreadsheet"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="writers_create_excel"></a>Create Excel spreadsheet</h2></div></div></div>
		
		<p>Writes a data set to an Excel spreadsheet.
			An advantage of this
			approach is that a single file can contain
			multiple sheets, and that
			it is easily navigable in Microsoft
			Excel. A disadvantage is that
			for very large data sets it is less
			performant.
		</p>
	</div>
	<div class="section" title="Create staging table"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="writers_create_staging"></a>Create staging table</h2></div></div></div>
		
		<p>Writes a data set to an embedded relational database, which
			DataCleaner manages. This option is primarily used for staging data
			for further analysis. The advantage of using the feature is that it
			retains column type information, it can handle a lot of data and
			multiple data sets can be written to the same datastore. A
			disadvantage is that the data is not easily readable by third party
			applications (unless exported again).
		</p>
	</div>
	<div class="section" title="Insert into table"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="writers_insert_into_table"></a>Insert into table</h2></div></div></div>
		
		<p>Using this writer you can insert your data into a table of an
			existing datastore. If you already have a table layout ready or if
			you want to append to eg. a database table, then this writing
			option
			is the right one for you.
		</p>
		<p>
			Optionally, you can make the 'Insert into table' component truncate
			your table before insertion. This will delete all existing records in
			the table, useful for
			<span class="emphasis"><em>initial load</em></span>
			situations.
		</p>
		<div class="mediaobject"><img src="writers_reference_insert_into_table.jpg"></div>
		<p>
			Currently target tables can be from any of the following
			datastore types:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>
					<span class="emphasis"><em>CSV file</em></span>
					. In this case data will be appended to the file.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>Excel spreadsheet</em></span>
					. In this case data will be appended to the file.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>Relational database</em></span>
					. In this case data will be inserted to the table using an INSERT
					statement.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>MongoDB database</em></span>
					. In this case data will be inserted into the MongoDB collection.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>CouchDB database</em></span>
					. In this case data will be inserted into the CouchDB database.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>Salesforce.com</em></span>
					. In this case data will be uploaded/inserted into Salesforce.com
					using the SOQL web services.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>ElasticSearch index</em></span>
					. In this case data will be indexed into ElasticSearch.
				</p>
			</li></ol></div>
	</div>
	<div class="section" title="Update table"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="writers_update_table"></a>Update table</h2></div></div></div>
		
		<p>
			The 'Update table' writer works like the '
			<a class="link" href="#writers_insert_into_table" title="Insert into table">Insert into table</a>
			' writer except that it issues UPDATE statements instead of INSERT
			statements. This obviously means that it has an additional property,
			used to specify the condition (the WHERE part) of the update.
		</p>
	</div>
</div>
	</div>

	<div class="part" title="Part&nbsp;III.&nbsp;Reference data"><div class="titlepage"><div><div><h1 class="title"><a name="d5e633"></a>Part&nbsp;III.&nbsp;Reference data</h1></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="chapter"><a href="#reference_data_dictionaries">7. Dictionaries</a></span></dt><dt><span class="chapter"><a href="#synonym_catalogs">8. Synonyms (aka. Synonym catalogs)</a></span></dt><dd><dl><dt><span class="section"><a href="#reference_data_synonym_catalog_text">Text file synonym catalog</a></span></dt><dt><span class="section"><a href="#reference_data_synonym_catalog_datastore">Datastore synonym catalog</a></span></dt></dl></dd><dt><span class="chapter"><a href="#reference_data_string_patterns">9. String patterns</a></span></dt></dl></div>
		
		<div class="chapter" title="Chapter&nbsp;7.&nbsp;Dictionaries"><div class="titlepage"><div><div><h2 class="title"><a name="reference_data_dictionaries"></a>Chapter&nbsp;7.&nbsp;Dictionaries</h2></div></div></div>

	

	<p>Dictionaries are reference data lists used for verifying or
		categorizing values against certain black- or whitelists. Dictionaries
		are generally enumerable and finite, whereas eg. string patterns are
		dynamic and evaluated each time.
	</p>
	<p>Examples of meaningful dictionaries are:</p>
	<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
			<p>A dictionary of product types like "jewelry", "menswear",
				"sportswear" etc.
			</p>
		</li><li class="listitem">
			<p>A dictionary of gender symbols like "M", "F" and maybe
				"UNKNOWN".
			</p>
		</li><li class="listitem">
			<p>A dictionary of age group names (eg. infant, child, young,
				mature, senior)</p>
		</li><li class="listitem">
			<p>Two dictionaries for male and female given names (in order to
				determine gender of persons)</p>
		</li></ol></div>

</div>
		<div class="chapter" title="Chapter&nbsp;8.&nbsp;Synonyms (aka. Synonym catalogs)"><div class="titlepage"><div><div><h2 class="title"><a name="synonym_catalogs"></a>Chapter&nbsp;8.&nbsp;Synonyms (aka. Synonym catalogs)</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>Synonym catalogs are used to replace and standardize values to
				their master terms, in order to avoid multiple terms for the same
				real
				world thing.
			</p>

			<p>There are many real life examples of synonyms that make for
				messy
				data, for example:
			</p>

			<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					<p>Company and brand names, like "Coca-Cola", "Coca cola" and
						"Coke".
					</p>
				</li><li class="listitem">
					<p>Titles, like "Doctor", "Dr." and "Doc"</p>
				</li></ol></div>

			<p>In the following sections we will describe how to set up
				synonym
				catalogs that can be used in a variety of ways to standardize your
				database.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#reference_data_synonym_catalog_text">Text file synonym catalog</a></span></dt><dt><span class="section"><a href="#reference_data_synonym_catalog_datastore">Datastore synonym catalog</a></span></dt></dl></div>

	

	
	
	<div class="section" title="Text file synonym catalog"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="reference_data_synonym_catalog_text"></a>Text file synonym catalog</h2></div></div></div>
		
		<p>A text file synonym catalog is the easiest and often also the
			fastest way to perform synonym replacement. Simply create a text file
			with content in a format, where the master term is succeeded with a
			comma-separated list of synonyms, like this:
		</p>
		<pre class="programlisting">
			M,Male,Man,Guy,Boy
			F,Female,Woman,Girl
		</pre>
		<p>In the above example, most typical gender tokens will be
			replaced with either "M" or "F".
		</p>
	</div>

	<div class="section" title="Datastore synonym catalog"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="reference_data_synonym_catalog_datastore"></a>Datastore synonym catalog</h2></div></div></div>
		
		<p>If your synonyms are located in a database or another type of
			datastore, then you can also create synonym catalogs based on this.
		</p>
		<p>Datastore synonym catalogs allow you to specify a single master
			term column and multiple synonym columns. The synonym catalog will
			look then find synonym matches by searching/querying the datastore.
		</p>
	</div>
</div>
		<div class="chapter" title="Chapter&nbsp;9.&nbsp;String patterns"><div class="titlepage"><div><div><h2 class="title"><a name="reference_data_string_patterns"></a>Chapter&nbsp;9.&nbsp;String patterns</h2></div></div></div>

	

	<p>String patterns define a "template" for string values which they
		may or may not conform to.
	</p>
	<p>DataCleaner currently supports two type of popular string
		formats:
	</p>

	<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
			<p>
				<span class="emphasis"><em>Regular expressions</em></span>
				, which is a general purpose string pattern matching language
				popular in computer science. Regular expressions does take a bit of
				time to learn, but are very powerful once harnessed.
			</p>
			<p>
				Explaining the syntax of regular expressions is definitely
				outside
				the scope of the DataCleaner documentation. We recommend the
				<a class="link" href="http://docs.oracle.com/javase/tutorial/essential/regex/" target="_top">Java Regular Expressions lesson</a>
				if you are looking for a resource on this.
			</p>
		</li><li class="listitem">
			<p>
				<span class="emphasis"><em>Simple string patterns</em></span>
				, which use the same syntax as the Pattern finder analyzer. Patterns
				such as "aaaa@aaaa.aaa"
				could for example be used to match typical
				email addresses.
			</p>
		</li></ol></div>

</div>
	</div>

	<div class="part" title="Part&nbsp;IV.&nbsp;Configuration reference"><div class="titlepage"><div><div><h1 class="title"><a name="d5e683"></a>Part&nbsp;IV.&nbsp;Configuration reference</h1></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="chapter"><a href="#chapter_configuration_file">10. Configuration file</a></span></dt><dd><dl><dt><span class="section"><a href="#configuration_file_xml_schema">XML schema</a></span></dt><dt><span class="section"><a href="#configuration_file_datastores">Datastores</a></span></dt><dd><dl><dt><span class="section"><a href="#configuration_file_datastore_jdbc">Database (JDBC) connections</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_csv">Comma-Separated Values (CSV) files</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_fixed_width">Fixed width value files</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_excel">Excel spreadsheets</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_xml">XML file datastores</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_elasticsearch">ElasticSearch index</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_mongodb">MongoDB databases</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_couchdb">CouchDB databases</a></span></dt><dt><span class="section"><a href="#d5e767">Composite datastore</a></span></dt></dl></dd><dt><span class="section"><a href="#configuration_file_reference_data">Reference data</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e774">Dictionaries</a></span></dt><dt><span class="section"><a href="#d5e786">Synonym catalogs</a></span></dt><dt><span class="section"><a href="#d5e795">String patterns</a></span></dt></dl></dd><dt><span class="section"><a href="#configuration_file_task_runner">Task runner</a></span></dt><dt><span class="section"><a href="#configuration_file_storage_provider">Storage provider</a></span></dt></dl></dd><dt><span class="chapter"><a href="#d5e825">11. Analysis job files</a></span></dt><dd><dl><dt><span class="section"><a href="#job_files_xml_schema">XML schema</a></span></dt><dt><span class="section"><a href="#job_files_source_section">Source section</a></span></dt></dl></dd><dt><span class="chapter"><a href="#d5e853">12. Logging</a></span></dt><dd><dl><dt><span class="section"><a href="#logging_configuration_file">Logging configuration file</a></span></dt><dt><span class="section"><a href="#logging_default_configuration">Default logging configuration</a></span></dt><dt><span class="section"><a href="#logging_levels">Modifying logging levels</a></span></dt><dt><span class="section"><a href="#logging_alternative_outputs">Alternative logging outputs</a></span></dt></dl></dd><dt><span class="chapter"><a href="#d5e903">13. Database drivers</a></span></dt><dd><dl><dt><span class="section"><a href="#database_drivers_desktop">Installing Database drivers in DataCleaner desktop</a></span></dt></dl></dd></dl></div>
		
		<div class="chapter" title="Chapter&nbsp;10.&nbsp;Configuration file"><div class="titlepage"><div><div><h2 class="title"><a name="chapter_configuration_file"></a>Chapter&nbsp;10.&nbsp;Configuration file</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>
				In this chapter we go through the elements of a configuration
				file,
				<span class="emphasis"><em>conf.xml</em></span>
				, making it possible (although optional) to change the
				static
				configuration and configure the environment of DataCleaner. The
				configuration file and related files are stored by convention in a
				folder called
				<span class="emphasis"><em>.datacleaner</em></span>
				within your user's home directory.
			</p>
			<p>
				Most of the elements in the configuration file is also editable
				within the Desktop application. It is however important to note that
				changes made in the GUI are not saved directly to the configuration
				file, but to the
				<span class="emphasis"><em>userpreferences.dat</em></span>
				file. You can consider the relationship between the two files this
				way: The configuration file defines a static, unmodifyable prototype
				of the applications environment. All customizations made to this
				prototype in the Desktop application is saved in the
				userpreferences.dat file.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#configuration_file_xml_schema">XML schema</a></span></dt><dt><span class="section"><a href="#configuration_file_datastores">Datastores</a></span></dt><dd><dl><dt><span class="section"><a href="#configuration_file_datastore_jdbc">Database (JDBC) connections</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_csv">Comma-Separated Values (CSV) files</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_fixed_width">Fixed width value files</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_excel">Excel spreadsheets</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_xml">XML file datastores</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_elasticsearch">ElasticSearch index</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_mongodb">MongoDB databases</a></span></dt><dt><span class="section"><a href="#configuration_file_datastore_couchdb">CouchDB databases</a></span></dt><dt><span class="section"><a href="#d5e767">Composite datastore</a></span></dt></dl></dd><dt><span class="section"><a href="#configuration_file_reference_data">Reference data</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e774">Dictionaries</a></span></dt><dt><span class="section"><a href="#d5e786">Synonym catalogs</a></span></dt><dt><span class="section"><a href="#d5e795">String patterns</a></span></dt></dl></dd><dt><span class="section"><a href="#configuration_file_task_runner">Task runner</a></span></dt><dt><span class="section"><a href="#configuration_file_storage_provider">Storage provider</a></span></dt></dl></div>

	

	

	<div class="section" title="XML schema"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="configuration_file_xml_schema"></a>XML schema</h2></div></div></div>
		
		<p>The configuration file (conf.xml) is an XML file pertaining to
			the XML namespace
			"http://eobjects.org/analyzerbeans/configuration/1.0".
		</p>
		<p>
			For XML-savvy readers, who prefer to use XML schema aware editors to
			edit their XML files, you can find the XML schema for this namespace
			here:
			<a class="link" href="https://github.com/datacleaner/DataCleaner/blob/master/engine/xml-config/src/main/resources/configuration.xsd" target="_top">https://github.com/datacleaner/DataCleaner/blob/master/engine/xml-config/src/main/resources/configuration.xsd
			</a>
			.
		</p>
	</div>

	<div class="section" title="Datastores"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="configuration_file_datastores"></a>Datastores</h2></div></div></div>
		
		<p>Datastores can be configured in the configuration file under
			the
			element &lt;datastore-catalog&gt;. The following sections will go
			into further details with particular types of datastores.
		</p>

		<div class="section" title="Database (JDBC) connections"><div class="titlepage"><div><div><h3 class="title"><a name="configuration_file_datastore_jdbc"></a>Database (JDBC) connections</h3></div></div></div>
			
	
			<p>Here are a few examples of common database types.</p>
	
			<div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Tip</h3>
				<p>The DataCleaner User Interface makes it a lot easier to figure
					out the
					url (connection string) and driver class part of the
					connection
					properties. It's a good place to start if you don't know
					these
					properties already.
				</p>
			</div>
	
			<p>
				<span class="emphasis"><em>MySQL</em></span>
			</p>
			<pre lang="xml" class="programlisting">
				&lt;jdbc-datastore&nbsp;name="MySQL&nbsp;datastore"&gt;
				&nbsp;&lt;url&gt;jdbc:mysql://hostname:3306/database?defaultFetchSize=-2147483648&lt;/url&gt;
				&nbsp;&lt;driver&gt;com.mysql.jdbc.Driver&lt;/driver&gt;
				&nbsp;&lt;username&gt;username&lt;/username&gt;
				&nbsp;&lt;password&gt;password&lt;/password&gt;
				&nbsp;&lt;multiple-connections&gt;true&lt;/multiple-connections&gt;
				&lt;/jdbc-datastore&gt; </pre>
	
			<p>
				<span class="emphasis"><em>Oracle</em></span>
			</p>
			<pre lang="xml" class="programlisting">
				&lt;jdbc-datastore&nbsp;name="Oracle&nbsp;datastore"&gt;
				&nbsp;&lt;url&gt;jdbc:oracle:thin:@hostname:1521:sid&lt;/url&gt;
				&nbsp;&lt;driver&gt;oracle.jdbc.OracleDriver&lt;/driver&gt;
				&nbsp;&lt;username&gt;username&lt;/username&gt;
				&nbsp;&lt;password&gt;password&lt;/password&gt;
				&nbsp;&lt;multiple-connections&gt;true&lt;/multiple-connections&gt;
				&lt;/jdbc-datastore&gt; </pre>
	
			<p>
				<span class="emphasis"><em>Microsoft SQL Server</em></span>
			</p>
			<p>A typical connection to Microsoft SQL server will look like
				this:
			</p>
			<pre lang="xml" class="programlisting">
				&lt;jdbc-datastore&nbsp;name="MS&nbsp;SQL&nbsp;Server&nbsp;datastore"&gt;
				&nbsp;&lt;url&gt;jdbc:jtds:sqlserver://hostname/database;useUnicode=true;characterEncoding=UTF-8&lt;/url&gt;
				&nbsp;&lt;driver&gt;net.sourceforge.jtds.jdbc.Driver&lt;/driver&gt;
				&nbsp;&lt;username&gt;username&lt;/username&gt;
				&nbsp;&lt;password&gt;password&lt;/password&gt;
				&nbsp;&lt;multiple-connections&gt;true&lt;/multiple-connections&gt;
				&lt;/jdbc-datastore&gt; </pre>
			<p>However, if you want to use an instance name based connection,
				then the SQL Server Browser service MUST BE RUNNING and then you can
				include the instance parameter: Here's an example for connecting to a
				SQLEXPRESS instance:
			</p>
			<pre lang="xml" class="programlisting">
				&nbsp;&lt;url&gt;jdbc:jtds:sqlserver://hostname/database;instance=SQLEXPRESS;useUnicode=true;characterEncoding=UTF-8&lt;/url&gt;
			</pre>
		</div>
	
		<div class="section" title="Comma-Separated Values (CSV) files"><div class="titlepage"><div><div><h3 class="title"><a name="configuration_file_datastore_csv"></a>Comma-Separated Values (CSV) files</h3></div></div></div>
			
			<p>This is an example of a CSV file datastore</p>
			<pre class="programlisting">
				&lt;csv-datastore&nbsp;name="my_csv_file"&gt;
				&nbsp;&lt;filename&gt;/path/to/file.csv&lt;/filename&gt;
				&nbsp;&lt;quote-char&gt;"&lt;/quote-char&gt;
				&nbsp;&lt;separator-char&gt;;&lt;/separator-char&gt;
				&nbsp;&lt;encoding&gt;UTF-8&lt;/encoding&gt;
				&nbsp;&lt;fail-on-inconsistencies&gt;true&lt;/fail-on-inconsistencies&gt;
				&nbsp;&lt;header-line-number&gt;1&lt;/header-line-number&gt;
				&lt;/csv-datastore&gt; </pre>
		</div>
	
		<div class="section" title="Fixed width value files"><div class="titlepage"><div><div><h3 class="title"><a name="configuration_file_datastore_fixed_width"></a>Fixed width value files</h3></div></div></div>
			
			<p>Files with fixed width values can be registered in two ways -
				either with a single fixed-width size for all columns, or with
				individual value-widths.
			</p>
			<p>Here's an example with a fixed width specification for all columns:</p>
			<pre class="programlisting">
				&lt;fixed-width-datastore&nbsp;name="FIXED-WIDTH-ALL-COLUMNS"&gt;
				&nbsp;&lt;filename&gt;/path/to/the/file.txt&lt;/filename&gt;
				&nbsp;&lt;width-specification&gt;
				&nbsp;&nbsp;&lt;fixed-value-width&gt;20&lt;/fixed-value-width&gt;
				&nbsp;&lt;/width-specification&gt;
				&nbsp;&lt;encoding&gt;UTF-8&lt;/encoding&gt;
				&nbsp;&lt;header-line-number&gt;1&lt;/header-line-number&gt;
				&nbsp;&lt;fail-on-inconsistencies&gt;true&lt;/fail-on-inconsistencies&gt;
				&nbsp;&lt;skip-ebcdic-header&gt;false&lt;/skip-ebcdic-header&gt;
				&nbsp;&lt;eol-present&gt;true&lt;/eol-present&gt;
				&lt;/fixed-width-datastore&gt;
			</pre>
			<p>Here's an example with individual (2 columns) width specifications:</p>
			<pre class="programlisting">
				&lt;fixed-width-datastore&nbsp;name="FIXED-WIDTH-2-COLUMNS"&gt;
				&nbsp;&lt;filename&gt;/path/to/the/file.txt&lt;/filename&gt;
				&nbsp;&lt;width-specification&gt;
				&nbsp;&nbsp;&lt;value-width&gt;20&lt;/value-width&gt;
				&nbsp;&nbsp;&lt;value-width&gt;30&lt;/value-width&gt;
				&nbsp;&lt;/width-specification&gt;
				&nbsp;&lt;encoding&gt;UTF-8&lt;/encoding&gt;
				&nbsp;&lt;header-line-number&gt;1&lt;/header-line-number&gt;
				&nbsp;&lt;fail-on-inconsistencies&gt;true&lt;/fail-on-inconsistencies&gt;
				&nbsp;&lt;skip-ebcdic-header&gt;false&lt;/skip-ebcdic-header&gt;
				&nbsp;&lt;eol-present&gt;true&lt;/eol-present&gt;
				&lt;/fixed-width-datastore&gt;
			</pre>
			<p>Here's an example with an EBCDIC file:</p>
			<pre class="programlisting">
				&lt;fixed-width-datastore&nbsp;name="FIXED-WIDTH-EBCDIC"&gt;
				&nbsp;&lt;filename&gt;/path/to/the/file.ebc&lt;/filename&gt;
				&nbsp;&lt;width-specification&gt;
				&nbsp;&nbsp;&lt;value-width&gt;2&lt;/value-width&gt;
				&nbsp;&nbsp;&lt;value-width&gt;10&lt;/value-width&gt;
				&nbsp;&lt;/width-specification&gt;
				&nbsp;&lt;encoding&gt;IBM01148&lt;/encoding&gt;
				&nbsp;&lt;header-line-number&gt;0&lt;/header-line-number&gt;
				&nbsp;&lt;fail-on-inconsistencies&gt;true&lt;/fail-on-inconsistencies&gt;
				&nbsp;&lt;skip-ebcdic-header&gt;true&lt;/skip-ebcdic-header&gt;
				&nbsp;&lt;eol-present&gt;false&lt;/eol-present&gt;
				&lt;/fixed-width-datastore&gt;
			</pre>
		</div>
	
		<div class="section" title="Excel spreadsheets"><div class="titlepage"><div><div><h3 class="title"><a name="configuration_file_datastore_excel"></a>Excel spreadsheets</h3></div></div></div>
			
			<p>This is an example of an Excel spreadsheet datastore</p>
			<pre class="programlisting">
				&lt;excel-datastore&nbsp;name="my_excel_spreadsheet"&gt;
				&nbsp;&lt;filename&gt;/path/to/file.xls&lt;/filename&gt;
				&lt;/excel-datastore&gt; </pre>
		</div>
	
		<div class="section" title="XML file datastores"><div class="titlepage"><div><div><h3 class="title"><a name="configuration_file_datastore_xml"></a>XML file datastores</h3></div></div></div>
			
			<p>Defining XML datastores can be done in both a simple
				(automatically mapped) way, or an advanced (and more performant and
				memory effective way).
			</p>
			<p>The simple way is just to define a xml-datastore with a
				filename, like this:
			</p>
			<pre class="programlisting">
				&lt;xml-datastore&nbsp;name="my_xml_datastore"&gt;
				&nbsp;&lt;filename&gt;/path/to/file.xml&lt;/filename&gt;
				&lt;/xml-datastore&gt; </pre>
			<p>This kind of XML datastore works find when the file size is
				small and the hierarchy is not too complex. The downside to it is
				that it tries to automatically detect a table structure that is
				fitting to represent the XML contents (which is a tree structure, not
				really a table).
			</p>
			<p>To get around this problem you can also define your own table
				structure in which you specify the XPaths that make up your rows and
				the values within your rows. Here's an example:
			</p>
			<pre class="programlisting">
				&lt;xml-datastore&nbsp;name="my_xml_datastore"&gt;
				&nbsp;&lt;filename&gt;/path/to/file.xml&lt;/filename&gt;
				&nbsp;&lt;table-def&gt;
				&nbsp;&nbsp;&nbsp;&lt;rowXpath&gt;/greetings/greeting&lt;/rowXpath&gt;
				&nbsp;&nbsp;&nbsp;&lt;valueXpath&gt;/greetings/greeting/how&lt;/valueXpath&gt;
				&nbsp;&nbsp;&nbsp;&lt;valueXpath&gt;/greetings/greeting/what&lt;/valueXpath&gt;
				&nbsp;&lt;/table-def&gt;
				&lt;/xml-datastore&gt; </pre>
			<p>The datastore defines a single table, where each record is
				defined as the element which matches the XPath "/greetings/greeting".
				The table has two columns, which are represented by the "how" and
				"what" elements that are child elements to the row's path.
			</p>
			<p>
				For more details on the XPath expressions that define the table model
				of XML datastores, please refer to
				<a class="link" href="http://metamodel.eobjects.org/example_xml_mapping.html" target="_top">MetaModel's tutorial on the topic</a>
				(MetaModel is the data access library used to read data in
				DataCleaner).
			</p>
		</div>
	
		<div class="section" title="ElasticSearch index"><div class="titlepage"><div><div><h3 class="title"><a name="configuration_file_datastore_elasticsearch"></a>ElasticSearch index</h3></div></div></div>
			
			<p>This is an example of an ElasticSearch index datastore</p>
			<pre class="programlisting">
				&lt;elasticsearch-datastore&nbsp;name="my_elastic_search_index"&gt;
				&nbsp;&lt;hostname&gt;localhost&lt;/hostname&gt;
				&nbsp;&lt;port&gt;9300&lt;/port&gt;
				&nbsp;&lt;cluster-name&gt;my_es_cluster&lt;/cluster-name&gt;
				&nbsp;&lt;index-name&gt;my_index&lt;/index-name&gt;
				&lt;/elasticsearch-datastore&gt; </pre>
		</div>
	
		<div class="section" title="MongoDB databases"><div class="titlepage"><div><div><h3 class="title"><a name="configuration_file_datastore_mongodb"></a>MongoDB databases</h3></div></div></div>
			
			<p>This is an example of a fully specified MongoDB datastore, with
				an example table structure based on two collections.
			</p>
			<pre class="programlisting">
				&lt;mongodb-datastore&nbsp;name="my_mongodb_datastore"&gt;
				&nbsp;&lt;hostname&gt;localhost&lt;/hostname&gt;
				&nbsp;&lt;port&gt;27017&lt;/port&gt;
				&nbsp;&lt;database-name&gt;my_database&lt;/database-name&gt;
				&nbsp;&lt;username&gt;user&lt;/username&gt;
				&nbsp;&lt;password&gt;pass&lt;/password&gt;
				&nbsp;&lt;table-def&gt;
				&nbsp;&nbsp;&nbsp;&lt;collection&gt;company_collection&lt;/collection&gt;
				&nbsp;&nbsp;&nbsp;&lt;property&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;company_name&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;VARCHAR&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/property&gt;
				&nbsp;&nbsp;&nbsp;&lt;property&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;customer&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;BOOLEAN&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/property&gt;
				&nbsp;&nbsp;&nbsp;&lt;property&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;num_employees&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;INTEGER&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/property&gt;
				&nbsp;&nbsp;&nbsp;&lt;property&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;address_details&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;MAP&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/property&gt;
				&nbsp;&lt;/table-def&gt;
				&nbsp;&lt;table-def&gt;
				&nbsp;&nbsp;&nbsp;&lt;collection&gt;person_collection&lt;/collection&gt;
				&nbsp;&nbsp;&nbsp;&lt;property&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;person_name&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;VARCHAR&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/property&gt;
				&nbsp;&nbsp;&nbsp;&lt;property&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;birthdate&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;DATE&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/property&gt;
				&nbsp;&nbsp;&nbsp;&lt;property&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;emails&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;LIST&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/property&gt;
				&nbsp;&lt;/table-def&gt;
				&lt;/mongodb-datastore&gt; </pre>
			<p>If the hostname and port elements are left out, localhost:27017
				will be assumed.
			</p>
			<p>If the username and password elements are left out, an anonymous
				connection will be made.
			</p>
			<p>If there are no table-def elements, the database will be
				inspected and table definitions will be auto-detected based on the
				first 1000 documents of each collection.
			</p>
		</div>
	
		<div class="section" title="CouchDB databases"><div class="titlepage"><div><div><h3 class="title"><a name="configuration_file_datastore_couchdb"></a>CouchDB databases</h3></div></div></div>
			
			<p>This is an example of a fully specified CouchDB datastore, with
				an example table structure based on two CouchDB databases.
			</p>
			<pre class="programlisting">
				&lt;couchdb-datastore&nbsp;name="my_couchdb_datastore"&gt;
				&nbsp;&lt;hostname&gt;localhost&lt;/hostname&gt;
				&nbsp;&lt;port&gt;5984&lt;/port&gt;
				&nbsp;&lt;username&gt;user&lt;/username&gt;
				&nbsp;&lt;password&gt;pass&lt;/password&gt;
				&nbsp;&lt;ssl&gt;true&lt;/ssl&gt;
				&nbsp;&lt;table-def&gt;
				&nbsp;&nbsp;&nbsp;&lt;database&gt;company_collection&lt;/database&gt;
				&nbsp;&nbsp;&nbsp;&lt;field&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;company_name&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;VARCHAR&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/field&gt;
				&nbsp;&nbsp;&nbsp;&lt;field&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;customer&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;BOOLEAN&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/field&gt;
				&nbsp;&nbsp;&nbsp;&lt;field&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;num_employees&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;INTEGER&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/field&gt;
				&nbsp;&nbsp;&nbsp;&lt;field&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;address_details&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;MAP&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/field&gt;
				&nbsp;&lt;/table-def&gt;
				&nbsp;&lt;table-def&gt;
				&nbsp;&nbsp;&nbsp;&lt;database&gt;person_collection&lt;/database&gt;
				&nbsp;&nbsp;&nbsp;&lt;field&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;person_name&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;VARCHAR&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/field&gt;
				&nbsp;&nbsp;&nbsp;&lt;field&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;birthdate&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;DATE&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/field&gt;
				&nbsp;&nbsp;&nbsp;&lt;field&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;emails&lt;/name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;type&gt;LIST&lt;/type&gt;
				&nbsp;&nbsp;&nbsp;&lt;/field&gt;
				&nbsp;&lt;/table-def&gt;
				&lt;/couchdb-datastore&gt; </pre>
			<p>If the hostname and port elements are left out, localhost:5984
				will be assumed.
			</p>
			<p>If the username and password elements are left out, an anonymous
				connection will be made.
			</p>
			<p>If the "ssl" element is false or left out, a regular HTTP
				connection will be used.
			</p>
			<p>If there are no table-def elements, the database will be
				inspected and table definitions will be auto-detected based on the
				first 1000 documents of each database.
			</p>
		</div>
		<div class="section" title="Composite datastore"><div class="titlepage"><div><div><h3 class="title"><a name="d5e767"></a>Composite datastore</h3></div></div></div>
			
			<p> This is an example of a composite datastore. It contains data
				from 2 other datastores: Datastore 1 and Datastore 2.
			</p>
			<pre lang="xml" class="programlisting">
				&lt;composite-datastore name="my composite"&gt;
					&lt;datastore-name&gt;Datastore 1&lt;/datastore-name&gt;
					&lt;datastore-name&gt;Datastore 2&lt;/datastore-name&gt;
				&lt;/composite-datastore&gt;
			</pre>
		</div>
	</div>

	<div class="section" title="Reference data"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="configuration_file_reference_data"></a>Reference data</h2></div></div></div>
		
		<p>Reference data items (dictionaries, synonym catalogs and string patterns) are defined
		in the configuration file in the element &lt;reference-data-catalog&gt;. Below some examples:</p>
		
		<div class="section" title="Dictionaries"><div class="titlepage"><div><div><h3 class="title"><a name="d5e774"></a>Dictionaries</h3></div></div></div>
			
			<p>Dictionaries are stored within the &lt;dictionaries&gt; element within the reference data section. Three types of dictionaries can be added.</p>
			
			<p>
				<span class="emphasis"><em>Datastore dictionaries</em></span>
			</p>
			<pre lang="xml" class="programlisting">
				&lt;reference-data-catalog&gt;
				&nbsp;&nbsp;&lt;dictionaries&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;datastore-dictionary&nbsp;name="Lastnames"&nbsp;description="My&nbsp;datastore&nbsp;based&nbsp;dictionary"&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;datastore-name&gt;orderdb&lt;/datastore-name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;column-path&gt;EMPLOYEES.LASTNAME&lt;/column-path&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;/datastore-dictionary&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&lt;/dictionaries&gt;
				&lt;/reference-data-catalog&gt;
			</pre>
			<p>
				<span class="emphasis"><em>Text file dictionaries</em></span>
			</p>
			<pre lang="xml" class="programlisting">
				&lt;reference-data-catalog&gt;
				&nbsp;&nbsp;&lt;dictionaries&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;text-file-dictionary&nbsp;name="Firstnames"&nbsp;description="My&nbsp;file&nbsp;based&nbsp;dictionary"&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;filename&gt;/path/to/first.txt&lt;/filename&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;encoding&gt;UTF-8&lt;/encoding&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;/text-file-dictionary&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&lt;/dictionaries&gt;
				&lt;/reference-data-catalog&gt;
			</pre>
			<p>
				<span class="emphasis"><em>Value list dictionaries</em></span>
			</p>
			<pre lang="xml" class="programlisting">
				&lt;reference-data-catalog&gt;
				&nbsp;&nbsp;&lt;dictionaries&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;value-list-dictionary&nbsp;name="Greetings"&nbsp;description="My&nbsp;simple&nbsp;value&nbsp;list"&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;value&gt;hello&lt;/value&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;value&gt;hi&lt;/value&gt;	
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;value&gt;greetings&lt;/value&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;value&gt;godday&lt;/value&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;/value-list-dictionary&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&lt;/dictionaries&gt;
				&lt;/reference-data-catalog&gt;
			</pre>
		</div>
		<div class="section" title="Synonym catalogs"><div class="titlepage"><div><div><h3 class="title"><a name="d5e786"></a>Synonym catalogs</h3></div></div></div>
			
			<p>Synonym catalogs are stored within the &lt;synonym-catalogs&gt; element within the reference data section. Two types of dictionaries can be added.</p>
			<p>
				<span class="emphasis"><em>Text file synonym catalogs</em></span>
			</p>
			<pre lang="xml" class="programlisting">
				&lt;reference-data-catalog&gt;
				&nbsp;&nbsp;&lt;synonym-catalogs&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;text-file-synonym-catalog name="textfile_syn" description="My text file synonyms"&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;filename&gt;/path/to/synonyms.txt&lt;/filename&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;encoding&gt;UTF-8&lt;/encoding&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;case-sensitive&gt;false&lt;/case-sensitive&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;/text-file-synonym-catalog&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&lt;/synonym-catalogs&gt;
				&lt;/reference-data-catalog&gt;
			</pre>
			<p>
				<span class="emphasis"><em>Datastore synonym catalogs</em></span>
			</p>
			<pre lang="xml" class="programlisting">
				&lt;reference-data-catalog&gt;
				&nbsp;&nbsp;&lt;synonym-catalogs&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;datastore-synonym-catalog name="datastore_syn" description="My datastore synonyms"&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;datastore-name&gt;orderdb&lt;/datastore-name&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;master-term-column-path&gt;CUSTOMERS.CUSTOMERNAME&lt;/master-term-column-path&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;synonym-column-path&gt;CUSTOMERS.CUSTOMERNUMBER&lt;/synonym-column-path&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;synonym-column-path&gt;CUSTOMERS.PHONE&lt;/synonym-column-path&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;/datastore-synonym-catalog&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&lt;/synonym-catalogs&gt;
				&lt;/reference-data-catalog&gt;
			</pre>
		</div>
		<div class="section" title="String patterns"><div class="titlepage"><div><div><h3 class="title"><a name="d5e795"></a>String patterns</h3></div></div></div>
			
			<p>Dictionaries are stored within the &lt;string-patterns&gt; element within the reference data section. Two types of string patterns can be added.</p>
			<p>
				<span class="emphasis"><em>Regular expression (regex) string patterns</em></span>
			</p>
			<pre lang="xml" class="programlisting">
				&lt;reference-data-catalog&gt;
				&nbsp;&nbsp;&lt;string-patterns&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;regex-pattern name="regex danish email" description="Danish email addresses"&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;expression&gt;[a-z]+@[a-z]+\.dk&lt;/expression&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;match-entire-string&gt;true&lt;/match-entire-string&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;/regex-pattern&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&lt;/string-patterns&gt;
				&lt;/reference-data-catalog&gt;
			</pre>
			<p>
				<span class="emphasis"><em>Simple string patterns</em></span>
			</p>
			<pre lang="xml" class="programlisting">
				&lt;reference-data-catalog&gt;
				&nbsp;&nbsp;&lt;string-patterns&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;simple-pattern name="simple email" description="Simple email pattern"&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;expression&gt;aaaa@aaaaa.aa&lt;/expression&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;/simple-pattern&gt;
				&nbsp;&nbsp;&nbsp;...
				&nbsp;&nbsp;&lt;/string-patterns&gt;
				&lt;/reference-data-catalog&gt;
			</pre>
		</div>
	</div>

	<div class="section" title="Task runner"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="configuration_file_task_runner"></a>Task runner</h2></div></div></div>
		
		<p>The task runner defines how DataCleaner's engine will
			execute the tasks of an analysis job. Typically you shouldn't edit
			this element. However, here are the two options:
		</p>
		<pre class="programlisting">&lt;multithreaded-taskrunner max-threads="30" /&gt;
		</pre>
		<p>Defines a multi threaded task runner with a thread pool of 30
			available threads. Beware that although 30 might seem like a high
			number that too small a pool of threads might cause issues because
			some tasks schedule additional tasks and thus there's a risk of dead
			lock when thread count is very low.
		</p>

		<pre class="programlisting">&lt;singlethreaded-taskrunner /&gt;</pre>
		<p>Defines a single threaded task runner. On legacy hardware or
			operating systems this setting will be better, but it will not take
			advantage of the multi threading capabilities of modern architecture.
		</p>
	</div>

	<div class="section" title="Storage provider"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="configuration_file_storage_provider"></a>Storage provider</h2></div></div></div>
		
		<p>The storage provider is used for storing temporary data used
			while executing an analysis job. There are two types of storage:
			Large collections of (single) values and
			"annotated rows", ie. rows that have been sampled or marked with
			a specific category which will be of interest to the user to inspect.
		</p>
		<p>To explain the storage provider configuration let's look at the
			default element:
		</p>
		<pre class="programlisting">
			&lt;storage-provider&gt;
			&nbsp;&lt;combined&gt;
			&nbsp;&nbsp;&lt;collections-storage&gt;
			&nbsp;&nbsp;&nbsp;&lt;berkeley-db/&gt;
			&nbsp;&nbsp;&lt;/collections-storage&gt;
			&nbsp;&nbsp;&lt;row-annotation-storage&gt;
			&nbsp;&nbsp;&nbsp;&lt;in-memory&nbsp;max-rows-threshold="1000"&nbsp;max-sets-threshold="200"/&gt;
			&nbsp;&nbsp;&lt;/row-annotation-storage&gt;
			&nbsp;&lt;/combined&gt;
			&lt;/storage-provider&gt; </pre>
		<p>
			The element defines a combined storage strategy.
		</p>
		<p>
			Collections are stored using berkeley-db, an embedded database
			by Oracle. This is the recommended strategy for collections.
		</p>
		<p>
			Row annotations are stored in memory. There's a threshold of
			1000 rows in maximum 200 sets. This means that if more than 1000 records are annotated
			with the same category then additional records will not be saved (and
			thus is not viewable by the user). Furthermore it means that only up until 200 sample
			sets will be saved. Further annotations will not be sampled, but metrics still be counted.
			Most user scenarios will not
			require more than max. 1000 annotated records for inspection, but if
			this is really neccessary a different strategy can be pursued:
		</p>

		<div class="sect2" title="Using MongoDB for annotated rows"><div class="titlepage"><div><div><h3 class="title"><a name="d5e819"></a>Using MongoDB for annotated rows</h3></div></div></div>
			
			<p>
				If you have a local
				<a class="link" href="http://www.mongodb.org/" target="_top">MongoDB</a>
				instance, you can use this as a store for annotated rows. This is
				how the configuration looks like:
			</p>
			<pre class="programlisting">
				&nbsp;&nbsp;&lt;row-annotation-storage&gt;
				&nbsp;&nbsp;&nbsp;&lt;custom-storage-provider&nbsp;class-name="org.datacleaner.storage.MongoDbStorageProvider"/&gt;
				&nbsp;&nbsp;&lt;/row-annotation-storage&gt; </pre>
			<p>The MongoDB storage provider solution has shown very good
				performance metrics, but does add more complexity to the
				installation, which is why it is still considered experimental and
				only for savvy users.
			</p>
		</div>
	</div>
</div>
		<div class="chapter" title="Chapter&nbsp;11.&nbsp;Analysis job files"><div class="titlepage"><div><div><h2 class="title"><a name="d5e825"></a>Chapter&nbsp;11.&nbsp;Analysis job files</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>
				Job files contain the information about the execution of a
				DataCleaner job. Typically these files have the file extension
				<span class="emphasis"><em>.analysis.xml</em></span>
				. In this file we will explain the file format, which is XML based,
				and explain how it relates to what DataCleaner does.
			</p>
			<p>
				A job will always reference items in a configuration, such as
				datastores, reference data and more. Therefore a job alone is not
				enough to execute. But multiple jobs can use the same configuration.
				For more information on the configuration, see the
				<a class="link" href="#chapter_configuration_file" title="Chapter&nbsp;10.&nbsp;Configuration file">configuration file</a>
				chapter.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#job_files_xml_schema">XML schema</a></span></dt><dt><span class="section"><a href="#job_files_source_section">Source section</a></span></dt></dl></div>

	

	

	<div class="section" title="XML schema"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="job_files_xml_schema"></a>XML schema</h2></div></div></div>
		
		<p>Analysis job files are written in an XML format pertaining to
			the XML namespace
			"http://eobjects.org/analyzerbeans/job/1.0".
		</p>
		<p>
			For XML-savvy readers, who prefer to use XML schema aware editors to
			edit their XML files, you can find the XML schema for this namespace
			here:
			<a class="link" href="https://github.com/datacleaner/DataCleaner/blob/master/engine/xml-config/src/main/resources/job.xsd" target="_top">https://github.com/datacleaner/DataCleaner/blob/master/engine/xml-config/src/main/resources/job.xsd
			</a>
			.
		</p>
		<p>Read on in this chapter for notes on individual parts of the job
			file format.
		</p>
	</div>

	<div class="section" title="Source section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="job_files_source_section"></a>Source section</h2></div></div></div>
		
		<p>The source section of the job file format is probably the most
			interesting one to manually edit or review. Here's an example source
			section:
		</p>
		<pre class="programlisting">
			&lt;source&gt;
			&nbsp;&nbsp;&lt;data-context&nbsp;ref="orderdb"&nbsp;/&gt;
			&nbsp;&nbsp;&lt;columns&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;column&nbsp;path="PUBLIC.EMPLOYEES.EMPLOYEENUMBER"&nbsp;id="col_0"&nbsp;type="INTEGER"&nbsp;/&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;column&nbsp;path="PUBLIC.EMPLOYEES.LASTNAME"&nbsp;id="col_1"&nbsp;type="VARCHAR"&nbsp;/&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;column&nbsp;path="PUBLIC.EMPLOYEES.FIRSTNAME"&nbsp;id="col_2"&nbsp;type="VARCHAR"&nbsp;/&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;column&nbsp;path="PUBLIC.EMPLOYEES.EXTENSION"&nbsp;id="col_3"&nbsp;type="VARCHAR"&nbsp;/&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;column&nbsp;path="PUBLIC.EMPLOYEES.EMAIL"&nbsp;id="col_4"&nbsp;type="VARCHAR"&nbsp;/&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;column&nbsp;path="PUBLIC.EMPLOYEES.OFFICECODE"&nbsp;id="col_5"&nbsp;type="VARCHAR"&nbsp;/&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;column&nbsp;path="PUBLIC.EMPLOYEES.REPORTSTO"&nbsp;id="col_6"&nbsp;type="INTEGER"&nbsp;/&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;column&nbsp;path="PUBLIC.EMPLOYEES.JOBTITLE"&nbsp;id="col_7"&nbsp;type="VARCHAR"&nbsp;/&gt;
			&nbsp;&nbsp;&lt;/columns&gt;
			&nbsp;&nbsp;&lt;variables&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;variable&nbsp;id="employee&nbsp;type"&nbsp;value="Sales&nbsp;Rep"&nbsp;/&gt;
			&nbsp;&nbsp;&lt;/variables&gt;
			&lt;/source&gt; </pre>
		<p>
			From this source section we can derive these interesting facts:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>
					The job is using the datastore called 'orderdb'. How this datastore
					is configured, the job is not aware of, since it is defined in the
					<span class="emphasis"><em>configuration</em></span>
					. Potentially the job could be used with multiple similar
					datastores, as long as their name was 'orderdb'.
				</p>
			</li><li class="listitem">
				<p>
					The columns defined make up the base of the source query that
					the job will fire. Each column is assigned an artificial ID, and a
					hint about it's data type is provided. This information is there to
					be able to detach or replace a column with a new definition. That
					means that if you've spend a long time building the perfect job,
					but want to apply it to a different column, you can potentially
					"just" change the column definition here and retain the original
					column ID.
				</p>
			</li><li class="listitem">
				<p>
					In this source section we also see some variables. This is an
					optional sub-section and not that common. The variables are
					property values that can be replaced at runtime with new values.
					See the chapter
					<a class="link" href="#section_parameterizable_jobs" title="Parameterizable jobs">Parameterizable jobs</a>
					for more information and examples.
				</p>
			</li></ol></div>
	</div>
</div>
		<div class="chapter" title="Chapter&nbsp;12.&nbsp;Logging"><div class="titlepage"><div><div><h2 class="title"><a name="d5e853"></a>Chapter&nbsp;12.&nbsp;Logging</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>
				Logging in DataCleaner is configurable either by supplying an
				XML file or a properties file. In this chapter we explore logging
				configuration and how you can fine-tune logging to your needs.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#logging_configuration_file">Logging configuration file</a></span></dt><dt><span class="section"><a href="#logging_default_configuration">Default logging configuration</a></span></dt><dt><span class="section"><a href="#logging_levels">Modifying logging levels</a></span></dt><dt><span class="section"><a href="#logging_alternative_outputs">Alternative logging outputs</a></span></dt></dl></div>

	

	

	<div class="section" title="Logging configuration file"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="logging_configuration_file"></a>Logging configuration file</h2></div></div></div>
		
		<p>Logging in DataCleaner is based on Log4j, an open source logging
			framework by the Apache foundation. With log4j you can configure
			logging at a very detailed level, while at the same time keeping a
			centralized configuration.
		</p>
		<p>There are three approaches to configuring logging in
			DataCleaner:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>
					<span class="emphasis"><em>The default logging configuration</em></span>
					. This requires no changes to the standard distribution of
					DataCleaner. Log files will be generated in the log/datacleaner.log
					file.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>Specifying your own XML log configuration</em></span>
					. This requires you to put a file named log4j.xml in the root
					directory of DataCleaner.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>Specifying your own property file log configuration
					</em></span>
					. This requires you to put a file named log4j.properties in the
					root directory of DataCleaner.
				</p>
			</li></ol></div>

		<p>
			The recommended way of doing custom configuration of DataCleaner
			logging is using the XML format. In the following sections we will
			explain this approach using examples. For more detailed documentation
			on Log4j configuration, please refer to the
			<a class="link" href="http://logging.apache.org/log4j/" target="_top">Log4j website</a>
			.
		</p>
	</div>

	<div class="section" title="Default logging configuration"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="logging_default_configuration"></a>Default logging configuration</h2></div></div></div>
		
		<p>Here's a listing of the default logging configuration, in XML
			format:
		</p>
		<pre lang="xml" class="programlisting">
			&lt;?xml&nbsp;version="1.0"
			encoding="UTF-8"&nbsp;?&gt;
			&lt;!DOCTYPE&nbsp;log4j:configuration&nbsp;SYSTEM
			"log4j.dtd"&gt;
			&lt;log4j:configuration&nbsp;xmlns:log4j="http://jakarta.apache.org/log4j/"&gt;

			&nbsp;&lt;appender&nbsp;name="consoleAppender"&nbsp;class="org.apache.log4j.ConsoleAppender"&gt;
			&nbsp;&nbsp;&lt;param&nbsp;name="Target"&nbsp;value="System.out"/&gt;
			&nbsp;&nbsp;&lt;layout&nbsp;class="org.apache.log4j.PatternLayout"&gt;
			&nbsp;&nbsp;&nbsp;&lt;param&nbsp;name="ConversionPattern"&nbsp;value="%-5p&nbsp;%d{HH:mm:ss}&nbsp;%c{1}&nbsp;-&nbsp;%m%n"/&gt;
			&nbsp;&nbsp;&lt;/layout&gt;
			&nbsp;&nbsp;&lt;filter&nbsp;class="org.apache.log4j.varia.LevelRangeFilter"&gt;
			&nbsp;&nbsp;&nbsp;&lt;param&nbsp;name="levelMin"&nbsp;value="WARN"/&gt;
			&nbsp;&nbsp;&lt;/filter&gt;
			&nbsp;&lt;/appender&gt;

			&nbsp;&lt;appender&nbsp;name="fileAppender"&nbsp;class="org.apache.log4j.DailyRollingFileAppender"&gt;
			&nbsp;&nbsp;&lt;param&nbsp;name="File"&nbsp;value="${user.home}/.datacleaner/log/datacleaner.log"/&gt;
			&nbsp;&nbsp;&lt;param&nbsp;name="DatePattern"&nbsp;value="'.'yyyy-MM-dd'.log'"/&gt;
			&nbsp;&nbsp;&lt;layout&nbsp;class="org.apache.log4j.PatternLayout"&gt;
			&nbsp;&nbsp;&nbsp;&lt;param&nbsp;name="ConversionPattern"&nbsp;value="%-5p&nbsp;%d{HH:mm:ss.SSS}&nbsp;%c{1}&nbsp;-&nbsp;%m%n"/&gt;
			&nbsp;&nbsp;&lt;/layout&gt;
			&nbsp;&lt;/appender&gt;

			&nbsp;&lt;logger&nbsp;name="org.apache.metamodel"&gt;
			&nbsp;&nbsp;&lt;level&nbsp;value="info"/&gt;
			&nbsp;&lt;/logger&gt;

			&nbsp;&lt;logger&nbsp;name="org.datacleaner"&gt;
			&nbsp;&nbsp;&lt;level&nbsp;value="info"/&gt;
			&nbsp;&lt;/logger&gt;

			&nbsp;&lt;root&gt;
			&nbsp;&nbsp;&lt;priority&nbsp;value="warn"/&gt;
			&nbsp;&nbsp;&lt;appender-ref&nbsp;ref="consoleAppender"/&gt;
			&nbsp;&nbsp;&lt;appender-ref&nbsp;ref="fileAppender"/&gt;
			&nbsp;&lt;/root&gt;

			&lt;/log4j:configuration&gt;</pre>
		<p>This logging configuration specifies the INFO level as the
			default level of logging. It appends (outputs) log messages to the
			console (if available) and to a file with the path:
			${user.home}.datacleaner/log/datacleaner.log
		</p>
		<p>We recommend using this default configuration as a template for
			custom log configurations. Next we will explore how to modify the
			configuration and create new logging outputs.
		</p>
	</div>

	<div class="section" title="Modifying logging levels"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="logging_levels"></a>Modifying logging levels</h2></div></div></div>
		
		<p>These are the logging levels available in DataCleaner and Log4j,
			order by priority (highest priority first):
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				error
			</li><li class="listitem">
				warn
			</li><li class="listitem">
				info
			</li><li class="listitem">
				debug
			</li><li class="listitem">
				trace
			</li></ol></div>
		<p>Typically the bottom-two logging levels (debug and trace) are
			not used unless unexpected situations has to be investigated by
			developers.
		</p>
		<p>
			Modifying the logging levels can be done either globally or in a
			hierarchical manner:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>If you change the &lt;priority&gt; element's value attribute,
					you change the global threshold for logging messages.
				</p>
			</li><li class="listitem">
				<p>If you change the &lt;logger&gt; element's level,
					you change
					the logging priority logging messages that pertain to a particular
					hierarchy of loggers.
				</p>
			</li></ol></div>
	</div>

	<div class="section" title="Alternative logging outputs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="logging_alternative_outputs"></a>Alternative logging outputs</h2></div></div></div>
		
		<p>Log
			messages are printed to an output, typically a file or the
			console. In
			the configuration file this is configured in the
			&lt;appender&gt;
			elements. Here's a few examples of alternative
			appenders you can use.
			For more examples and documentation, please
			refer to the
			<a class="link" href="http://logging.apache.org/log4j/" target="_top">Log4j website</a>
			.
		</p>

		<p>
			<span class="emphasis"><em>Logging in a PostgreSQL database:</em></span>
		</p>
		<pre lang="xml" class="programlisting">
			&lt;appender&nbsp;name="jdbcAppender"&nbsp;class="org.apache.log4j.jdbc.JDBCAppender"&gt;
			&nbsp;&lt;param&nbsp;name="URL"&nbsp;value="jdbc:postgresql:db"/&gt;
			&nbsp;&lt;param&nbsp;name="Driver"&nbsp;value="org.postgresql.Driver"/&gt;
			&nbsp;&lt;param&nbsp;name="User"&nbsp;value="user"/&gt;
			&nbsp;&lt;param&nbsp;name="Password"&nbsp;value="password"/&gt;
			&nbsp;&lt;layout class="org.apache.log4j.PatternLayout"&gt;
			&nbsp;&nbsp;&lt;param&nbsp;name="ConversionPattern"
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value="INSERT&nbsp;INTO&nbsp;log4j&nbsp;(log_date,log_level,log_location,log_message)&nbsp;VALUES&nbsp;('%d{yyyy-MM-dd}','%p','%C;%L','%m')"/&gt;
			&nbsp;&lt;/layout&gt;
			&lt;/appender&gt;</pre>
	</div>

</div>
		<div class="chapter" title="Chapter&nbsp;13.&nbsp;Database drivers"><div class="titlepage"><div><div><h2 class="title"><a name="d5e903"></a>Chapter&nbsp;13.&nbsp;Database drivers</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>
				DataCleaner ships with a set of standard database drivers,
				enabling you to connect to common databases such as PostgreSQL,
				MySQL, Microsoft SQL Server or Oracle.
			</p>
			<p>It's also not uncommon to wish to install additional drivers
				for other database brands. DataCleaner uses the JDBC standard for
				managing database drivers and connection. In this chapter we cover
				the process of installing additional database drivers.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#database_drivers_desktop">Installing Database drivers in DataCleaner desktop</a></span></dt></dl></div>

	

	

	<div class="section" title="Installing Database drivers in DataCleaner desktop"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="database_drivers_desktop"></a>Installing Database drivers in DataCleaner desktop</h2></div></div></div>
		

		<p>Installing database drivers in DataCleaner desktop is done in
			the application itself while it is running.
		</p>
		<p>First, go to the Options menu:</p>
		<div class="mediaobject"><img src="database_drivers_desktop_menu.png"></div>
		<p>Then select the 'Database drivers' tab. In this tab you will see
			a listing of your currently installed database drivers (that we know
			of):
		</p>
		<div class="mediaobject"><img src="database_drivers_desktop_listing.png"></div>
		<p>If you click the 'Add database driver' button and then select
			the 'Local JAR file(s)' option, you will see this dialog:
		</p>
		<div class="mediaobject"><img src="database_drivers_desktop_dialog.png"></div>
		<p>In this dialog, select driver class name and the files
			containing the database driver. If you don't know what this is please
			refer to your database vendor's documentation on JDBC database
			drivers.
		</p>

		<p>In the example above we see the IBM
			DB2 driver which involves
			installing two files since the software
			license also has to be
			included.
		</p>

	</div>
</div>
	</div>

	<div class="part" title="Part&nbsp;V.&nbsp;Invoking DataCleaner jobs"><div class="titlepage"><div><div><h1 class="title"><a name="d5e926"></a>Part&nbsp;V.&nbsp;Invoking DataCleaner jobs</h1></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="chapter"><a href="#command_line_interface">14. Command-line interface</a></span></dt><dd><dl><dt><span class="section"><a href="#cli_executables">Executables</a></span></dt><dt><span class="section"><a href="#cli_usage_scenarios">Usage scenarios</a></span></dt><dt><span class="section"><a href="#cli_executing_a_job">Executing an analysis job</a></span></dt><dt><span class="section"><a href="#cli_listing">Listing datastore contents and available components</a></span></dt><dt><span class="section"><a href="#section_parameterizable_jobs">Parameterizable jobs</a></span></dt><dt><span class="section"><a href="#cli_overriding_configuration_elements">Dynamically overriding configuration elements</a></span></dt></dl></dd><dt><span class="chapter"><a href="#hadoop_interface">15. Apache Hadoop and Spark interface</a></span></dt><dd><dl><dt><span class="section"><a href="#hadoop_deployment">Hadoop deployment overview</a></span></dt><dt><span class="section"><a href="#hadoop_spark_setup">Setting up Spark and DataCleaner environment</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e1052">Upload configuration file to HDFS</a></span></dt><dt><span class="section"><a href="#d5e1065">Upload job file to HDFS</a></span></dt><dt><span class="section"><a href="#d5e1079">Upload executables to HDFS</a></span></dt></dl></dd><dt><span class="section"><a href="#hadoop_launch">Launching DataCleaner jobs using Spark</a></span></dt><dt><span class="section"><a href="#hadoop_desktop">Using Hadoop in DataCleaner desktop</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e1123">Configure Hadoop clusters</a></span></dt><dt><span class="section"><a href="#d5e1150">CSV datastores on HDFS</a></span></dt></dl></dd><dt><span class="section"><a href="#hadoop_limitations">Limitations of the Hadoop interface</a></span></dt></dl></dd></dl></div>
		
		<div class="chapter" title="Chapter&nbsp;14.&nbsp;Command-line interface"><div class="titlepage"><div><div><h2 class="title"><a name="command_line_interface"></a>Chapter&nbsp;14.&nbsp;Command-line interface</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>DataCleaner offers a Command-Line Interface (CLI) for
				performing
				various tasks, including executing analysis jobs, via simple commands
				that can be invoked eg. as a scheduled tasks.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#cli_executables">Executables</a></span></dt><dt><span class="section"><a href="#cli_usage_scenarios">Usage scenarios</a></span></dt><dt><span class="section"><a href="#cli_executing_a_job">Executing an analysis job</a></span></dt><dt><span class="section"><a href="#cli_listing">Listing datastore contents and available components</a></span></dt><dt><span class="section"><a href="#section_parameterizable_jobs">Parameterizable jobs</a></span></dt><dt><span class="section"><a href="#cli_overriding_configuration_elements">Dynamically overriding configuration elements</a></span></dt></dl></div>

	

	
	
	<div class="section" title="Executables"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cli_executables"></a>Executables</h2></div></div></div>
		
		<p>Depending on your distribution of DataCleaner, you will have one
			of these CLI executables included:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>
					<span class="emphasis"><em>datacleaner-console.exe</em></span>
					, which is a Windows-only executable.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>datacleaner.cmd</em></span>
					, which is a script to start DataCleaner in Windows.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>datacleaner.sh</em></span>
					, which is a script to start DataCleaner in Unix-like systems, like
					Linux and Mac OS.
				</p>
			</li><li class="listitem">
				<p>If you're running DataCleaner in Java Webstart mode, then
					there is no Command-Line Interface!
				</p>
			</li></ol></div>
	</div>

	<div class="section" title="Usage scenarios"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cli_usage_scenarios"></a>Usage scenarios</h2></div></div></div>
		
		<p>The usage scenarios of DataCleaner's CLI are:</p>

		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>Executing an analysis job</p>
			</li><li class="listitem">
				<p>List registered datastores</p>
			</li><li class="listitem">
				<p>List schemas in a datastore</p>
			</li><li class="listitem">
				<p>List tables in a schema</p>
			</li><li class="listitem">
				<p>List columns in a table</p>
			</li><li class="listitem">
				<p>List available analyzers, transformers or filters</p>
			</li></ol></div>

		<p>
			How these scenarios are attained is revealed by invoking your
			executable with the
			<span class="emphasis"><em>-usage</em></span>
			argument:
		</p>

		<pre class="programlisting">
			&gt;&nbsp;datacleaner-console.exe&nbsp;-usage
			-conf&nbsp;(-configuration, --configuration-file) FILE
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;XML&nbsp;file&nbsp;describing&nbsp;the&nbsp;configuration&nbsp;of&nbsp;DataCleaner
			-ds (-datastore, --datastore-name) VAL
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;Name&nbsp;of&nbsp;datastore&nbsp;when&nbsp;printing&nbsp;a&nbsp;list&nbsp;of&nbsp;schemas,&nbsp;tables&nbsp;or&nbsp;columns
			-job (--job-file) FILE
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;An&nbsp;analysis&nbsp;job&nbsp;XML&nbsp;file&nbsp;to&nbsp;execute
			-list&nbsp;[ANALYZERS&nbsp;|&nbsp;TRANSFORMERS&nbsp;|&nbsp;FILTERS&nbsp;|&nbsp;DATASTORES&nbsp;|&nbsp;SCHEMAS&nbsp;|&nbsp;TABLES&nbsp;|&nbsp;COLUMNS]
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;Used&nbsp;to&nbsp;print&nbsp;a&nbsp;list&nbsp;of&nbsp;various&nbsp;elements&nbsp;available&nbsp;in&nbsp;the&nbsp;configuration
			-s (-schema, --schema-name) VAL
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;Name&nbsp;of&nbsp;schema&nbsp;when&nbsp;printing&nbsp;a&nbsp;list&nbsp;of&nbsp;tables&nbsp;or&nbsp;columns
			-t (-table, --table-name) VAL
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;Name&nbsp;of&nbsp;table&nbsp;when&nbsp;printing&nbsp;a&nbsp;list&nbsp;of&nbsp;columns
		</pre>

	</div>

	<div class="section" title="Executing an analysis job"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cli_executing_a_job"></a>Executing an analysis job</h2></div></div></div>
		
		<p>Here's how to execute an analysis job - we'll use the bundled
			example job "employees.analysis.xml":
		</p>
		<pre class="programlisting">
			&gt;&nbsp;datacleaner-console.exe&nbsp;-job&nbsp;examples/employees.analysis.xml
			SUCCESS!

			...

			RESULT:
			Value distribution for column: REPORTSTO
			Top values:
			&nbsp;- 1102: 6
			&nbsp;- 1143: 6
			&nbsp;- 1088: 5
			Null count: 0
			Unique&nbsp;values: 0


			RESULT:
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Match&nbsp;count&nbsp;Sample
			Aaaaaaa&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;22&nbsp;William
			Aaaa&nbsp;Aaa&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;Foon&nbsp;Yue


			RESULT:
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Match&nbsp;count&nbsp;Sample
			aaaaaaaaaa&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;23&nbsp;jfirrelli


			RESULT:
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Match&nbsp;count&nbsp;Sample
			Aaaaa&nbsp;Aaa&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;17&nbsp;Sales&nbsp;Rep
			AA&nbsp;Aaaaaaaaa&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;VP&nbsp;Marketing
			Aaaa&nbsp;Aaaaaaa&nbsp;(AAAA)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;Sale&nbsp;Manager&nbsp;(EMEA)
			Aaaaa&nbsp;Aaaaaaa&nbsp;(AA)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;Sales&nbsp;Manager&nbsp;(NA)
			Aaaaa&nbsp;Aaaaaaa&nbsp;(AAAAA,&nbsp;AAAA)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;Sales&nbsp;Manager&nbsp;(JAPAN,&nbsp;APAC)
			Aaaaaaaaa&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;President

			...
		</pre>

		<p>As you can see from the listing, the results of the analysis
			will be printed directly to the command-line output. If you want to
			save the results to a file, simply use your operating systems
			built-in functionality to pipe command-line output to a file,
			typically using the '&gt;' operator.</p>

        <p>You can override the datastore the job uses by passing the <span class="emphasis"><em>-ds</em></span> argument when invoking the command-line interface: 
        </p>
        <pre class="programlisting">
			&gt;&nbsp;datacleaner-console.exe&nbsp;-job&nbsp;examples/employees.analysis.xml&nbsp;-ds&nbsp;orderdb
        </pre>
	</div>

	<div class="section" title="Listing datastore contents and available components"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cli_listing"></a>Listing datastore contents and available components</h2></div></div></div>
		
		<p>
			The Command-Line Interface allows for listing of datastore
			contents
			and available components. The intended usage for this is to
			aid in
			hand-editing an analysis file, if this is desired. By using the
			<span class="emphasis"><em>-list</em></span>
			arguments you can get the metadata of your datastore and the
			DataCleaner components that will allow you to manually compose an
			analysis file.
		</p>

		<p>
			Listing the contents of a datastore is pretty self-explanatory, if
			you look at the output of the
			<span class="emphasis"><em>-usage</em></span>
			command. Here's a few examples, using the example database 'orderdb':
		</p>

		<pre class="programlisting">
			&gt;&nbsp;datacleaner-console.exe&nbsp;-list&nbsp;datastores
			Datastores:
			-----------
			Country codes
			orderdb

			&gt;&nbsp;datacleaner-console.exe&nbsp;-list&nbsp;tables&nbsp;-ds&nbsp;orderdb
			Tables:
			-------
			CUSTOMERS
			CUSTOMER_W_TER
			DEPARTMENT_MANAGERS
			DIM_TIME
			EMPLOYEES
			OFFICES
			ORDERDETAILS
			ORDERFACT
			ORDERS
			PAYMENTS
			PRODUCTS
			QUADRANT_ACTUALS
			TRIAL_BALANCE

			&gt;&nbsp;datacleaner-console.exe&nbsp;-list&nbsp;columns&nbsp;-ds&nbsp;orderdb&nbsp;-table&nbsp;employees
			Columns:
			--------
			EMPLOYEENUMBER
			LASTNAME
			FIRSTNAME
			EXTENSION
			EMAIL
			OFFICECODE
			REPORTSTO
			JOBTITLE
		</pre>

		<p>
			Listing DataCleaner's components is done by setting the
			<span class="emphasis"><em>-list</em></span>
			argument to one of the three component types: ANALYZER, TRANSFORMER
			or FILTER:
		</p>

		<pre class="programlisting">
			&gt;&nbsp;datacleaner-console.exe&nbsp;-list&nbsp;analyzers

			...

			name:&nbsp;Matching&nbsp;analyzer
			-&nbsp;Consumes&nbsp;multiple&nbsp;input&nbsp;columns&nbsp;(type:&nbsp;UNDEFINED)
			-&nbsp;Property:&nbsp;name=Dictionaries,&nbsp;type=Dictionary,&nbsp;required=false
			-&nbsp;Property:&nbsp;name=String&nbsp;patterns,&nbsp;type=StringPattern,&nbsp;required=false
			name:&nbsp;Pattern&nbsp;finder
			-&nbsp;Consumes&nbsp;2&nbsp;named&nbsp;inputs
			&nbsp;&nbsp;&nbsp;Input&nbsp;column:&nbsp;Column&nbsp;(type:&nbsp;STRING)
			&nbsp;&nbsp;&nbsp;Input&nbsp;column:&nbsp;Group&nbsp;column&nbsp;(type:&nbsp;STRING)
			-&nbsp;Property:&nbsp;name=Discriminate&nbsp;text&nbsp;case,&nbsp;type=Boolean,&nbsp;required=false
			-&nbsp;Property:&nbsp;name=Discriminate&nbsp;negative&nbsp;numbers,&nbsp;type=Boolean,&nbsp;required=false
			-&nbsp;Property:&nbsp;name=Discriminate&nbsp;decimals,&nbsp;type=Boolean,&nbsp;required=false
			-&nbsp;Property:&nbsp;name=Enable&nbsp;mixed&nbsp;tokens,&nbsp;type=Boolean,&nbsp;required=false
			-&nbsp;Property:&nbsp;name=Ignore&nbsp;repeated&nbsp;spaces,&nbsp;type=Boolean,&nbsp;required=false
			-&nbsp;Property:&nbsp;name=Upper&nbsp;case&nbsp;patterns&nbsp;expand&nbsp;in&nbsp;size,&nbsp;type=boolean,&nbsp;required=false
			-&nbsp;Property:&nbsp;name=Lower&nbsp;case&nbsp;patterns&nbsp;expand&nbsp;in&nbsp;size,&nbsp;type=boolean,&nbsp;required=false
			-&nbsp;Property:&nbsp;name=Predefined&nbsp;token&nbsp;name,&nbsp;type=String,&nbsp;required=false
			-&nbsp;Property:&nbsp;name=Predefined&nbsp;token&nbsp;regexes,&nbsp;type=String,&nbsp;required=false
			-&nbsp;Property:&nbsp;name=Decimal&nbsp;separator,&nbsp;type=Character,&nbsp;required=false
			-&nbsp;Property:&nbsp;name=Thousands&nbsp;separator,&nbsp;type=Character,&nbsp;required=false
			-&nbsp;Property:&nbsp;name=Minus&nbsp;sign,&nbsp;type=Character,&nbsp;required=false

			...

			&gt;&nbsp;datacleaner-console.exe&nbsp;-list&nbsp;transformers

			...

			name:&nbsp;Tokenizer
			-&nbsp;Consumes&nbsp;a&nbsp;single&nbsp;input&nbsp;column&nbsp;(type:&nbsp;STRING)
			-&nbsp;Property:&nbsp;name=Delimiters,&nbsp;type=char,&nbsp;required=true
			-&nbsp;Property:&nbsp;name=Number&nbsp;of&nbsp;tokens,&nbsp;type=Integer,&nbsp;required=true
			-&nbsp;Output&nbsp;type&nbsp;is:&nbsp;STRING
			name:&nbsp;Whitespace&nbsp;trimmer
			-&nbsp;Consumes&nbsp;multiple&nbsp;input&nbsp;columns&nbsp;(type:&nbsp;STRING)
			-&nbsp;Property:&nbsp;name=Trim&nbsp;left,&nbsp;type=boolean,&nbsp;required=true
			-&nbsp;Property:&nbsp;name=Trim&nbsp;right,&nbsp;type=boolean,&nbsp;required=true
			-&nbsp;Property:&nbsp;name=Trim&nbsp;multiple&nbsp;to&nbsp;single&nbsp;space,&nbsp;type=boolean,&nbsp;required=true
			-&nbsp;Output&nbsp;type&nbsp;is:&nbsp;STRING

			...
		</pre>
	</div>

	<div class="section" title="Parameterizable jobs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="section_parameterizable_jobs"></a>Parameterizable jobs</h2></div></div></div>
		
		<p>If you want to make a part of a job parameterizable/variable,
			then it is possible to do so. Currently this is a feature only
			supported by means of editing the .analysis.xml files though, since
			the DataCleaner graphical user interface does not store job variables
			when saving jobs.
		</p>
		<p>In the source section of your job, you can add variables which
			are key/value pairs that will be referenced throughout your job. Each
			variable can have a default value which will be used in case the
			variable value is not specified. Here's a simple example:
		</p>
		<pre class="programlisting">
			...

			&lt;source&gt;
			&nbsp;&nbsp;&lt;data-context&nbsp;ref="my_datastore"&nbsp;/&gt;
			&nbsp;&nbsp;&lt;columns&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;column&nbsp;path="column1"&nbsp;id="col_1"&nbsp;/&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;column&nbsp;path="column2"&nbsp;id="col_2"&nbsp;/&gt;
			&nbsp;&nbsp;&lt;/columns&gt;
			&nbsp;&nbsp;&lt;variables&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;variable&nbsp;id="filename"&nbsp;value="/output/dc_output.csv"&nbsp;/&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;variable&nbsp;id="separator"&nbsp;value=","&nbsp;/&gt;
			&nbsp;&nbsp;&lt;/variables&gt;
			&lt;/source&gt;

			...
		</pre>
		<p>
			In the example we've defined two variables:
			<span class="emphasis"><em>filename</em></span>
			and
			<span class="emphasis"><em>separator</em></span>
			. These we can refer to for specific property values, further down in
			our job:
		</p>

		<pre class="programlisting">
			...

			&lt;analyzer&gt;
			&nbsp;&nbsp;&lt;descriptor&nbsp;ref="Write&nbsp;to&nbsp;CSV&nbsp;file"/&gt;
			&nbsp;&nbsp;&lt;properties&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;property&nbsp;name="File"&nbsp;ref="filename"&nbsp;/&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;property&nbsp;name="Quote&nbsp;char"&nbsp;value="&amp;quot;"&nbsp;/&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;property&nbsp;name="Separator&nbsp;char"&nbsp;ref="separator"&nbsp;/&gt;
			&nbsp;&nbsp;&lt;/properties&gt;
			&nbsp;&nbsp;&lt;input&nbsp;ref="col_1"&nbsp;/&gt;
			&nbsp;&nbsp;&lt;input&nbsp;ref="col_2"&nbsp;/&gt;
			&lt;/analyzer&gt;

			...
		</pre>

		<p>
			Now the property values of the
			<span class="emphasis"><em>File</em></span>
			and
			<span class="emphasis"><em>Separator char</em></span>
			properties in the
			<span class="emphasis"><em>Write to CSV file</em></span>
			have been made parameterizable. To execute the job with new variable
			values, use
			<span class="emphasis"><em>-var</em></span>
			parameters from the command line, like this:
		</p>

		<pre class="programlisting">
			DataCleaner-console.exe&nbsp;-job&nbsp;my_job.analysis.xml&nbsp;-var&nbsp;filename=/output/my_file.csv&nbsp;-var&nbsp;separator=;
		</pre>
	</div>

	<div class="section" title="Dynamically overriding configuration elements"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cli_overriding_configuration_elements"></a>Dynamically overriding configuration elements</h2></div></div></div>
		
		<p>
			Since version 2.5 of DataCleaner it is possible to override elements
			in the
			<a class="link" href="#chapter_configuration_file" title="Chapter&nbsp;10.&nbsp;Configuration file">configuration file</a>
			dynamically from the command line. This is a feature which can be
			useful in scenarios where you want the to invoke the same job but
			with slightly different configuration details.
		</p>
		<p>For example, you might want to reuse the same job to be executed
			on several similar CSV files, or similar database environments.
			Let us
			assume that you have a CSV datastore that is defined like this:
		</p>
		<pre class="programlisting">
			&lt;/datastore-catalog&gt;
			&nbsp;&nbsp;&lt;csv-datastore&nbsp;name="My csv file"&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;filename&gt;/path/to/file.csv&lt;/filename&gt;
			&nbsp;&nbsp;&lt;/csv-datastore&gt;
			&lt;/datastore-catalog&gt;
		</pre>
		<p>To override the filename dynamically, you have to specify the
			property path (datastore catalog, then datastore name, then property
			name) with a '-D' parameter on the command line. Furthermore any
			spaces or dashes are removed and the succeeding character is
			uppercased. In the end it will look like "camelCase" strings, like
			this:
		</p>
		<pre class="programlisting">DataCleaner-console.exe&nbsp;...&nbsp;-DdatastoreCatalog.myCsvFile.filename=anotherfile.csv
		</pre>
		<p>This mechanism can be used for any configuration property within
			the datastore catalog and reference data catalog.
		</p>
	</div>

</div>
		<div class="chapter" title="Chapter&nbsp;15.&nbsp;Apache Hadoop and Spark interface"><div class="titlepage"><div><div><h2 class="title"><a name="hadoop_interface"></a>Chapter&nbsp;15.&nbsp;Apache Hadoop and Spark interface</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>
				DataCleaner allows big data processing on the Apache Hadoop platform.
				In this chapter we will guide you through the process of setting up
				and running your first DataCleaner job on Hadoop.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#hadoop_deployment">Hadoop deployment overview</a></span></dt><dt><span class="section"><a href="#hadoop_spark_setup">Setting up Spark and DataCleaner environment</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e1052">Upload configuration file to HDFS</a></span></dt><dt><span class="section"><a href="#d5e1065">Upload job file to HDFS</a></span></dt><dt><span class="section"><a href="#d5e1079">Upload executables to HDFS</a></span></dt></dl></dd><dt><span class="section"><a href="#hadoop_launch">Launching DataCleaner jobs using Spark</a></span></dt><dt><span class="section"><a href="#hadoop_desktop">Using Hadoop in DataCleaner desktop</a></span></dt><dd><dl><dt><span class="section"><a href="#d5e1123">Configure Hadoop clusters</a></span></dt><dt><span class="section"><a href="#d5e1150">CSV datastores on HDFS</a></span></dt></dl></dd><dt><span class="section"><a href="#hadoop_limitations">Limitations of the Hadoop interface</a></span></dt></dl></div>

	

	


	<div class="section" title="Hadoop deployment overview"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hadoop_deployment"></a>Hadoop deployment overview</h2></div></div></div>
		
		<p>Apache Hadoop is a distributed system with a number of key components of which a few are important to understand:</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>
					<span class="emphasis"><em>YARN</em></span>, which is often referred to as the 'operating system' of Hadoop. YARN is the managing entity which assigns resources to running a specific job or task.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>HDFS</em></span>, which is the Hadoop Distributed File System. This is the location where data is located, but also the place where executables are often shared so that a distributed process can be picked up on many nodes in the cluster.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>Namenode</em></span>, is a dedicated node in the cluster which deals with HDFS and distribution of data to other nodes, so-called datanodes.
				</p>
			</li></ol></div>
		<p>In addition, the DataCleaner Hadoop support is built using Apache Spark, which is a data processing framework that works with Hadoop as well as other clustering technologies. A few important concepts of Apache Spark are useful to understand for DataCleaner's deployment on Hadoop:</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>
					<span class="emphasis"><em>Cluster manager</em></span>, which is the component that negotiates with the cluster - for instance Hadoop/YARN. From the perspective of Apache Spark, YARN is a <span class="emphasis"><em>cluster manager</em></span>.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>Driver program</em></span>, which is the program that directs the cluster manager and tells it what to do. In Apache Spark for Hadoop you have two choices: To run the Driver program as an external process ('yarn-client') or to run the Driver program as a process inside YARN itself ('yarn-cluster').
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>Executor</em></span>, which is a node in a Spark cluster that executes a partition (chunk) of a job.
				</p>
			</li></ol></div>
		<p>In the top-part of the below image you see Hadoop/YARN as well as Apache Spark, and how they are componentized.</p>
		<span class="inlinemediaobject"><img src="hadoop_deployment_overview.png"></span>
		<p>In the lower part of the image you see DataCleaner's directory structure on HDFS. As you can see, the usual configuration and job files are used, but placed on HDFS. A special JAR file is placed on HDFS to act as executable for the Apache Spark executors.</p>
	</div>

	<div class="section" title="Setting up Spark and DataCleaner environment"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hadoop_spark_setup"></a>Setting up Spark and DataCleaner environment</h2></div></div></div>
		
		<p>
			In order to work, Apache Spark requires either of environmental variables
			<span class="emphasis"><em>HADOOP_CONF_DIR</em></span>
			or
			<span class="emphasis"><em>YARN_CONF_DIR</em></span>
			to a directory containing your Hadoop/Yarn configuration files such as
			<span class="emphasis"><em>core-site.xml</em></span>
			,
			<span class="emphasis"><em>yarn-site.xml</em></span>
			etc.
		</p>
		
		<div class="section" title="Upload configuration file to HDFS"><div class="titlepage"><div><div><h3 class="title"><a name="d5e1052"></a>Upload configuration file to HDFS</h3></div></div></div>
			
			
			<p>
				DataCleaner on Hadoop needs a regular
				<a class="link" href="#chapter_configuration_file" title="Chapter&nbsp;10.&nbsp;Configuration file">DataCleaner configuration file</a> (conf.xml).
				It's best to upload this to the hadoop distributed file system (HDFS).
				We recommend putting this file into the path
				<span class="emphasis"><em>/datacleaner/conf.xml</em></span>
				.
	
				Simple example of a configuration file (conf.xml) with a CSV datastore based on a HDFS file or directory:
			</p>
			
			<pre class="programlisting">
				&lt;?xml version="1.0" encoding="UTF-8"?&gt;
				&lt;configuration xmlns="http://eobjects.org/analyzerbeans/configuration/1.0"&gt;
				&nbsp;&lt;datastore-catalog&gt;
				&nbsp;&nbsp;&lt;csv-datastore name="mydata"&gt;
				&nbsp;&nbsp;&nbsp;&lt;filename&gt;hdfs://bigdatavm:9000/path/to/data.txt&lt;/filename&gt;
				&nbsp;&nbsp;&nbsp;&lt;multiline-values&gt;false&lt;/multiline-values&gt;
				&nbsp;&nbsp;&lt;/csv-datastore&gt;
				&nbsp;&lt;/datastore-catalog&gt;
				&lt;/configuration&gt;
			</pre>
			
			<p>
				Notice the filename which is here specified with scheme, hostname and port:
			</p>
			
			<pre class="programlisting">
				&lt;filename&gt;hdfs://bigdatavm:9000/path/to/data.txt&lt;/filename&gt;
			</pre>
			
			<p>
				This here refers to the Hadoop Namenode's hostname and port.
			</p>
			<p>
				It can also be specified more implicityly, without the username and port:
			</p>
			
			<pre class="programlisting">
				&lt;filename&gt;hdfs:///path/to/data.txt&lt;/filename&gt;
			</pre>
			
			<p>
				Or even without scheme:
			</p>
			
			<pre class="programlisting">
				&lt;filename&gt;/path/to/data.txt&lt;/filename&gt;
			</pre>
		</div>

		<div class="section" title="Upload job file to HDFS"><div class="titlepage"><div><div><h3 class="title"><a name="d5e1065"></a>Upload job file to HDFS</h3></div></div></div>
			
			
			<p>
				Upload the DataCleaner job you wish to run (a DataCleaner .analysis.xml job file) to HDFS.
				We recommend putting this file into a path such as <span class="emphasis"><em>/datacleaner/jobs/myjob.xml</em></span>.
				The jobs can be built using the DataCleaner desktop UI, but do ensure that they map well to the configuration file also on HDFS.
			</p>
			<p>
				Example job file based on the above datastore:
			</p>
			<pre class="programlisting">
				&lt;?xml version="1.0" encoding="UTF-8"?&gt;
				&lt;job xmlns="http://eobjects.org/analyzerbeans/job/1.0"&gt;
				&nbsp;&lt;source&gt;
				&nbsp;&nbsp;&lt;data-context ref="mydata" /&gt;
				&nbsp;&nbsp;&lt;columns&gt;
				&nbsp;&nbsp;&nbsp;&lt;column id="col_country" path="country" /&gt;
				&nbsp;&nbsp;&nbsp;&lt;column id="col_company" path="company" /&gt;
				&nbsp;&nbsp;&lt;/columns&gt;
				&nbsp;&lt;/source&gt;
				&nbsp;&lt;analysis&gt;
				&nbsp;&nbsp;&lt;analyzer&gt;
				&nbsp;&nbsp;&nbsp;&lt;descriptor ref="Create CSV file"/&gt;
				&nbsp;&nbsp;&nbsp;&lt;properties&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;property name="File" value="hdfs:///path/to/output.csv"/&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;property name="Separator char" value="&amp;#44;"/&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;property name="Quote char" value="&amp;quot;"/&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;property name="Escape char" value="\"/&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;property name="Include header" value="true"/&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;property name="Encoding" value="UTF-8"/&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;property name="Fields" value="[COUNTRY,CUSTOMERNUMBER]"/&gt;
				&nbsp;&nbsp;&nbsp;&nbsp;&lt;property name="Overwrite file if exists" value="true"/&gt;
				&nbsp;&nbsp;&nbsp;&lt;/properties&gt;
				&nbsp;&nbsp;&nbsp;&lt;input ref="col_country" name="Columns"/&gt;
				&nbsp;&nbsp;&nbsp;&lt;input ref="col_company" name="Columns"/&gt;
				&nbsp;&nbsp;&nbsp;&lt;/analyzer&gt;
				&nbsp;&lt;/analysis&gt;
				&lt;/job&gt;
			</pre>
			
			<p>This particular job is very simplistic - it just copies data from A to B. Notes about the job file contents:</p>
			
			<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					<p>
						The job is referring to <span class="emphasis"><em>mydata</em></span> which was the name of the CSV datastore
						defined in the configuration file.
					</p>
				</li><li class="listitem">
					<p>
						There is another HDFS file reference used in the "File" property.
						The filename format is the same as in the configuration file.
					</p>
				</li></ol></div>
			
			<p>If your desktop application has access to the namenode then you can build this job in the desktop application, save it and run it on spark. There is nothing particular about this job that makes it runnable on spark, except that the file references involved are resolvable from the hadoop nodes.</p>
		</div>
		
		<div class="section" title="Upload executables to HDFS"><div class="titlepage"><div><div><h3 class="title"><a name="d5e1079"></a>Upload executables to HDFS</h3></div></div></div>
			
			
			<p>In the installation of DataCleaner you will find the file 'DataCleaner-spark.jar'.</p>

			<p>This jar file contains the core of what is needed to run DataCleaner with Apache Spark on Hadoop. It contains also the standard components of DataCleaner.</p>

			<p>Upload this jar file to HDFS in the folder <span class="emphasis"><em>/datacleaner/lib</em></span>.</p>
			<p>Upload your DataCleaner license file to <span class="emphasis"><em>/datacleaner/hi_datacleaner.lic</em></span>.</p>
			<p>Upload any extension jar files that you need (for instance Groovy-DataCleaner.jar) to that same folder.</p>

		</div>
	</div>

	<div class="section" title="Launching DataCleaner jobs using Spark"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hadoop_launch"></a>Launching DataCleaner jobs using Spark</h2></div></div></div>
		

		<p>Go to the Spark installation path to run the job. Use the following command line template:</p>

		<pre class="programlisting">
			bin/spark-submit --class org.datacleaner.spark.Main --master yarn-cluster /path/to/DataCleaner-spark.jar
			/path/to/conf.xml /path/to/job_file.analysis.xml ([/path/to/custom_properties.properties])
		</pre>
		
		<p>
			A convenient way to organize it is in a shell script like the below, where every individual argument can be edited line by line:
		</p>
		
		<pre class="programlisting">
			#!/bin/sh
			SPARK_HOME=/path/to/apache-spark
			SPARK_MASTER=yarn-cluster
			DC_PRIMARY_JAR=/path/to/DataCleaner-spark.jar
			DC_EXTENSION_JARS=/path/to/extension1.jar,/path/to/extension2.jar
			DC_CONF_FILE=hdfs:///path/to/conf.xml
			DC_JOB_FILE=hdfs:///path/to/job_file.analysis.xml
			DC_PROPS=hdfs:///path/to/custom_properties.properties
			
			DC_COMMAND="$SPARK_HOME/bin/spark-submit"
			DC_COMMAND="$DC_COMMAND --class org.datacleaner.spark.Main"
			DC_COMMAND="$DC_COMMAND --master $SPARK_MASTER"
			
			echo "Using DataCleaner executable: $DC_PRIMARY_JAR"
			if [ "$DC_EXTENSION_JARS" != "" ]; then
			&nbsp;&nbsp;echo "Adding extensions: $DC_EXTENSION_JARS"
			&nbsp;&nbsp;DC_COMMAND="$DC_COMMAND --jars $DC_EXTENSION_JARS"
			fi

			DC_COMMAND="$DC_COMMAND $DC_PRIMARY_JAR $DC_CONF_FILE $DC_JOB_FILE $DC_PROPS"
			
			echo "Submitting DataCleaner job $DC_JOB_FILE to Spark $SPARK_MASTER"
			$DC_COMMAND
		</pre>

		<p>The example makes it clear that there are a few more parameters to invoking the job. Let's go through them:</p>
		
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>
					<span class="emphasis"><em>SPARK_MASTER</em></span> represents the location of the Driver program, see the section on <a class="link" href="#hadoop_deployment" title="Hadoop deployment overview">Hadoop deployment overview</a>.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>DC_EXTENSION_JARS</em></span> allows you to add additional JAR files with extensions to DataCleaner.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>DC_PROPS</em></span> is maybe the most important one. It allows you to add a .properties file which can be used for a number of things:
				</p>
				<div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
						<p>
							Special property
							<span class="emphasis"><em>datacleaner.result.hdfs.path</em></span>
							which allows you to specify the filename (on HDFS) where the analysis result (.analysis.result.dat) file is stored. It defaults to
							<span class="emphasis"><em>/datacleaner/results/[job name]-[timestamp].analysis.result.dat</em></span>
						</p>
					</li><li class="listitem">
						<p>
							Special property
							<span class="emphasis"><em>datacleaner.result.hdfs.enabled</em></span>
							which can be either 'true' (default) or 'false'. Setting this property to false will disable result gathering completely from the
							DataCleaner job, which gives a significant increase in performance, but no analyzer results are gathered or written. This is thus only
							relevant for ETL-style jobs where the purpose of the job is to create/insert/update/delete from other datastores or files.
						</p>
					</li><li class="listitem">
						<p>Properties to <a class="link" href="#cli_overriding_configuration_elements" title="Dynamically overriding configuration elements">override configuration defaults</a>.</p>
					</li><li class="listitem">
						<p>Properties to <a class="link" href="#section_parameterizable_jobs" title="Parameterizable jobs">set job variables/parameters</a>.</p>
					</li></ol></div>
			</li></ol></div>
	</div>
	
    <div class="section" title="Using Hadoop in DataCleaner desktop"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hadoop_desktop"></a>Using Hadoop in DataCleaner desktop</h2></div></div></div>
        
        
        <p>
            Within DataCleaner desktop you can process CSV datastores located on HDFS.
        </p>

        <div class="section" title="Configure Hadoop clusters"><div class="titlepage"><div><div><h3 class="title"><a name="d5e1123"></a>Configure Hadoop clusters</h3></div></div></div>
            
            
            <p>
                To be able to execute jobs from DataCleaner desktop on a Hadoop Cluster you have a number of configuration options which are managed in the <span class="emphasis"><em>Hadoop clusters</em></span> tab in the <span class="emphasis"><em>Options</em></span> dialog.
                
                </p><div class="mediaobject"><img src="hadoop_options_clusters.png"></div><p>

                </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
                        <p><span class="emphasis"><em>Default</em></span></p>
                        <p>By default DataCleaner uses the HADOOP_CONF_DIR and YARN_CONF_DIR environment variables to determine the location of the Hadoop/Yarn configuration files such as <span class="emphasis"><em>core-site.xml</em></span> and <span class="emphasis"><em>yarn-site.xml</em></span>.</p>
                    </li><li class="listitem">
                        <p><span class="emphasis"><em>Using configuration directory</em></span></p>
                        <p>By clicking the <span class="emphasis"><em>Add Hadoop cluster</em></span> button and then selecting the <span class="emphasis"><em>Using configuration directory</em></span> you can register additional Hadoop clusters by adding locations which contain Hadoop/Yarn configuration files.</p>
                    </li><li class="listitem">
                        <p><span class="emphasis"><em>Using direct namenode connection</em></span></p>
                        <p>By clicking the <span class="emphasis"><em>Add Hadoop cluster</em></span> button and then selecting the <span class="emphasis"><em>Using direct namenode connection</em></span> you can registerd additional Hadoop clusters using their file system URI (e.g. hdfs://bigdatavm:9000/).</p>
                    </li></ol></div><p>
                
                If you have added additional Hadoop clusters, when selecting a file on HDFS, it first opens a dialog where you can select from which Hadoop custer you want to select a file.
            </p>
            
        </div>
        
        <div class="section" title="CSV datastores on HDFS"><div class="titlepage"><div><div><h3 class="title"><a name="d5e1150"></a>CSV datastores on HDFS</h3></div></div></div>
            
            
            <p>
                When registering a CSV datastore you have the option to select "hdfs" as scheme for the source of the CSV. In the path field you can either fill in an absolute path, including the scheme, e.g. <span class="emphasis"><em>hdfs://bigdatavm:9000/datacleaner/customers.csv</em></span> or the relative path to a file on HDFS, e.g. <span class="emphasis"><em>/datacleaner/customers.csv</em></span>. Note that a relative path only works when you have set the HADOOP_CONF_DIR or YARN_CONF_DIR environment variables (see <a class="link" href="#hadoop_spark_setup" title="Setting up Spark and DataCleaner environment">Setting up Spark and DataCleaner environment</a>). 
				</p><div class="mediaobject"><img src="hadoop_register_datastore.png"></div><p>
            </p>
        </div>
    </div>
        
	<div class="section" title="Limitations of the Hadoop interface"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hadoop_limitations"></a>Limitations of the Hadoop interface</h2></div></div></div>
		

		<p>While the Hadoop interface for DataCleaner allows distributed running of DataCleaner jobs on the Hadoop platform, there are a few limitations:</p>
	
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p><span class="emphasis"><em>Datastore support</em></span></p>
				<p>Currently we support a limited set of source datastores from HDFS. CSV files are the primary source here.
				We do require that files on HDFS are UTF8 encoded and that only single-line values occur.</p>
			</li><li class="listitem">
				<p><span class="emphasis"><em>Non-distributable components</em></span></p>
				<p>A few components are by nature not distributable. If your job depends on these, DataCleaner will resort to
				executing the job on a single Spark executor, which may have a significant performance impact.</p>
			</li><li class="listitem">
				<p><span class="emphasis"><em>Hadoop Distributions without Namenode</em></span></p>
				<p>Some Hadoop Distributions (such as MapR) have replaced the concept of Namenode with something else.
				This is mostly fine, but it does mean that file paths with username+port of Namenodes are obviously not working.</p>
			</li></ol></div>
	</div>

</div>
	</div>
	
	<div class="part" title="Part&nbsp;VI.&nbsp;Third party integrations"><div class="titlepage"><div><div><h1 class="title"><a name="d5e1175"></a>Part&nbsp;VI.&nbsp;Third party integrations</h1></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="chapter"><a href="#d5e1178">16. Pentaho integration</a></span></dt><dd><dl><dt><span class="section"><a href="#pentaho_configure_dc_installation">Configure DataCleaner in Pentaho Data Integration </a></span></dt><dt><span class="section"><a href="#pentaho_launch_dc_from_pdi">Launch DataCleaner to profile Pentaho Data Integration steps
		</a></span></dt><dt><span class="section"><a href="#pentaho_run_dc_jobs_from_pdi">Run DataCleaner jobs in Pentaho Data Integration</a></span></dt></dl></dd></dl></div>
		
		<div class="chapter" title="Chapter&nbsp;16.&nbsp;Pentaho integration"><div class="titlepage"><div><div><h2 class="title"><a name="d5e1178"></a>Chapter&nbsp;16.&nbsp;Pentaho integration</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>
				DataCleaner offers a number of integrations with the
				<a class="link" href="http://www.pentaho.com" target="_top">Pentaho</a>
				open source business intelligence suite. In this chapter we will
				present an overview of the options available.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#pentaho_configure_dc_installation">Configure DataCleaner in Pentaho Data Integration </a></span></dt><dt><span class="section"><a href="#pentaho_launch_dc_from_pdi">Launch DataCleaner to profile Pentaho Data Integration steps
		</a></span></dt><dt><span class="section"><a href="#pentaho_run_dc_jobs_from_pdi">Run DataCleaner jobs in Pentaho Data Integration</a></span></dt></dl></div>

	
	

	<div class="section" title="Configure DataCleaner in Pentaho Data Integration"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="pentaho_configure_dc_installation"></a>Configure DataCleaner in Pentaho Data Integration </h2></div></div></div>
	
		<p>
			In order to use the plugin of DataCleaner in Pentaho, it is required to
			have a local installation of DataCleaner. The DataCleaner
			installation can be either a community or a professional
			edition. The configuration can be set from "Tools". Moreover, after setting the DataCleaner configuration, from the same menu one can launch DataCleaner independent of context in Pentaho. 
		</p>
		
		<div class="mediaobject"><img src="pdi-tools-menu.png"></div>
		
		<div class="mediaobject"><img src="pdi_dc_configuration.png"></div>
		
	</div>
	<div class="section" title="Launch DataCleaner to profile Pentaho Data Integration steps"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="pentaho_launch_dc_from_pdi"></a>Launch DataCleaner to profile Pentaho Data Integration steps
		</h2></div></div></div>
		
		<p>In Pentaho Data Integration you can launch DataCleaner by
			right-clicking any step of your transformations. This will start up
			DataCleaner with the transformations data pre-loaded, ready for
			profiling.
		</p>
		<div class="mediaobject"><img src="dc_profile_context_menu.png"></div>
		<p>
			This functionality requires installation of the data profiling
			plugin
			for Pentaho Data Integration. The instructions and further
			documentation of this is maintained at Pentaho's wiki page:
			<a class="link" href="http://wiki.pentaho.com/display/EAI/Kettle+Data+Profiling+with+DataCleaner" target="_top">Kettle Data Profiling with DataCleaner</a>
			.
		</p>
	</div>

	<div class="section" title="Run DataCleaner jobs in Pentaho Data Integration"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="pentaho_run_dc_jobs_from_pdi"></a>Run DataCleaner jobs in Pentaho Data Integration</h2></div></div></div>
		

		<p>
			<span class="emphasis"><em>Pentaho Data Integration job entry</em></span>
			. If you want to have DataCleaner scheduled and integrated into an
			environment where you can eg. iterate over files in a folder etc.,
			then you can use Pentaho Data Integration (PDI), which is an open
			source
			ETL tool that includes a scheduler.
		</p>
		<p>Construct a PDI "job" (ie. not a "transformation") and add the
			DataCleaner job entry. The entry can be found in the submenu 'Utility'. The configuration dialog will look like
			this:
		</p>
		<div class="mediaobject"><img src="dc_job_entry.png"></div>
		<p>The most tricky part is to fill out the executable and the job
			filename. Note that all configuration options can contain PDI
			variables, like it is the case with ${user.home} in the screenshot
			above. This is useful if you want to eg. timestamp your resulting
			files etc.
		</p>
	</div>

</div>
	</div>

    <div class="part" title="Part&nbsp;VII.&nbsp;Developer's guide"><div class="titlepage"><div><div><h1 class="title"><a name="d5e1210"></a>Part&nbsp;VII.&nbsp;Developer's guide</h1></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="chapter"><a href="#d5e1213">17. Architecture</a></span></dt><dd><dl><dt><span class="section"><a href="#architecture_data_access">Data access</a></span></dt><dt><span class="section"><a href="#architecture_processing_framework">Processing framework</a></span></dt></dl></dd><dt><span class="chapter"><a href="#chapter_executing_jobs_through_code">18. Executing jobs through code</a></span></dt><dd><dl><dt><span class="section"><a href="#section_development_executing_job_steps">Overview of steps and options</a></span></dt><dt><span class="section"><a href="#section_development_configuration_configuration">Step 1: Configuration</a></span></dt><dt><span class="section"><a href="#section_development_configuration_job">Step 2: Job</a></span></dt><dt><span class="section"><a href="#section_development_configuration_execution">Step 3: Execution</a></span></dt><dt><span class="section"><a href="#section_development_configuration_result">Step 4: Result</a></span></dt></dl></dd><dt><span class="chapter"><a href="#chapter_developer_resources">19. Developer resources</a></span></dt><dd><dl><dt><span class="section"><a href="#section_development_tutorials">Extension development tutorials</a></span></dt><dt><span class="section"><a href="#developer_building_datacleaner">Building DataCleaner</a></span></dt></dl></dd><dt><span class="chapter"><a href="#d5e1407">20. Extension packaging</a></span></dt><dd><dl><dt><span class="section"><a href="#extensions_annotated_components">Annotated components</a></span></dt><dt><span class="section"><a href="#extensions_single_jar_file">Single JAR file</a></span></dt><dt><span class="section"><a href="#extensions_metadata_xml">Extension metadata XML</a></span></dt><dt><span class="section"><a href="#extensions_component_icons">Component icons</a></span></dt></dl></dd><dt><span class="chapter"><a href="#embedding_datacleaner">21. Embedding DataCleaner</a></span></dt></dl></div>
		
		<div class="chapter" title="Chapter&nbsp;17.&nbsp;Architecture"><div class="titlepage"><div><div><h2 class="title"><a name="d5e1213"></a>Chapter&nbsp;17.&nbsp;Architecture</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>
				The architecture of DataCleaner can be described from different
				angles depending on the topic of interest. In the following sections
				we cover different aspects of the DataCleaner architecture.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#architecture_data_access">Data access</a></span></dt><dt><span class="section"><a href="#architecture_processing_framework">Processing framework</a></span></dt></dl></div>

	

	

	<div class="section" title="Data access"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="architecture_data_access"></a>Data access</h2></div></div></div>
		

		<p>In DataCleaner all sources of data are called 'datastores'. This
			concept covers both sources that are read/parsed locally and those
			that are 'connected to', eg. databases and applications. Some
			datastores can also be written to, for instance relational databases.
		</p>

		<p>
			DataCleaner uses the
			<a class="link" href="http://metamodel.apache.org" target="_top">Apache MetaModel framework</a>
			for data access. From DataCleaner's perspective, Apache MetaModel
			provides a
			number of features:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>A common way of interacting with different datastores.</p>
			</li><li class="listitem">
				<p>A programmatic query syntax that abstracts away
					database-specific SQL dialects, and that is usable also for non-SQL
					oriented datastores (files etc.).
				</p>
			</li><li class="listitem">
				<p>Out-of-the-box connectivity to a lot of sources, eg. CSV
					files, relational databases, Excel spreadsheets and a lot more.
				</p>
			</li><li class="listitem">
				<p>A framework for modelling new sources using the same common
					model.
				</p>
			</li></ol></div>

		<p>
			DataCleaners datastore model is also extensible in the way that
			you
			can yourself implement new datastores in order to hook up
			DataCleaner
			to legacy systems, application interfaces and more. For more
			information refer to the
			<a class="link" href="#section_development_tutorials" title="Extension development tutorials">Developer resources</a>
			chapter.
		</p>

	</div>
	<div class="section" title="Processing framework"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="architecture_processing_framework"></a>Processing framework</h2></div></div></div>
		

		<p>The way DataCleaner processes data is
			slightly different compared
			to most similar (ETL-like) tools. Firstly in the way multithreading
			is applied, secondly in the way DataCleaner may sometimes optimize
			the graph at execution time.
		</p>

		<p>
			<span class="emphasis"><em>Multithreading:</em></span>
			The multithreading strategy in DataCleaner enables the tool to have
			the minimum amount of blocking and buffering and the maximum amount
			of parallelism and potentially also distribution. Most ETL-like tools
			apply a threading strategy where each component in a job has its own
			thread-management as well as an input- and an output-buffer. In
			DataCleaner the thread management is made such that every record is
			processed in parallel - each unit of work is stepping through the
			complete job graph in one single pass. This has a number of
			interesting traits:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>There is a high degree of automatic 'load balancing' among
					the components - less constraints and bottlenecks around the
					slowest components in the job.
				</p>
			</li><li class="listitem">
				<p>The system lends itself to highly distributed processing
					because statefulness is the exception instead of the
					rule.
				</p>
			</li><li class="listitem">
				<p>There is less waste in the form of buffers inbetween the
					components of a job.
				</p>
			</li><li class="listitem">
				<p>One downside to this approach is that the order of the
					processed records cannot be guaranteed. This is only very rarely
					required in the domain of data profiling and analysis, and if it is
					required there are technical workarounds to apply.
				</p>
			</li></ol></div>

		<p>
			<span class="emphasis"><em>Graph optimization:</em></span>
			While a job graph (see
			<a class="link" href="#wiring_components" title="Wiring components together">wiring components together</a>
			) may show a particular following order, the engine may at runtime do
			certain optimizations to it. Some components may provide optimization
			strategies that involves changing the source query so that the number
			of (or content of) processed records is changed. Obviously this is a
			side-effect of using a component that will only be applied in cases
			where it does not impact other components in a job. The principle is
			sometimes also referred to as 'Push down optimization'.
		</p>
		<p>
			An example of this is a 'Null check' filter: If a Null check is
			applied on a source column and all other components require either a
			NULL or a NOT_NULL outcome (either explicitly or implicitly), then
			the 'Null check' filter may add a predicate to the source query to
			filter out all irrelevant records. For more information on this
			principle, please read the blog entry '
			<a class="link" href="http://kasper.eobjects.org/2011/12/push-down-query-optimization-in.html" target="_top">Push down query optimization in DataCleaner</a>
			' by Kasper S&oslash;rensen.
		</p>

	</div>

</div>
		<div class="chapter" title="Chapter&nbsp;18.&nbsp;Executing jobs through code"><div class="titlepage"><div><div><h2 class="title"><a name="chapter_executing_jobs_through_code"></a>Chapter&nbsp;18.&nbsp;Executing jobs through code</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>
				In this chapter we will go through one of the most common
				use-cases for integrating DataCleaner into your own application;
				Executing a DataCleaner job through code.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#section_development_executing_job_steps">Overview of steps and options</a></span></dt><dt><span class="section"><a href="#section_development_configuration_configuration">Step 1: Configuration</a></span></dt><dt><span class="section"><a href="#section_development_configuration_job">Step 2: Job</a></span></dt><dt><span class="section"><a href="#section_development_configuration_execution">Step 3: Execution</a></span></dt><dt><span class="section"><a href="#section_development_configuration_result">Step 4: Result</a></span></dt></dl></div>

	

	

	<div class="section" title="Overview of steps and options"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="section_development_executing_job_steps"></a>Overview of steps and options</h2></div></div></div>
		
		<p>There's a couple of variants of this story - What kind of
			configuration options do you want? Would you like to build the job
			programmatically, or have it somewhere on disk as a .analysis.xml
			file? Will you be doing any processing of the result, or will the
			job
			itself contain all the necesary logic.
		</p>
		<p>The various steps and options are depicted in the diagram below.
			In the following sections we'll go through each of the 4
			steps/columns in the diagram:
		</p>
		<div class="mediaobject"><img src="dev_job_execution_overview.png"></div>
	</div>

	<div class="section" title="Step 1: Configuration"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="section_development_configuration_configuration"></a>Step 1: Configuration</h2></div></div></div>
		<div class="mediaobject" align="right"><img src="dev_job_execution_configuration.png" align="right"></div>
		
		<p>
			The configuration for DataCleaner is represented in the class
			<span class="emphasis"><em>DataCleanerConfiguration</em></span>
			(previously 'AnalyzerBeansConfiguration').
			You need a
			DataCleanerConfiguration as a prerequisite for most of
			the coming
			operations.
		</p>
		<p>
			The easiest and probably most convenient option for acquiring an
			DataCleanerConfiguration instance is to load it from a file,
			typically named conf.xml (See the
			<a class="link" href="#chapter_configuration_file" title="Chapter&nbsp;10.&nbsp;Configuration file">Configuration file</a>
			chapter for more details on this file format). To load the file, use
			the JaxbConfigurationReader class, like this:
		</p>
		<pre class="programlisting">
			InputStream&nbsp;inputStream&nbsp;=&nbsp;new&nbsp;FileInputStream("conf.xml");
			JaxbConfigurationReader&nbsp;configurationReader&nbsp;=&nbsp;new&nbsp;JaxbConfigurationReader();
			DataCleanerConfiguration&nbsp;configuration&nbsp;=&nbsp;configurationReader.read(inputStream);
		</pre>
		<p>Alternatively, you can build the configuration programmatically,
			through code. This is typically more cumbersome, but in some cases
			also quite useful if the configuration is to be build dynamically or
			something like that.
		</p>
		<p>Here's an example where we configure
			DataCleaner with 2 example
			datastores and a threadpool of 10
			threads:
		</p>
		<pre class="programlisting">
			Datastore&nbsp;datastore1&nbsp;=&nbsp;new&nbsp;CsvDatastore("my&nbsp;CSV&nbsp;file",&nbsp;"some_data.csv");
			boolean&nbsp;multipleConnections&nbsp;=&nbsp;true
			Datastore&nbsp;datastore2&nbsp;=&nbsp;new&nbsp;JdbcDatastore("my&nbsp;database",
			&nbsp;&nbsp;&nbsp;&nbsp;"jdbc:vendor://localhost/database",&nbsp;"com.database.Driver",
			&nbsp;&nbsp;&nbsp;&nbsp;"username",&nbsp;"password",&nbsp;multipleConnections);

			DataCleanerConfigurationImpl&nbsp;configuration&nbsp;=&nbsp;new&nbsp;DataCleanerConfigurationImpl();
			configuration&nbsp;=&nbsp;configuration.replace(new&nbsp;MultiThreadedTaskRunner(10));
			configuration&nbsp;=&nbsp;configuration.replace(new&nbsp;DatastoreCatalogImpl(datastore1,&nbsp;datastore2));
		</pre>
		<p>
			Either way we do it, we now have an
			<span class="emphasis"><em>DataCleanerConfiguration</em></span>
			with the variable name 'configuration'. Then we can proceed to
			defining the job to run.
		</p>
	</div>

	<div class="section" title="Step 2: Job"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="section_development_configuration_job"></a>Step 2: Job</h2></div></div></div>
		<div class="mediaobject" align="right"><img src="dev_job_execution_job.png" align="right"></div>
		
		<p>Like with the configuration, we can choose to either load the
			job we want to run from a file, or build it programmatically.
		</p>
		<p>Let's start by simply loading a job from a file. We'll need to
			use the JaxbJobReader class:
		</p>
		<pre class="programlisting">
			InputStream&nbsp;inputStream&nbsp;=&nbsp;new&nbsp;FileInputStream("my_job.analysis.xml");
			JaxbJobReader&nbsp;jobReader&nbsp;=&nbsp;new&nbsp;JaxbJobReader(configuration);
			AnalysisJob&nbsp;analysisJob&nbsp;=&nbsp;jobReader.read(inputStream);
		</pre>
		<p>
			Note that this is the 'vanilla' case. You can also use the
			JaxbJobReader to read metadata about a job, and even to read a job
			'as a template', which makes it possible to instantiate the job with
			certain replacements. For an example of how this functionality is
			used in DataCleaner's desktop application, see the
			<a class="link" href="#template_jobs" title="Template jobs">template jobs</a>
			section.
		</p>
		<p>
			The other way of producing a job is to build it
			programmatically. This is quite involved process that varies quite a
			lot depending on what kind of job you want to build. But the API has
			been designed to make it as easy as possible.
		</p>
		<p>To give an overview of the API, consider this list of important
			classes:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>
					<span class="emphasis"><em>AnalysisJobBuilder</em></span>
					:
					Represents a mutable job that is being built. This builder object
					contains source columns of the job, and all the components that
					consume source columns (or sometimes transformed columns).
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>TransformerComponentBuilder</em></span>
					,
					<span class="emphasis"><em>FilterComponentBuilder</em></span>
					, and
					<span class="emphasis"><em>AnalyzerComponentBuilder</em></span>
					:
					, represents mutable components of the job that is being built.
					These can each have configuration properties, filter requirements,
					input and output columns.
				</p>
			</li></ol></div>

		<div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Tip</h3>
			<p>Be aware of the unfortunate similarity between the
				'AnalyzerComponentBuilder' class name and the 'AnalysisJobBuilder'
				class
				name. To rid the confusion, remember that the 'analysis'
				represents
				the full scope of the job, whereas an 'analyzer' is just a
				single
				active part ('component') of the job.
			</p>
		</div>

		<p>Let's see an example of building a job programmatically. And to
			ensure that we don't miss important insights, we'll make it a fairly
			non-trivial job with both filters, transformers and analyzers. The
			job
			will encompass:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>Three source columns from the datastore 'my database': Name,
					Age and Company_name.
				</p>
			</li><li class="listitem">
				<p>All records where 'Company_name' is null will be inserted into
					the
					datastore called 'my CSV file'. In the CSV file the columns are
					called 'fullname' and 'age_years'.
				</p>
			</li><li class="listitem">
				<p>All records where 'Company_name' isn't null will 1) have their
					working address looked up in another table of the database, and 2)
					the name and the working address will be passed on to a 'Pattern
					finder' analyzer.
				</p>
			</li></ol></div>
		<pre class="programlisting">
			Datastore&nbsp;myDatabase&nbsp;=&nbsp;configuration.getDatastoreCatalog().getDatastore("my&nbsp;database");
			Datastore&nbsp;myCsvFile&nbsp;=&nbsp;configuration.getDatastoreCatalog().getDatastore("my&nbsp;CSV&nbsp;file");

			AnalysisJobBuilder&nbsp;builder&nbsp;=&nbsp;new&nbsp;AnalysisJobBuilder(configuration);
			builder.setDatastore(myDatabase);
			builder.addSourceColumns("public.persons.Name","public.persons.Age","public.persons.Company_name")

			InputColumn&lt;?&gt;&nbsp;nameColumn&nbsp;=&nbsp;builder.getSourceColumnByName("Name");
			InputColumn&lt;?&gt;&nbsp;ageColumn&nbsp;=&nbsp;builder.getSourceColumnByName("Age");
			InputColumn&lt;?&gt;&nbsp;companyColumn&nbsp;=&nbsp;builder.getSourceColumnByName("Company_name");

			//&nbsp;add&nbsp;a&nbsp;filter&nbsp;to&nbsp;check&nbsp;for&nbsp;null&nbsp;'company'
			FilterComponentBuilder&lt;NullCheckFilter&gt;&nbsp;nullCheckBuilder&nbsp;=&nbsp;builder.addFilter(NullCheckFilter.class);
			nullCheckBuilder.addInputColumn(companyColumn);

			//&nbsp;add&nbsp;a&nbsp;InsertIntoTable&nbsp;analyzer&nbsp;to&nbsp;write&nbsp;the&nbsp;records&nbsp;without&nbsp;a&nbsp;company&nbsp;to&nbsp;the&nbsp;csv&nbsp;file
			AnalyzerComponentBuilder&lt;InsertIntoTableAnalyzer&gt;&nbsp;insertBuilder&nbsp;=&nbsp;builder.addAnalyzer(InsertIntoTableAnalyzer.class);
			insertBuilder.addInputColumns(nameColumn,&nbsp;ageColumn);
			insertBuilder.setConfiguredProperty("Datastore",&nbsp;myCsvFile);
			insertBuilder.setConfiguredProperty("Columns",&nbsp;new&nbsp;String[]&nbsp;{"fullname","age_years"});
			insertBuilder.setRequirement(nullCheckBuilder.getOutcome(NullCheckFilter.Category.NULL));

			//&nbsp;add&nbsp;a&nbsp;lookup&nbsp;for&nbsp;the&nbsp;company&nbsp;working&nbsp;address
			&nbsp;&nbsp;&nbsp;&nbsp;TransformerComponentBuilder&lt;TableLookupTransformer&gt;&nbsp;lookupBuilder&nbsp;=
			builder.addTransformer(TableLookupTransformer.class);
			lookupBuilder.addInputColumn(companyColumn);
			lookupBuilder.setConfiguredProperty("Datastore",&nbsp;myDatabase);
			lookupBuilder.setConfiguredProperty("Schema&nbsp;name",&nbsp;"public");
			lookupBuilder.setConfiguredProperty("Table&nbsp;name",&nbsp;"companies");
			lookupBuilder.setConfiguredProperty("Condition&nbsp;columns",&nbsp;new&nbsp;String[]&nbsp;{"name"});
			lookupBuilder.setConfiguredProperty("Output&nbsp;columns",&nbsp;new&nbsp;String[]&nbsp;{"address"});
			lookupBuilder.setRequirement(nullCheckBuilder.getOutcome(NullCheckFilter.Category.NOT_NULL));

			//&nbsp;reference&nbsp;the&nbsp;'working&nbsp;address'&nbsp;column&nbsp;and&nbsp;give&nbsp;it&nbsp;a&nbsp;proper&nbsp;name
			MutableInputColumn&lt;?&gt;&nbsp;addressColumn&nbsp;=&nbsp;lookupBuilder.getOutputColumns().get(0);
			addressColumn.setName("Working&nbsp;address");

			//&nbsp;add&nbsp;the&nbsp;Pattern&nbsp;finder&nbsp;analyzer
			PatternFinder&nbsp;patternFinder&nbsp;=&nbsp;jobBuilder.addAnalyzer(PatternFinder.class);
			patternFinder.addInputColumns(nameColumn,&nbsp;addressColumn);

			//&nbsp;validate&nbsp;and&nbsp;produce&nbsp;to&nbsp;AnalysisJob
			AnalysisJob&nbsp;analysisJob&nbsp;=&nbsp;jobBuilder.toAnalysisJob();
		</pre>

		<p>Things to note from this example:</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>Notice how the filter requirements are set up using the
					.setRequirement(...) method on the succeeding components.
				</p>
			</li><li class="listitem">
				<p>There aren't any explicit filter requirements set on the
					'Pattern finder' analyzer. This isn't necesary since it depends on
					a transformed input column ('Working address') which itself has the
					requirement. DataCleaner will figure out the transitive
					requirements automatically.
				</p>
			</li><li class="listitem">
				<p>
					One piece of 'magic' is how to set the properties of the components
					correctly. We can see that we call
					<span class="emphasis"><em>.setConfiguredProperty(String,Object)</em></span>
					, but not how to figure out
					what to pass as arguments. There are two
					proper ways to figure this out...
				</p>
				<div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
						<p>You can use DataCleaner's command line to list all
							components of a specific type, e.g.:
						</p>
						<pre class="programlisting">
							&gt;&nbsp;DataCleaner-console.exe&nbsp;-list&nbsp;ANALYZERS
							...
							name:&nbsp;Insert&nbsp;into&nbsp;table
							&nbsp;-&nbsp;Consumes&nbsp;2&nbsp;named&nbsp;inputs
							&nbsp;&nbsp;&nbsp;Input&nbsp;columns:&nbsp;Additional&nbsp;error&nbsp;log&nbsp;values&nbsp;(type:&nbsp;Object)
							&nbsp;&nbsp;&nbsp;Input&nbsp;columns:&nbsp;Values&nbsp;(type:&nbsp;Object)
							&nbsp;-&nbsp;Property:&nbsp;name=Column&nbsp;names,&nbsp;type=String,&nbsp;required=true
							&nbsp;-&nbsp;Property:&nbsp;name=Datastore,&nbsp;type=UpdateableDatastore,&nbsp;required=true
							...
						</pre>
					</li><li class="listitem">
						<p>
							Or you can simply open up the component class in an IDE to
							inspect it's @Configured properties. For instance, if we look at
							<span class="emphasis"><em>InsertIntoTableAnalyzer.java</em></span>
							we'll see:
						</p>
						<pre class="programlisting">
							...

							@Inject
							@Configured
							@Description("Names&nbsp;of&nbsp;columns&nbsp;in&nbsp;the&nbsp;target&nbsp;table.")
							@ColumnProperty
							String[]&nbsp;columnNames;

							@Inject
							@Configured
							@Description("Datastore&nbsp;to&nbsp;write&nbsp;to")
							UpdateableDatastore&nbsp;datastore;

							...
						</pre>
						<p>From these fields we can infer that there will be two
							configured properties, 'Column names' and 'Datastore'.
						</p>
					</li></ol></div>
			</li></ol></div>

		<p>
			Either way we do it, we now have an
			<span class="emphasis"><em>AnalysisJob</em></span>
			with the variable name 'analysisJob'. Then we can proceed to
			actually
			executing the job.
		</p>

	</div>

	<div class="section" title="Step 3: Execution"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="section_development_configuration_execution"></a>Step 3: Execution</h2></div></div></div>
		<div class="mediaobject" align="right"><img src="dev_job_execution_execution.png" align="right"></div>
		
		<p>Executing the job is one of the easiest steps, but obviously
			there are options available beyond the 'vanilla' scenario.
		</p>
		<p>The simple scenario of running the job is to use the plain
			AnalysisRunnerImpl class, like this:
		</p>
		<pre class="programlisting">
			AnalysisRunner&nbsp;runner&nbsp;=&nbsp;new&nbsp;AnalysisRunnerImpl(configuration);
			AnalysisResultFuture&nbsp;resultFuture&nbsp;=&nbsp;runner.run(analysisJob);
		</pre>
		<p>
			This will return a
			<span class="emphasis"><em>AnalysisResultFuture</em></span>
			, which under most circumstances represents a still-running job. Your
			application can continue to do other work in the background, or it
			can decide to block by calling
			<span class="emphasis"><em>.await()</em></span>
			.
		</p>
		<p>Here's a typical example of handling the result future:</p>
		<pre class="programlisting">
			//&nbsp;block&nbsp;until&nbsp;the&nbsp;job&nbsp;has&nbsp;finished
			resultFuture.await();

			if&nbsp;(resultFuture.isSuccessful())&nbsp;{
			&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;do&nbsp;something&nbsp;with&nbsp;the&nbsp;successful&nbsp;result
			&nbsp;&nbsp;&nbsp;&nbsp;handleResult(resultFuture);
			}&nbsp;else&nbsp;{
			&nbsp;&nbsp;&nbsp;&nbsp;List&lt;Throwable&gt;&nbsp;errors&nbsp;=&nbsp;resultFuture.getErrors();
			&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(Throable&nbsp;error&nbsp;:&nbsp;errors)&nbsp;{
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;logger.error("An&nbsp;error&nbsp;occurred&nbsp;while&nbsp;executing&nbsp;job",&nbsp;error);
			&nbsp;&nbsp;&nbsp;&nbsp;}
			&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;usually&nbsp;the&nbsp;first&nbsp;error&nbsp;that&nbsp;occurred&nbsp;is&nbsp;the&nbsp;culprit,&nbsp;so&nbsp;we'll&nbsp;throw&nbsp;that&nbsp;one
			&nbsp;&nbsp;&nbsp;&nbsp;throw&nbsp;errors.get(0);
			}
		</pre>

		<p>You might ask what kind of errors will happen while executing a
			DataCleaner job? The answer is that it can be a lot of things, for
			instance:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>The connection to the source database or resource may fail
					somehow.
				</p>
			</li><li class="listitem">
				<p>One of the components in the job may throw an unexpected
					exception.
				</p>
			</li><li class="listitem">
				<p>One of the components may throw an exception because it's
					configuration is incomplete or invalid (although this will in most
					cases be detected while building the AnalysisJob instance).
				</p>
			</li><li class="listitem">
				<p>If you're writing data to another datastore, that may also
					fail for whatever datastore-dependent reasons.
				</p>
			</li><li class="listitem">
				<p>If your job is doing something stupid like a Value
					Distribution of a billion unique IDs, then you'll run out of
					memory.
				</p>
			</li></ol></div>

		<p>Let's now assume that your job has executed succesfully. We'll
			now look at how you can post-process results and how to save/load
			them to/from a file.
		</p>
	</div>

	<div class="section" title="Step 4: Result"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="section_development_configuration_result"></a>Step 4: Result</h2></div></div></div>
		<div class="mediaobject" align="right"><img src="dev_job_execution_result.png" align="right"></div>
		

		<p>
			Great, now we have an
			<span class="emphasis"><em>AnalysisResultFuture</em></span>
			, and we've determined that it was successful. What can we do with
			it?
		</p>

		<p>
			The results of each analyzer of the job are available through the
			'AnalysisResult' interface, which AnalysisResultFuture implements.
			Note that the analyzer result types are very different from each
			other. For instance, the 'Insert into table' analyzer produces a
			<span class="emphasis"><em>WriteDataResult</em></span>
			, while the 'Pattern finder' produces a
			<span class="emphasis"><em>PatternFinderResult</em></span>
			. Let's see how you can extract information from them:
		</p>
		<pre class="programlisting">
			//&nbsp;demonstrate&nbsp;the&nbsp;the&nbsp;result
			//&nbsp;future&nbsp;implements&nbsp;the&nbsp;AnalysisResult&nbsp;interface,&nbsp;which&nbsp;is&nbsp;sufficient
			//&nbsp;for&nbsp;all&nbsp;the&nbsp;followin&nbsp;operations
			AnalysisResult&nbsp;analysisResult&nbsp;=&nbsp;resultFuture;
			List&lt;AnalyzerResult&gt;&nbsp;results&nbsp;=&nbsp;analysisResult.getResults();
			for&nbsp;(AnalyzerResult&nbsp;result&nbsp;:&nbsp;results)&nbsp;{

			&nbsp;&nbsp;if&nbsp;(result&nbsp;instanceof&nbsp;WriteDataResult)&nbsp;{
			&nbsp;&nbsp;&nbsp;&nbsp;WriteDataResult&nbsp;writeDataResult&nbsp;=&nbsp;(WriteDataResult)result;
			&nbsp;&nbsp;&nbsp;&nbsp;System.out.println("Inserted&nbsp;"&nbsp;+&nbsp;writeDataResult.getWrittenRowCount()&nbsp;+&nbsp;"&nbsp;records");
			&nbsp;&nbsp;}

			&nbsp;&nbsp;if&nbsp;(result&nbsp;instanceof&nbsp;PatternFinderResult)&nbsp;{
			&nbsp;&nbsp;&nbsp;&nbsp;PatternFinderResult&nbsp;patternFinderResult&nbsp;=&nbsp;(PatternFinderResult)result;
			&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;matches&nbsp;=&nbsp;patternFinderResult.getMatchCount("Aaaaa&nbsp;Aaaaa")
			&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;total&nbsp;=&nbsp;patternFinderResult.getTotalCount();
			&nbsp;&nbsp;&nbsp;&nbsp;System.out.println("There&nbsp;where&nbsp;"&nbsp;+&nbsp;matches&nbsp;+&nbsp;"&nbsp;matches&nbsp;out&nbsp;of&nbsp;"&nbsp;+&nbsp;total&nbsp;+&nbsp;"&nbsp;for&nbsp;our&nbsp;standard&nbsp;pattern.");
			&nbsp;&nbsp;}
			}
		</pre>

		<p>As you can see, how you handle the result depends a lot on what
			type of result is produced.
		</p>

		<p>For generic handling of results, including all the possible
			result extensions that might occur, DataCleaner employs a renderer
			framework which selects a result renderer according to type and
			precedence. If you need such generic functionality, take a look at
			the classes RendererBean, RendererFactory, Renderer and
			RenderingFormat.
		</p>

		<p>
			One common requirement is to persisting it. We recommend doing
			this
			by means of Java's serialization, since analysis results are
			polymorphic and it's structure may be dependent on extensions. You
			can also device a more "structured" persistance scheme, but beware
			that it will require quite some stability in terms of which analyzers
			you add to your jobs.
		</p>
		<p>
			So let's see how we use Java serialization. But
			unfortunately
			<span class="emphasis"><em>AnalysisResultFuture</em></span>
			isn't serializable! There is however a class which shares the
			interface 'AnalysisResult' with 'AnalysisResultFuture', that is
			serializable, Namely 'SimpleAnalysisResult'. Let's see how to use it
			and serialize our result to a .analysis.result.dat file, (which
			DataCleaner can read):
		</p>
		<pre class="programlisting">
			//&nbsp;make&nbsp;the&nbsp;result&nbsp;serializeable
			AnalysisResult&nbsp;analysisResult&nbsp;=&nbsp;resultFuture;
			analysisResult&nbsp;=&nbsp;new&nbsp;SimpleAnalysisResult(analysisResult.getResultMap());
			ObjectOutputStream&nbsp;oos&nbsp;=&nbsp;new&nbsp;ObjectOutputStream(new&nbsp;FileOutputStream("my_result.analysis.result.dat"));
			oos.writeObject(analysisResult);
			oos.close();
		</pre>
		<p>And now let's, for example sake, also load our file by
			deserializing it. For this we need to use the
			ChangeAwareObjectInputStream class, which ensures backwards
			compatible deserialization of objects:
		</p>
		<pre class="programlisting">
			ObjectInputStream&nbsp;ois&nbsp;=&nbsp;new&nbsp;ChangeAwareObjectInputStream(new&nbsp;FileInputStream("my_result.analysis.result.dat"));
			AnalysisResult&nbsp;analysisResult&nbsp;=&nbsp;(AnalysisResult)&nbsp;ois.readObject();
		</pre>
		<p>Now the result is restored and you can further work with it.
		</p>
	</div>

</div>
		<div class="chapter" title="Chapter&nbsp;19.&nbsp;Developer resources"><div class="titlepage"><div><div><h2 class="title"><a name="chapter_developer_resources"></a>Chapter&nbsp;19.&nbsp;Developer resources</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#section_development_tutorials">Extension development tutorials</a></span></dt><dt><span class="section"><a href="#developer_building_datacleaner">Building DataCleaner</a></span></dt></dl></div>

	

	<div class="section" title="Extension development tutorials"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="section_development_tutorials"></a>Extension development tutorials</h2></div></div></div>
		
		<p>
			There are many useful resources for those who engage in
			developing extensions (aka. plugins / add-ons) to DataCleaner. To
			help you on your way, here's a list of useful links. If you think
			this list is missing a link, please let us know:
		</p>
		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>
					<a class="link" href="http://kasper.eobjects.org/2010/09/developing-value-transformer-using.html" target="_top">Tutorial: Developing a transformer</a>
				</p>
			</li><li class="listitem">
				<p>
					<a class="link" href="http://kasper.eobjects.org/2010/09/developing-analyzer-using-analyzerbeans.html" target="_top">Tutorial: Developing an analyzer</a>
				</p>
			</li><li class="listitem">
				<p>
					<a class="link" href="http://kasper.eobjects.org/2012/04/implementing-custom-datastore-in.html" target="_top">Tutorial: Implementing a custom datastore</a>
				</p>
			</li><li class="listitem">
				<p>
					<a class="link" href="http://eobjects.org/datacleaner/apidocs/current/" target="_top">Javadoc: DataCleaner</a>
				</p>
			</li><li class="listitem">
				<p>
					<a class="link" href="http://metamodel.apache.org/apidocs/current" target="_top">Javadoc: MetaModel</a>
				</p>
			</li></ol></div>
	</div>

	<div class="section" title="Building DataCleaner"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="developer_building_datacleaner"></a>Building DataCleaner</h2></div></div></div>
		

		<p>
			Get the source code for DataCleaner from
			GitHub:
		</p>
		<pre class="programlisting">
			&gt;&nbsp;git&nbsp;clone&nbsp;https://github.com/datacleaner/DataCleaner.git&nbsp;DataCleaner
		</pre>

		<p>
			Build the projects:
		</p>
		<pre class="programlisting">
			&gt;&nbsp;cd&nbsp;DataCleaner
			&gt;&nbsp;mvn&nbsp;clean&nbsp;install
		</pre>

		<p>Run DataCleaner</p>
		<pre class="programlisting">
			&gt;&nbsp;cd&nbsp;desktop/ui/target
			&gt;&nbsp;java&nbsp;-jar&nbsp;DataCleaner-desktop-ui-[version].jar
		</pre>
	</div>

</div>
		<div class="chapter" title="Chapter&nbsp;20.&nbsp;Extension packaging"><div class="titlepage"><div><div><h2 class="title"><a name="d5e1407"></a>Chapter&nbsp;20.&nbsp;Extension packaging</h2></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p>
			<p>
				DataCleaner extensions are packages of added functionality,
				written in Java. To correctly package an extension, this chapter
				will walk through the details.
			</p>
		</div></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#extensions_annotated_components">Annotated components</a></span></dt><dt><span class="section"><a href="#extensions_single_jar_file">Single JAR file</a></span></dt><dt><span class="section"><a href="#extensions_metadata_xml">Extension metadata XML</a></span></dt><dt><span class="section"><a href="#extensions_component_icons">Component icons</a></span></dt></dl></div>

	

	

	<div class="section" title="Annotated components"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="extensions_annotated_components"></a>Annotated components</h2></div></div></div>
		
		<p>The main principle behind extension discovery in DataCleaner is
			annotated classes. Any component that should be discovered should
			have either of these annotations:
		</p>

		<div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
				<p>
					<span class="emphasis"><em>@java.inject.Named</em></span>
					- for classes that implement the Transformer, Filter or Analyzer
					interface.
				</p>
			</li><li class="listitem">
				<p>
					<span class="emphasis"><em>@org.datacleaner.api.RendererBean</em></span>
					- for classes that implement the Renderer interface.
				</p>
			</li></ol></div>

		<p>Please refer to the javadoc documentation of the interfaces for
			details on usage.
		</p>
	</div>

	<div class="section" title="Single JAR file"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="extensions_single_jar_file"></a>Single JAR file</h2></div></div></div>
		
		<p>The extension must consist of a single JAR file. If you have
			dependencies other than the libraries provided by the DataCleaner
			distribution, you need to package these inside your own JAR file. If
			you're using Maven for your build, the Maven Assembly Plugin can
			provide this functionality easily using this snippet in your POM:
		</p>
		<pre lang="xml" class="programlisting">
			&lt;build&gt;
			&nbsp;&lt;plugins&gt;
			&nbsp;&nbsp;&lt;plugin&gt;
			&nbsp;&nbsp;&nbsp;&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
			&nbsp;&nbsp;&nbsp;&lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
			&nbsp;&nbsp;&nbsp;&lt;version&gt;2.2.1&lt;/version&gt;
			&nbsp;&nbsp;&nbsp;&lt;configuration&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;descriptorRefs&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
			&nbsp;&nbsp;&nbsp;&nbsp;&lt;/descriptorRefs&gt;
			&nbsp;&nbsp;&nbsp;&lt;/configuration&gt;
			&nbsp;&nbsp;&lt;/plugin&gt;
			&nbsp;&lt;/plugins&gt;
			&lt;/build&gt; </pre>
	</div>

	<div class="section" title="Extension metadata XML"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="extensions_metadata_xml"></a>Extension metadata XML</h2></div></div></div>
		
		<p>
			To improve the experience, you can optionally include metadata about
			the extension in an XML file, bundled
			<span class="emphasis"><em>within</em></span>
			the JAR file itself.
		</p>
		<p>
			The name of the extension metadata file has to be
			<span class="emphasis"><em>datacleaner-extension.xml</em></span>
			and be placed in the root directory of the JAR file. Here's an
			example of how the file looks like:
		</p>
		<pre lang="xml" class="programlisting">
			&lt;extension&nbsp;xmlns="http://eobjects.org/datacleaner/extension/1.0"&gt;
			&nbsp;&nbsp;&lt;name&gt;My&nbsp;extension&lt;/name&gt;
			&nbsp;&nbsp;&lt;package&gt;path.to.extension&lt;/package&gt;
			&nbsp;&nbsp;&lt;description&gt;This&nbsp;is&nbsp;an&nbsp;example&nbsp;extension.&nbsp;I&nbsp;should&nbsp;put&nbsp;a&nbsp;short&nbsp;description&nbsp;here.&lt;/description&gt;
			&nbsp;&nbsp;&lt;icon&gt;path/to/extension/ExtensionIcon.png&lt;/icon&gt;
			&nbsp;&nbsp;&lt;author&gt;John&nbsp;Doe&lt;/author&gt;
			&nbsp;&nbsp;&lt;url&gt;https://datacleaner.org/extensions&lt;/url&gt;
			&nbsp;&nbsp;&lt;version&gt;1.0&lt;/version&gt;
			&lt;/extension&gt;
		</pre>
		<p>The added value of this metadata is that DataCleaner can expose
			this information to the user and also use it to manage updates of the
			extension etc. The metadata file is however, completely optional.
		</p>
	</div>

	<div class="section" title="Component icons"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="extensions_component_icons"></a>Component icons</h2></div></div></div>
		
		<p>If you wish to add a custom icon for your components (eg. a
			transformer or analyzer), you need to place the icon as a PNG image
			with the same name as the fully classified class name of the
			component.
		</p>
		<p>An example: If your component class name is
			"com.company.ext.MyAnalyzer", then the icon for this component should
			be located at "/com/company/ext/MyAnalyzer.png" in the extension JAR
			file.
		</p>
		<p>Similarly, if you bundle your own ComponentCategory
			implementations (which define the menu groups in DataCleaner), you
			can define icons for these by adding a PNG file with a fully
			classified filename corresponding to the ComponentCategory class
			name.
		</p>
	</div>

</div>
		<div class="chapter" title="Chapter&nbsp;21.&nbsp;Embedding DataCleaner"><div class="titlepage"><div><div><h2 class="title"><a name="embedding_datacleaner"></a>Chapter&nbsp;21.&nbsp;Embedding DataCleaner</h2></div></div></div>

	

	<p>
		It is possible to embed DataCleaner into other Java applications.
		This allows a simple way to add Data Quality Analysis (DQA) and Data
		Profiling functionality as an addition to the applications that you
		are building.
	</p>

	<p>
		The simplest way to embed DataCleaner is simply by doing what
		DataCleaner's main executable does - instantiate the
		<span class="emphasis"><em>Bootstrap</em></span>
		class with default arguments:
	</p>

	<pre class="programlisting">
		BootstrapOptions&nbsp;bootstrapOptions&nbsp;=&nbsp;new&nbsp;DefaultBootstrapOptions(args);
		Bootstrap&nbsp;bootstrap&nbsp;=&nbsp;new&nbsp;Bootstrap(bootstrapOptions);
		bootstrap.run();
	</pre>

	<p>
		To customize further, add your own implementation of the
		<span class="emphasis"><em>BootstrapOptions</em></span>
		class. The main scenario for embedding DataCleaner is to run the
		application in the so-called "single datastore mode". This can be
		achieved by implementing the BootstrapOptions and providing a
		non-null value for the getSingleDatastore() method.
	</p>

</div>
	</div>

</div></body></html>